{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target experiments\n",
    "\n",
    "- [ ] 4o + CoT + RAG\n",
    "- [ ] 4o + CoT + No-RAG  \n",
    "- [ ] 4o + CoT + Ideal RAG\n",
    "- [ ] 4o + CoT + RAG + better prompt\n",
    "- [ ] 4o + RAG + better prompt (without CoT)\n",
    "- [ ] o4 + CoT + RAG + better prompt\n",
    "- [ ] o4 + CoT + No-RAG + better prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self, embedding_model: SentenceTransformer, rerank_model: CrossEncoder):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.rerank_model = rerank_model\n",
    "        self.vector_store = []\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def index_documents(self, documents):\n",
    "        # Encode and normalize all document embeddings\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            [doc['payload'] for doc in documents],\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        # Store each document along with its embedding\n",
    "        self.vector_store = []\n",
    "        for idx, embedding in enumerate(embeddings):\n",
    "            self.vector_store.append({\n",
    "                'id': documents[idx]['id'],\n",
    "                'payload': documents[idx]['payload'],\n",
    "                'vector': embedding  # keep it as a torch tensor\n",
    "            })\n",
    "\n",
    "        # Stack all embeddings into a single tensor for search\n",
    "        self.embedding_matrix = torch.stack([entry['vector'] for entry in self.vector_store])\n",
    "\n",
    "    def search(self, query, top_k=6):\n",
    "        # Encode and normalize the query\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query],\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=False\n",
    "        )  # shape: (1, dim)\n",
    "\n",
    "        # Compute cosine similarity (dot product of normalized vectors)\n",
    "        scores = torch.matmul(query_embedding, self.embedding_matrix.T)  # shape: (1, num_docs)\n",
    "\n",
    "        # Get top_k scores and corresponding indices\n",
    "        top_k_scores, top_k_indices = torch.topk(scores, k=top_k, dim=1)\n",
    "\n",
    "        # Retrieve top_k documents\n",
    "        top_k_documents = [self.vector_store[idx] for idx in top_k_indices[0].tolist()]\n",
    "\n",
    "        return top_k_documents, top_k_scores[0].tolist()\n",
    "\n",
    "    def rerank(self, query, top_k_documents, top_k=6):\n",
    "        scores = self.rerank_model.rank(\n",
    "            query=query,\n",
    "            documents=[doc['payload'] for doc in top_k_documents]\n",
    "        )\n",
    "        return [top_k_documents[idx['corpus_id']] for idx in scores][:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset_V3/corpus.csv\", index_col=\"id\")\n",
    "documents = []\n",
    "for idx, row in df.iterrows():\n",
    "    documents.append({\"id\": idx, \"payload\": f\"{row['title_metadata']} | {row['content']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset_V3/corpus.csv\", index_col=\"id\")\n",
    "laws = {}\n",
    "for idx, row in df.iterrows():\n",
    "    laws[idx] = f\"{row['title_metadata']} | {row['content']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_finetuned = VectorStore(SentenceTransformer('../models/data_trained_V2') , None)\n",
    "vector_store_finetuned.index_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_strategy_7(question, possible_answer, id):\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned.search(f\"{question} | {possible_answer}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "\n",
    "llm_4o = ChatOpenAI(model_name=\"gpt-4o-mini\", api_key=\"\", seed=25, temperature=0)\n",
    "llm_o4 = ChatOpenAI(model_name=\"o4-mini\", api_key=\"\", seed=25, temperature=0, output_version=\"responses/v1\")\n",
    "\n",
    "# top_k_search = 10\n",
    "MAX_CONCURRENT_REQUESTS = 15\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "async def mass_qa_runner(data: pd.DataFrame, strategy):\n",
    "    tasks = []\n",
    "    for real_idx, (idx, item) in tqdm(enumerate(data.iterrows())):\n",
    "        # Create tasks for parallel processing\n",
    "        tasks.append(strategy(item))\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    final_input_prompt = []\n",
    "    final_output_prompt = []\n",
    "    final_qa_result = []\n",
    "    final_correct_answers = []\n",
    "    for result, (idx, item) in zip(results, data.iterrows()):\n",
    "        input_prompt, output_prompt, qa_result, correct_answers = result\n",
    "        final_input_prompt.append(input_prompt)\n",
    "        final_output_prompt.append(output_prompt)\n",
    "        final_qa_result.append(qa_result)\n",
    "        final_correct_answers.append(correct_answers)\n",
    "\n",
    "    data['input_prompt'] = final_input_prompt\n",
    "    data['output_prompt'] = final_output_prompt\n",
    "    data['qa_result'] = final_qa_result\n",
    "    data['correct_answers'] = final_correct_answers\n",
    "    return data[['id', 'input_prompt', 'output_prompt', 'qa_result', 'correct_answers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = {\n",
    "    \"effort\": \"medium\",  # 'low', 'medium', or 'high'\n",
    "    \"summary\": \"detailed\",  # 'detailed', 'auto', or None\n",
    "}\n",
    "\n",
    "llm_o4 = ChatOpenAI(model_name=\"o4-mini\", api_key=\"\", reasoning=reasoning, output_version=\"responses/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_stats(data: pd.DataFrame):\n",
    "    final_em = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    exact_match_count = 0\n",
    "    total_questions = 0\n",
    "    for real_idx, (idx, item) in tqdm(enumerate(data.iterrows())):\n",
    "        given = item[\"qa_result\"]\n",
    "        real = item[\"correct_answers\"]\n",
    "        if set(given) == set(real):\n",
    "            exact_match_count += 1\n",
    "\n",
    "        final_em.append(set(given) == set(real))\n",
    "\n",
    "        true_positives = len([x for x in given if x in real])\n",
    "        false_positives = len([x for x in given if x not in real])\n",
    "        false_negatives = len([x for x in real if x not in given])\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives + 1e-8)\n",
    "        recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        total_questions += 1\n",
    "    \n",
    "    data['exact_match'] = final_em\n",
    "    \n",
    "    avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "    avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    exact_match = exact_match_count / total_questions\n",
    "\n",
    "    print(f\"\\n====== EVALUATION RESULTS ======\")\n",
    "    print(f\"Precision:    {avg_precision:.3f}\")\n",
    "    print(f\"Recall:       {avg_recall:.3f}\")\n",
    "    print(f\"F1 Score:     {avg_f1:.3f}\")\n",
    "    print(f\"Exact Match:  {exact_match:.3f} ({exact_match_count}/{total_questions})\")\n",
    "\n",
    "    return data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def strategy_1(item):\n",
    "    question=item['question']\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "\n",
    "\n",
    "    top_k_documents = retrieve_strategy_7(question, answers, None)\n",
    "    retrieved_set = [(doc, laws[doc]) for doc in top_k_documents]\n",
    "    ideal_rag = [(ideal, laws[ideal]) for ideal in ast.literal_eval(item['legislation'])]\n",
    "\n",
    "    docs = retrieved_set\n",
    "\n",
    "    formated_docs = \"\\n\\n\".join([f\"[{entry[0]}]: {entry[1]}\" for entry in docs])\n",
    "\n",
    "\n",
    "    full_prompt = \"\"\"Esti un politist rutier. Vorbesti doar Limba romana.\n",
    "Trebuie sa rezolvi o grila de la un test auto. Aceasta grila poate avea unul sau mai multe raspunsuri corecte.\n",
    "Gandestete care e raspunsul corect si raspunde la intrebare. La final, ultima parte din raspuns trebuie sa fie litera sau literele corecte.\n",
    "De exemplu, raspunsul tau se va incheia cu\n",
    "\n",
    "\"Raspuns corect: A\"\n",
    "sau\n",
    "\"Raspuns corect: A,B\"\n",
    "\n",
    "Acesta este modul in care trebuie sa gandesti:\n",
    "1. Citeste atent intrebare si variantele de raspuns.\n",
    "2. Identifica ce informatii din legislatie ar putea fi relevante. (Legislatia Romaniei)\n",
    "3. Daca ai mai multe raspunsuri corecte, argumenteaza fiecare alegere.\n",
    "\n",
    "Aceasta este intrebarea:\n",
    "{question}\n",
    "\n",
    "Aceastea sunt variantele de raspuns:\n",
    "{answers}\n",
    "\n",
    "Aceastea sunt legile relevante, dar nu neaparat toate sunt relevante:\n",
    "{documents}\n",
    "===================\n",
    "\"\"\".format(question = question, answers = answers, documents = formated_docs)\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_4o.ainvoke([message])\n",
    "        res = res.content\n",
    "    \n",
    "    final_res = [x.strip() for x in res.split('corect:')[-1].split(',')]\n",
    "    final_res = [re.sub(r'[^\\w\\s]', '', x).strip() for x in final_res]\n",
    "\n",
    "    correct_answers = [entry['answer_text'][0] for entry in ast.literal_eval(item['answers']) if entry['is_correct']]\n",
    "    # return full_prompt, res, final_res\n",
    "\n",
    "    return full_prompt, res, final_res, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def strategy_2(item):\n",
    "    question=item['question']\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "\n",
    "\n",
    "    top_k_documents = retrieve_strategy_7(question, answers, None)\n",
    "    retrieved_set = [(doc, laws[doc]) for doc in top_k_documents]\n",
    "    ideal_rag = [(ideal, laws[ideal]) for ideal in ast.literal_eval(item['legislation'])]\n",
    "\n",
    "    docs = retrieved_set\n",
    "\n",
    "    formated_docs = \"\\n\\n\".join([f\"[{entry[0]}]: {entry[1]}\" for entry in docs])\n",
    "\n",
    "\n",
    "    full_prompt = \"\"\"Esti un politist rutier. Vorbesti doar Limba romana.\n",
    "Trebuie sa rezolvi o grila de la un test auto. Aceasta grila poate avea unul sau mai multe raspunsuri corecte.\n",
    "Gandestete care e raspunsul corect si raspunde la intrebare. La final, ultima parte din raspuns trebuie sa fie litera sau literele corecte.\n",
    "De exemplu, raspunsul tau se va incheia cu\n",
    "\n",
    "\"Raspuns corect: A\"\n",
    "sau\n",
    "\"Raspuns corect: A,B\"\n",
    "\n",
    "Acesta este modul in care trebuie sa gandesti:\n",
    "1. Citeste atent intrebare si variantele de raspuns.\n",
    "2. Identifica ce informatii din legislatie ar putea fi relevante. (Legislatia Romaniei)\n",
    "3. Daca ai mai multe raspunsuri corecte, argumenteaza fiecare alegere.\n",
    "\n",
    "Aceasta este intrebarea:\n",
    "{question}\n",
    "\n",
    "Aceastea sunt variantele de raspuns:\n",
    "{answers}\n",
    "===================\n",
    "\"\"\".format(question = question, answers = answers)\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_4o.ainvoke([message])\n",
    "        res = res.content\n",
    "    \n",
    "    final_res = [x.strip() for x in res.split('corect:')[-1].split(',')]\n",
    "    final_res = [re.sub(r'[^\\w\\s]', '', x).strip() for x in final_res]\n",
    "\n",
    "    correct_answers = [entry['answer_text'][0] for entry in ast.literal_eval(item['answers']) if entry['is_correct']]\n",
    "    # return full_prompt, res, final_res\n",
    "\n",
    "    return full_prompt, res, final_res, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def strategy_3(item):\n",
    "    question=item['question']\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "\n",
    "\n",
    "    top_k_documents = retrieve_strategy_7(question, answers, None)\n",
    "    retrieved_set = [(doc, laws[doc]) for doc in top_k_documents]\n",
    "    ideal_rag = [(ideal, laws[ideal]) for ideal in ast.literal_eval(item['legislation'])]\n",
    "\n",
    "    docs = ideal_rag\n",
    "\n",
    "    formated_docs = \"\\n\\n\".join([f\"[{entry[0]}]: {entry[1]}\" for entry in docs])\n",
    "\n",
    "\n",
    "    full_prompt = \"\"\"Esti un politist rutier. Vorbesti doar Limba romana.\n",
    "Trebuie sa rezolvi o grila de la un test auto. Aceasta grila poate avea unul sau mai multe raspunsuri corecte.\n",
    "Gandestete care e raspunsul corect si raspunde la intrebare. La final, ultima parte din raspuns trebuie sa fie litera sau literele corecte.\n",
    "De exemplu, raspunsul tau se va incheia cu\n",
    "\n",
    "\"Raspuns corect: A\"\n",
    "sau\n",
    "\"Raspuns corect: A,B\"\n",
    "\n",
    "Acesta este modul in care trebuie sa gandesti:\n",
    "1. Citeste atent intrebare si variantele de raspuns.\n",
    "2. Identifica ce informatii din legislatie ar putea fi relevante. (Legislatia Romaniei)\n",
    "3. Daca ai mai multe raspunsuri corecte, argumenteaza fiecare alegere.\n",
    "\n",
    "Aceasta este intrebarea:\n",
    "{question}\n",
    "\n",
    "Aceastea sunt variantele de raspuns:\n",
    "{answers}\n",
    "\n",
    "Aceastea sunt legile relevante, dar nu neaparat toate sunt relevante:\n",
    "{documents}\n",
    "===================\n",
    "\"\"\".format(question = question, answers = answers, documents = formated_docs)\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_4o.ainvoke([message])\n",
    "        res = res.content\n",
    "    \n",
    "    final_res = [x.strip() for x in res.split('corect:')[-1].split(',')]\n",
    "    final_res = [re.sub(r'[^\\w\\s]', '', x).strip() for x in final_res]\n",
    "\n",
    "    correct_answers = [entry['answer_text'][0] for entry in ast.literal_eval(item['answers']) if entry['is_correct']]\n",
    "    # return full_prompt, res, final_res\n",
    "\n",
    "    return full_prompt, res, final_res, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def strategy_4(item):\n",
    "    question=item['question']\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "\n",
    "\n",
    "    top_k_documents = retrieve_strategy_7(question, answers, None)\n",
    "    retrieved_set = [(doc, laws[doc]) for doc in top_k_documents]\n",
    "    ideal_rag = [(ideal, laws[ideal]) for ideal in ast.literal_eval(item['legislation'])]\n",
    "\n",
    "    docs = retrieved_set\n",
    "\n",
    "    formated_docs = \"\\n\\n\".join([f\"[{entry[0]}]: {entry[1]}\" for entry in docs])\n",
    "\n",
    "\n",
    "    full_prompt = \"\"\"Esti un politist rutier. Vorbesti doar in limba romana.\n",
    "Trebuie sa rezolvi o grila de la un test auto. Grila poate avea unul sau mai multe raspunsuri corecte. Vei folosi strict legile din Romania.\n",
    "\n",
    "Gandeste logic, dar nu extrapola peste informatiile oferite. Judeca doar momentul descris, nu presupune alte situatii.\n",
    "\n",
    "Reguli de gandire:\n",
    "1. Citeste cu maxima atentie intrebarea si variantele de raspuns.\n",
    "2. Identifica strict ce prevederi din legislatia rutiera din Romania se aplica situatiei date.\n",
    "3. Daca raspunsul pare \"mai sigur\" dar este contrar legislatiei, urmeaza legea, nu instinctul de precautie.\n",
    "4. Alege DOAR raspunsurile care sunt complet corecte conform textului legii — nu ghici, nu completa informatii lipsa.\n",
    "5. Daca un raspuns corect este mai bun decat altul dat ca si corect, include mai multe situatii specifice sau exceptii, atunci trebuie ales doar acela.\n",
    "6. Argumenteaza clar de ce ai ales fiecare raspuns corect. Daca exista mai multe raspunsuri corecte, explica fiecare alegere separat.\n",
    "7. Fii atent la mici detalii care pot schimba sensul intrebarii sau al raspunsurilor (exista intrebari-capcana).\n",
    "\n",
    "\n",
    "La final, ultima parte din raspuns trebuie sa fie litera sau literele corecte.\n",
    "De exemplu, raspunsul tau se va incheia cu:\n",
    "\n",
    "\"Raspuns corect: A\"  \n",
    "sau  \n",
    "\"Raspuns corect: A,B\"\n",
    "\n",
    "Aceasta este intrebarea:\n",
    "{question}\n",
    "\n",
    "Acestea sunt variantele de raspuns:\n",
    "{answers}\n",
    "\n",
    "Aceastea sunt legile relevante, dar nu neaparat toate sunt relevante:\n",
    "{documents}\n",
    "===================\n",
    "\"\"\".format(question = question, answers = answers, documents = formated_docs)\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_4o.ainvoke([message])\n",
    "        res = res.content\n",
    "    \n",
    "    final_res = [x.strip() for x in res.split('corect:')[-1].split(',')]\n",
    "    final_res = [re.sub(r'[^\\w\\s]', '', x).strip() for x in final_res]\n",
    "\n",
    "    correct_answers = [entry['answer_text'][0] for entry in ast.literal_eval(item['answers']) if entry['is_correct']]\n",
    "    # return full_prompt, res, final_res\n",
    "\n",
    "    return full_prompt, res, final_res, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def strategy_5(item):\n",
    "    question=item['question']\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "\n",
    "\n",
    "    top_k_documents = retrieve_strategy_7(question, answers, None)\n",
    "    retrieved_set = [(doc, laws[doc]) for doc in top_k_documents]\n",
    "    ideal_rag = [(ideal, laws[ideal]) for ideal in ast.literal_eval(item['legislation'])]\n",
    "\n",
    "    docs = retrieved_set\n",
    "\n",
    "    formated_docs = \"\\n\\n\".join([f\"[{entry[0]}]: {entry[1]}\" for entry in docs])\n",
    "\n",
    "\n",
    "    full_prompt = \"\"\"Esti un politist rutier. Vorbesti doar in limba romana.\n",
    "Trebuie sa rezolvi o grila de la un test auto. Grila poate avea unul sau mai multe raspunsuri corecte. Vei folosi strict legile din Romania.\n",
    "\n",
    "Gandeste logic, dar nu extrapola peste informatiile oferite. Judeca doar momentul descris, nu presupune alte situatii.\n",
    "\n",
    "Reguli de gandire:\n",
    "1. Citeste cu maxima atentie intrebarea si variantele de raspuns.\n",
    "2. Identifica strict ce prevederi din legislatia rutiera din Romania se aplica situatiei date.\n",
    "3. Daca raspunsul pare \"mai sigur\" dar este contrar legislatiei, urmeaza legea, nu instinctul de precautie.\n",
    "4. Alege DOAR raspunsurile care sunt complet corecte conform textului legii — nu ghici, nu completa informatii lipsa.\n",
    "5. Daca un raspuns corect este mai bun decat altul dat ca si corect, include mai multe situatii specifice sau exceptii, atunci trebuie ales doar acela.\n",
    "6. Fii atent la mici detalii care pot schimba sensul intrebarii sau al raspunsurilor (exista intrebari-capcana).\n",
    "\n",
    "\n",
    "Raspunde direct cu variantale corecte.\n",
    "De exemplu, raspunsul tau se va incheia cu:\n",
    "\n",
    "\"Raspuns corect: A\"  \n",
    "sau  \n",
    "\"Raspuns corect: A,B\"\n",
    "\n",
    "Aceasta este intrebarea:\n",
    "{question}\n",
    "\n",
    "Acestea sunt variantele de raspuns:\n",
    "{answers}\n",
    "\n",
    "Aceastea sunt legile relevante, dar nu neaparat toate sunt relevante:\n",
    "{documents}\n",
    "===================\n",
    "\"\"\".format(question = question, answers = answers, documents = formated_docs)\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_4o.ainvoke([message])\n",
    "        res = res.content\n",
    "    \n",
    "    final_res = [x.strip() for x in res.split('corect:')[-1].split(',')]\n",
    "    final_res = [re.sub(r'[^\\w\\s]', '', x).strip() for x in final_res]\n",
    "\n",
    "    correct_answers = [entry['answer_text'][0] for entry in ast.literal_eval(item['answers']) if entry['is_correct']]\n",
    "    # return full_prompt, res, final_res\n",
    "\n",
    "    return full_prompt, res, final_res, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = {\n",
    "    \"effort\": \"medium\",  # 'low', 'medium', or 'high'\n",
    "    \"summary\": \"detailed\",  # 'detailed', 'auto', or None\n",
    "}\n",
    "\n",
    "llm_o4 = ChatOpenAI(model_name=\"o4-mini\", api_key=\"\", reasoning=reasoning, output_version=\"responses/v1\")\n",
    "\n",
    "async def strategy_6(item):\n",
    "    question=item['question']\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "\n",
    "\n",
    "    top_k_documents = retrieve_strategy_7(question, answers, None)\n",
    "    retrieved_set = [(doc, laws[doc]) for doc in top_k_documents]\n",
    "    ideal_rag = [(ideal, laws[ideal]) for ideal in ast.literal_eval(item['legislation'])]\n",
    "\n",
    "    docs = retrieved_set\n",
    "\n",
    "    formated_docs = \"\\n\\n\".join([f\"[{entry[0]}]: {entry[1]}\" for entry in docs])\n",
    "\n",
    "\n",
    "    full_prompt = \"\"\"Esti un politist rutier. Vorbesti doar in limba romana.\n",
    "Trebuie sa rezolvi o grila de la un test auto. Grila poate avea unul sau mai multe raspunsuri corecte. Vei folosi strict legile din Romania.\n",
    "\n",
    "Gandeste logic, dar nu extrapola peste informatiile oferite. Judeca doar momentul descris, nu presupune alte situatii.\n",
    "\n",
    "Reguli de gandire:\n",
    "1. Citeste cu maxima atentie intrebarea si variantele de raspuns.\n",
    "2. Identifica strict ce prevederi din legislatia rutiera din Romania se aplica situatiei date.\n",
    "3. Daca raspunsul pare \"mai sigur\" dar este contrar legislatiei, urmeaza legea, nu instinctul de precautie.\n",
    "4. Alege DOAR raspunsurile care sunt complet corecte conform textului legii — nu ghici, nu completa informatii lipsa.\n",
    "5. Daca un raspuns corect este mai bun decat altul dat ca si corect, include mai multe situatii specifice sau exceptii, atunci trebuie ales doar acela.\n",
    "6. Argumenteaza clar de ce ai ales fiecare raspuns corect. Daca exista mai multe raspunsuri corecte, explica fiecare alegere separat.\n",
    "7. Fii atent la mici detalii care pot schimba sensul intrebarii sau al raspunsurilor (exista intrebari-capcana).\n",
    "\n",
    "\n",
    "La final, ultima parte din raspuns trebuie sa fie litera sau literele corecte.\n",
    "De exemplu, raspunsul tau se va incheia cu:\n",
    "\n",
    "\"Raspuns corect: A\"  \n",
    "sau  \n",
    "\"Raspuns corect: A,B\"\n",
    "\n",
    "Aceasta este intrebarea:\n",
    "{question}\n",
    "\n",
    "Acestea sunt variantele de raspuns:\n",
    "{answers}\n",
    "\n",
    "Aceastea sunt legile relevante, dar nu neaparat toate sunt relevante:\n",
    "{documents}\n",
    "===================\n",
    "\"\"\".format(question = question, answers = answers, documents = formated_docs)\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_o4.ainvoke([message])\n",
    "        res = res.content\n",
    "    \n",
    "    reasoning = [x['text'] for x in res[0]['summary']]\n",
    "    output = res[1]['text']\n",
    "\n",
    "    output_full = \"\\n\".join([f\"[REASONING]{x}\" for x in reasoning]) + f\"\\n[OUTPUT]{output}\"\n",
    "    \n",
    "    \n",
    "    final_res = [x.strip() for x in output_full.split('corect:')[-1].split(',')]\n",
    "    final_res = [re.sub(r'[^\\w\\s]', '', x).strip() for x in final_res]\n",
    "\n",
    "    correct_answers = [entry['answer_text'][0] for entry in ast.literal_eval(item['answers']) if entry['is_correct']]\n",
    "    # return full_prompt, res, final_res\n",
    "\n",
    "    return full_prompt, output_full, final_res, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def strategy_7(item):\n",
    "    question=item['question']\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "\n",
    "\n",
    "    top_k_documents = retrieve_strategy_7(question, answers, None)\n",
    "    retrieved_set = [(doc, laws[doc]) for doc in top_k_documents]\n",
    "    ideal_rag = [(ideal, laws[ideal]) for ideal in ast.literal_eval(item['legislation'])]\n",
    "\n",
    "    docs = retrieved_set\n",
    "\n",
    "    formated_docs = \"\\n\\n\".join([f\"[{entry[0]}]: {entry[1]}\" for entry in docs])\n",
    "\n",
    "\n",
    "    full_prompt = \"\"\"Esti un politist rutier. Vorbesti doar in limba romana.\n",
    "Trebuie sa rezolvi o grila de la un test auto. Grila poate avea unul sau mai multe raspunsuri corecte. Vei folosi strict legile din Romania.\n",
    "\n",
    "Gandeste logic, dar nu extrapola peste informatiile oferite. Judeca doar momentul descris, nu presupune alte situatii.\n",
    "\n",
    "Reguli de gandire:\n",
    "1. Citeste cu maxima atentie intrebarea si variantele de raspuns.\n",
    "2. Identifica strict ce prevederi din legislatia rutiera din Romania se aplica situatiei date.\n",
    "3. Daca raspunsul pare \"mai sigur\" dar este contrar legislatiei, urmeaza legea, nu instinctul de precautie.\n",
    "4. Alege DOAR raspunsurile care sunt complet corecte conform textului legii — nu ghici, nu completa informatii lipsa.\n",
    "5. Daca un raspuns corect este mai bun decat altul dat ca si corect, include mai multe situatii specifice sau exceptii, atunci trebuie ales doar acela.\n",
    "6. Argumenteaza clar de ce ai ales fiecare raspuns corect. Daca exista mai multe raspunsuri corecte, explica fiecare alegere separat.\n",
    "7. Fii atent la mici detalii care pot schimba sensul intrebarii sau al raspunsurilor (exista intrebari-capcana).\n",
    "\n",
    "\n",
    "La final, ultima parte din raspuns trebuie sa fie litera sau literele corecte.\n",
    "De exemplu, raspunsul tau se va incheia cu:\n",
    "\n",
    "\"Raspuns corect: A\"  \n",
    "sau  \n",
    "\"Raspuns corect: A,B\"\n",
    "\n",
    "Aceasta este intrebarea:\n",
    "{question}\n",
    "\n",
    "Acestea sunt variantele de raspuns:\n",
    "{answers}\n",
    "===================\n",
    "\"\"\".format(question = question, answers = answers)\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_o4.ainvoke([message])\n",
    "        res = res.content\n",
    "    \n",
    "    reasoning = [x['text'] for x in res[0]['summary']]\n",
    "    output = res[1]['text']\n",
    "\n",
    "    output_full = \"\\n\".join([f\"[REASONING]{x}\" for x in reasoning]) + f\"\\n[OUTPUT]{output}\"\n",
    "    \n",
    "    \n",
    "    final_res = [x.strip() for x in output_full.split('corect:')[-1].split(',')]\n",
    "    final_res = [re.sub(r'[^\\w\\s]', '', x).strip() for x in final_res]\n",
    "\n",
    "    correct_answers = [entry['answer_text'][0] for entry in ast.literal_eval(item['answers']) if entry['is_correct']]\n",
    "    # return full_prompt, res, final_res\n",
    "\n",
    "    return full_prompt, output_full, final_res, correct_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1_train = pd.read_csv('../data/dataset_V3/split_1_train.csv')\n",
    "split_1_test = pd.read_csv('../data/dataset_V3/split_1_test.csv')\n",
    "split_2 = pd.read_csv('../data/dataset_V3/split_2.csv')\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_train, strategy_1)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_1_split_1_train.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_test, strategy_1)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_1_split_1_test.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_2, strategy_1)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_1_split_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1_train = pd.read_csv('../data/dataset_V3/split_1_train.csv')\n",
    "split_1_test = pd.read_csv('../data/dataset_V3/split_1_test.csv')\n",
    "split_2 = pd.read_csv('../data/dataset_V3/split_2.csv')\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_train, strategy_2)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_2_split_1_train.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_test, strategy_2)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_2_split_1_test.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_2, strategy_2)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_2_split_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_1_train = pd.read_csv('../data/dataset_V3/split_1_train.csv')\n",
    "# split_1_test = pd.read_csv('../data/dataset_V3/split_1_test.csv')\n",
    "# split_2 = pd.read_csv('../data/dataset_V3/split_2.csv')\n",
    "\n",
    "# result_df = await mass_qa_runner(split_1_train, strategy_3)\n",
    "# result_df = run_stats(result_df)\n",
    "# result_df.to_csv('../results/qa/qa_strat_3_split_1_train.csv', index=False)\n",
    "\n",
    "# result_df = await mass_qa_runner(split_1_test, strategy_3)\n",
    "# result_df = run_stats(result_df)\n",
    "# result_df.to_csv('../results/qa/qa_strat_3_split_1_test.csv', index=False)\n",
    "\n",
    "# result_df = await mass_qa_runner(split_2, strategy_3)\n",
    "# result_df = run_stats(result_df)\n",
    "# result_df.to_csv('../results/qa/qa_strat_3_split_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1_train = pd.read_csv('../data/dataset_V3/split_1_train.csv')\n",
    "split_1_test = pd.read_csv('../data/dataset_V3/split_1_test.csv')\n",
    "split_2 = pd.read_csv('../data/dataset_V3/split_2.csv')\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_train, strategy_4)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_4_split_1_train.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_test, strategy_4)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_4_split_1_test.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_2, strategy_4)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_4_split_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1_train = pd.read_csv('../data/dataset_V3/split_1_train.csv')\n",
    "split_1_test = pd.read_csv('../data/dataset_V3/split_1_test.csv')\n",
    "split_2 = pd.read_csv('../data/dataset_V3/split_2.csv')\n",
    "\n",
    "# result_df = await mass_qa_runner(split_1_train, strategy_5)\n",
    "# result_df = run_stats(result_df)\n",
    "# result_df.to_csv('../results/qa/qa_strat_5_split_1_train.csv', index=False)\n",
    "\n",
    "# result_df = await mass_qa_runner(split_1_test, strategy_5)\n",
    "# result_df = run_stats(result_df)\n",
    "# result_df.to_csv('../results/qa/qa_strat_5_split_1_test.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_2, strategy_5)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_5_split_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1_train = pd.read_csv('../data/dataset_V3/split_1_train.csv')\n",
    "split_1_test = pd.read_csv('../data/dataset_V3/split_1_test.csv')\n",
    "split_2 = pd.read_csv('../data/dataset_V3/split_2.csv')\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_train, strategy_6)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_6_split_1_train.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_test, strategy_6)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_6_split_1_test.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_2, strategy_6)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_6_split_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1_train = pd.read_csv('../data/dataset_V3/split_1_train.csv')\n",
    "split_1_test = pd.read_csv('../data/dataset_V3/split_1_test.csv')\n",
    "split_2 = pd.read_csv('../data/dataset_V3/split_2.csv')\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_train, strategy_7)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_7_split_1_train.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_1_test, strategy_7)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_7_split_1_test.csv', index=False)\n",
    "\n",
    "result_df = await mass_qa_runner(split_2, strategy_7)\n",
    "result_df = run_stats(result_df)\n",
    "result_df.to_csv('../results/qa/qa_strat_7_split_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_1_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_1_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_1_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_2_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_2_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_2_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_3_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_3_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_3_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_4_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_4_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_4_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_5_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_5_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm/qa_strat_5_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_1_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_1_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_1_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_2_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_2_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_2_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_3_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_3_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_3_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_4_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_4_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_4_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_5_split_1_train_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_5_split_1_test_vllm.csv')\n",
    "_ = run_stats(sp1)\n",
    "\n",
    "sp1 = pd.read_csv('../results/qa_vllm1/qa_strat_5_split_2_vllm.csv')\n",
    "_ = run_stats(sp1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proiect-NLP-Rutier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
