{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target experiments\n",
    "\n",
    "- [ ] QA (baseline to validate visual component)\n",
    "- [ ] Caption + QA\n",
    "- [ ] o4 + Image + QA\n",
    "- [ ] o4 + Image + Caption + QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self, embedding_model: SentenceTransformer, rerank_model: CrossEncoder):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.rerank_model = rerank_model\n",
    "        self.vector_store = []\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def index_documents(self, documents):\n",
    "        # Encode and normalize all document embeddings\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            [doc['payload'] for doc in documents],\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        # Store each document along with its embedding\n",
    "        self.vector_store = []\n",
    "        for idx, embedding in enumerate(embeddings):\n",
    "            self.vector_store.append({\n",
    "                'id': documents[idx]['id'],\n",
    "                'payload': documents[idx]['payload'],\n",
    "                'vector': embedding  # keep it as a torch tensor\n",
    "            })\n",
    "\n",
    "        # Stack all embeddings into a single tensor for search\n",
    "        self.embedding_matrix = torch.stack([entry['vector'] for entry in self.vector_store])\n",
    "\n",
    "    def search(self, query, top_k=6):\n",
    "        # Encode and normalize the query\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query],\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=False\n",
    "        )  # shape: (1, dim)\n",
    "\n",
    "        # Compute cosine similarity (dot product of normalized vectors)\n",
    "        scores = torch.matmul(query_embedding, self.embedding_matrix.T)  # shape: (1, num_docs)\n",
    "\n",
    "        # Get top_k scores and corresponding indices\n",
    "        top_k_scores, top_k_indices = torch.topk(scores, k=top_k, dim=1)\n",
    "\n",
    "        # Retrieve top_k documents\n",
    "        top_k_documents = [self.vector_store[idx] for idx in top_k_indices[0].tolist()]\n",
    "\n",
    "        return top_k_documents, top_k_scores[0].tolist()\n",
    "\n",
    "    def rerank(self, query, top_k_documents, top_k=6):\n",
    "        scores = self.rerank_model.rank(\n",
    "            query=query,\n",
    "            documents=[doc['payload'] for doc in top_k_documents]\n",
    "        )\n",
    "        return [top_k_documents[idx['corpus_id']] for idx in scores][:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset_V3/corpus.csv\", index_col=\"id\")\n",
    "documents = []\n",
    "for idx, row in df.iterrows():\n",
    "    documents.append({\"id\": idx, \"payload\": f\"{row['title_metadata']} | {row['content']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset_V3/corpus_indicators.csv\", index_col=\"id\")\n",
    "documents_indicators = []\n",
    "for idx, row in df.iterrows():\n",
    "    documents_indicators.append({\"id\": idx, \"payload\": f\"{row['title']} | {row['category']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_finetuned = VectorStore(SentenceTransformer('../models/data_trained_V2') , None)\n",
    "vector_store_finetuned.index_documents(documents)\n",
    "\n",
    "vector_store_finetuned_indicators = VectorStore(SentenceTransformer('../models/data_trained_V2') , None)\n",
    "vector_store_finetuned_indicators.index_documents(documents_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset_V3/corpus_indicators.csv\", index_col=\"id\")\n",
    "documents_indicators_2 = []\n",
    "for idx, row in df.iterrows():\n",
    "    documents_indicators_2.append({\"id\": idx, \"payload\": f\"{row['title']} | {row['category']} | {row['content']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_finetuned_indicators_2 = VectorStore(SentenceTransformer('../models/data_trained_V2') , None)\n",
    "vector_store_finetuned_indicators_2.index_documents(documents_indicators_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "reasoning = {\n",
    "    \"effort\": \"medium\",  # 'low', 'medium', or 'high'\n",
    "    \"summary\": \"detailed\",  # 'detailed', 'auto', or None\n",
    "}\n",
    "\n",
    "llm_o4 = ChatOpenAI(model_name=\"o4-mini\", api_key=\"\", reasoning=reasoning, output_version=\"responses/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_3 = pd.read_csv(\"../data/dataset_V3/split_3.csv\") # has laws + indicators\n",
    "split_4 = pd.read_csv(\"../data/dataset_V3/split_4.csv\") # has indicators (no laws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Vector Store & corpus embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "MAX_CONCURRENT_REQUESTS = 60\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "async def mass_runner(data: pd.DataFrame, strategy):\n",
    "    tasks = []\n",
    "    for real_idx, (idx, item) in tqdm(enumerate(data.iterrows())):\n",
    "        # Create tasks for parallel processing\n",
    "        tasks.append(strategy(item))\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    final_inputs = []\n",
    "    final_outputs = []\n",
    "    final_captions = []\n",
    "    for result, (idx, item) in zip(results, data.iterrows()):\n",
    "        input_prompt, output, caption = result\n",
    "        final_inputs.append(input_prompt)\n",
    "        final_outputs.append(output)\n",
    "        final_captions.append(caption)\n",
    "\n",
    "    data['input_prompt'] = final_inputs\n",
    "    data['output'] = final_outputs\n",
    "    data['rephrased_query'] = final_captions\n",
    "\n",
    "    final_df = data[[\"id\",\"input_prompt\", \"output\", \"rephrased_query\"]]\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"../results/captions-split-3.csv\", index_col=\"id\")\n",
    "df2 = pd.read_csv(\"../results/captions-split-4.csv\", index_col=\"id\")\n",
    "captions = {}\n",
    "for idx, row in df1.iterrows():\n",
    "    captions[idx] = row['captions']\n",
    "for idx, row in df2.iterrows():\n",
    "    captions[idx] = row['captions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(href):\n",
    "    local_url = href.split('/')[-1]\n",
    "    local_url = local_url.split('.')[0]\n",
    "    local_url = f\"../data/dataset_V3/images/{local_url}.jpg\"\n",
    "    with open(local_url, \"rb\") as f:\n",
    "        image_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        return image_data\n",
    "    \n",
    "def load_caption(id):\n",
    "    return captions[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q + A baseline\n",
    "async def strategy_1(item):\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "    return \"\", \"\", f'{item['question']} | {answers}'\n",
    "\n",
    "# Caption + QA\n",
    "async def strategy_2(item):\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "    return \"\", \"\", f'{load_caption(item['id'])} | {item['question']} | {answers}'\n",
    "\n",
    "# Image + QA\n",
    "async def strategy_3(item):\n",
    "    question = item[\"question\"]\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "    image = load_img(item['image'])\n",
    "\n",
    "\n",
    "    full_prompt = \"\"\"Esti un politist rutier. Vorbesti doar in limba romana.\n",
    "Primesti o grila de la un test auto care are atasata si o imagine. Trebuie sa selectezi din imagine informatiile necesare,\n",
    "astfel incat sa imbuntatesti intrebarea originala si a facilita cautarea unor articole de lege relevante.\n",
    "\n",
    "Include informatiile relevante despre situatie, indicatoare, si altele elemente specifice condusului si legii.\n",
    "\n",
    "Reguli de gandire:\n",
    "1. Citeste cu maxima atentie intrebarea si variantele de raspuns.\n",
    "2. Analizeaza imaginea si extrage informatiile cele mai importante\n",
    "3. Fii atent la mici detalii care pot schimba sensul intrebarii sau al raspunsurilor\n",
    "4. Explica ce anume trebuie introdus in intrebare pentru a cauta informatiile corecte\n",
    "\n",
    "\n",
    "La final, ultima parte din raspuns trebuie sa fie intrebarea reformulata.\n",
    "\n",
    "\"Raspuns final: [intrebare]\"  \n",
    "\n",
    "Aceasta este intrebarea:\n",
    "{question}\n",
    "\n",
    "Acestea sunt variantele de raspuns:\n",
    "{answers}\n",
    "===================\n",
    "\"\"\".format(question = question, answers = answers)\n",
    "\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": full_prompt},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\", \"detail\": \"high\"},\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_o4.ainvoke([message])\n",
    "        res = res.content\n",
    "\n",
    "    reasoning = [x['text'] for x in res[0]['summary']]\n",
    "    output = res[1]['text']\n",
    "\n",
    "    output_full = \"\\n\".join([f\"[REASONING]{x}\" for x in reasoning]) + f\"\\n[OUTPUT]{output}\"\n",
    "    final_res = output_full.split('final:')[-1].strip()\n",
    "    final_res = final_res.replace('[','').replace(']','')\n",
    "    return full_prompt, output_full, final_res\n",
    "\n",
    "# Image + Caption + QA\n",
    "async def strategy_4(item):\n",
    "    question = item[\"question\"]\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "    image = load_img(item['image'])\n",
    "\n",
    "\n",
    "    full_prompt = \"\"\"Esti un politist rutier. Vorbesti doar in limba romana.\n",
    "Primesti o grila de la un test auto care are atasata si o imagine. Trebuie sa selectezi din imagine informatiile necesare,\n",
    "astfel incat sa imbuntatesti intrebarea originala si a facilita cautarea unor articole de lege relevante.\n",
    "\n",
    "Include informatiile relevante despre situatie, indicatoare, si altele elemente specifice condusului si legii.\n",
    "\n",
    "Reguli de gandire:\n",
    "1. Citeste cu maxima atentie intrebarea si variantele de raspuns.\n",
    "2. Analizeaza imaginea si extrage informatiile cele mai importante\n",
    "3. Fii atent la mici detalii care pot schimba sensul intrebarii sau al raspunsurilor\n",
    "4. Explica ce anume trebuie introdus in intrebare pentru a cauta informatiile corecte\n",
    "\n",
    "\n",
    "La final, ultima parte din raspuns trebuie sa fie intrebarea reformulata.\n",
    "\n",
    "\"Raspuns final: [intrebare]\"  \n",
    "\n",
    "Aceasta este intrebarea:\n",
    "{question}\n",
    "\n",
    "Acestea sunt variantele de raspuns:\n",
    "{answers}\n",
    "\n",
    "Aceasta este descrierea imaginii:\n",
    "{caption}\n",
    "===================\n",
    "\"\"\".format(question = question, answers = answers, caption=load_caption(item['id']))\n",
    "\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": full_prompt},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\", \"detail\": \"high\"},\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_o4.ainvoke([message])\n",
    "        res = res.content\n",
    "\n",
    "    reasoning = [x['text'] for x in res[0]['summary']]\n",
    "    output = res[1]['text']\n",
    "\n",
    "    output_full = \"\\n\".join([f\"[REASONING]{x}\" for x in reasoning]) + f\"\\n[OUTPUT]{output}\"\n",
    "    final_res = output_full.split('final:')[-1].strip()\n",
    "    final_res = final_res.replace('[','').replace(']','')\n",
    "    return full_prompt, output_full, final_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "# df2 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "\n",
    "# results = await mass_runner(df1, strategy_1)\n",
    "# results.to_csv('../results/vir/captions_strat_1_split_3.csv', index=False)\n",
    "\n",
    "# results = await mass_runner(df2, strategy_1)\n",
    "# results.to_csv('../results/vir/captions_strat_1_split_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "# df2 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "\n",
    "# results = await mass_runner(df1, strategy_2)\n",
    "# results.to_csv('../results/vir/captions_strat_2_split_3.csv', index=False)\n",
    "\n",
    "# results = await mass_runner(df2, strategy_2)\n",
    "# results.to_csv('../results/vir/captions_strat_2_split_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "# df2 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "\n",
    "# results = await mass_runner(df1, strategy_3)\n",
    "# results.to_csv('../results/vir/captions_strat_3_split_3.csv', index=False)\n",
    "\n",
    "# results = await mass_runner(df2, strategy_3)\n",
    "# results.to_csv('../results/vir/captions_strat_3_split_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "# df2 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "\n",
    "# results = await mass_runner(df1, strategy_4)\n",
    "# results.to_csv('../results/vir/captions_strat_4_split_3.csv', index=False)\n",
    "\n",
    "# results = await mass_runner(df2, strategy_4)\n",
    "# results.to_csv('../results/vir/captions_strat_4_split_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "captions_st1_sp_3 = pd.read_csv('../results/vir/captions_strat_1_split_3.csv')\n",
    "captions_st1_sp_4 = pd.read_csv('../results/vir/captions_strat_1_split_4.csv')\n",
    "merged_1 = pd.concat([captions_st1_sp_3, captions_st1_sp_4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "captions_st2_sp_3 = pd.read_csv('../results/vir/captions_strat_2_split_3.csv')\n",
    "captions_st2_sp_4 = pd.read_csv('../results/vir/captions_strat_2_split_4.csv')\n",
    "merged_2 = pd.concat([captions_st2_sp_3, captions_st2_sp_4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "captions_st3_sp_3 = pd.read_csv('../results/vir/captions_strat_3_split_3.csv')\n",
    "captions_st3_sp_4 = pd.read_csv('../results/vir/captions_strat_3_split_4.csv')\n",
    "merged_3 = pd.concat([captions_st3_sp_3, captions_st3_sp_4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "captions_st4_sp_3 = pd.read_csv('../results/vir/captions_strat_4_split_3.csv')\n",
    "captions_st4_sp_4 = pd.read_csv('../results/vir/captions_strat_4_split_4.csv')\n",
    "merged_4 = pd.concat([captions_st4_sp_3, captions_st4_sp_4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_1_law(question, possible_answer, id):\n",
    "    rephrased_question = merged_1.loc[merged_1['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_2_law(question, possible_answer, id):\n",
    "    rephrased_question = merged_2.loc[merged_2['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_3_law(question, possible_answer, id):\n",
    "    rephrased_question = merged_3.loc[merged_3['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_4_law(question, possible_answer, id):\n",
    "    rephrased_question = merged_4.loc[merged_4['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_5_law(question, possible_answer, id):\n",
    "    rephrased_question = merged_3.loc[merged_3['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned.search(f\"{rephrased_question} | {question} | {possible_answer}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_6_law(question, possible_answer, id):\n",
    "    rephrased_question = merged_4.loc[merged_4['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned.search(f\"{rephrased_question} | {question} | {possible_answer}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_1_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_1.loc[merged_1['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_2_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_2.loc[merged_2['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_3_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_3.loc[merged_3['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_4_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_4.loc[merged_4['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_5_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_3.loc[merged_3['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators.search(f\"{rephrased_question} | {question} | {possible_answer}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_6_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_4.loc[merged_4['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators.search(f\"{rephrased_question} | {question} | {possible_answer}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_7_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_1.loc[merged_1['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators_2.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_8_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_2.loc[merged_2['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators_2.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_9_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_3.loc[merged_3['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators_2.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_10_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_4.loc[merged_4['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators_2.search(f\"{rephrased_question}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_11_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_3.loc[merged_3['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators_2.search(f\"{rephrased_question} | {question} | {possible_answer}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set\n",
    "\n",
    "def strategy_12_ind(question, possible_answer, id):\n",
    "    rephrased_question = merged_4.loc[merged_4['id'] == id, 'rephrased_query'].values[0]\n",
    "    top_k_documents, top_k_scores = vector_store_finetuned_indicators_2.search(f\"{rephrased_question} | {question} | {possible_answer}\", top_k=10)\n",
    "    retrieved_set = [doc['id'] for doc in top_k_documents]\n",
    "    return retrieved_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from collections.abc import Callable, Awaitable\n",
    "import numpy as np\n",
    "\n",
    "def eval_vir_framework_law(dataset: pd.DataFrame, strategy: Callable, name: str, top_k_search=10):\n",
    "    print('='*30)\n",
    "    print(f\"{name}\")\n",
    "    print('='*30)\n",
    "\n",
    "    fnqrel = {}\n",
    "    fqrel = {}\n",
    "    frun = defaultdict(dict)\n",
    "\n",
    "    docs_retrieved = []\n",
    "\n",
    "    for idx, item in tqdm(dataset.iterrows()):\n",
    "        articles = ast.literal_eval(item['legislation'])  # ground truth: list of relevant articles\n",
    "        question = item[\"question\"]  # the question for search\n",
    "        possible_answer = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "        relevant_set = list(set(articles))\n",
    "\n",
    "        retrieved_set_1 = strategy(question, possible_answer, item['id'])\n",
    "        docs_retrieved.append(retrieved_set_1)\n",
    "        qrel = {}\n",
    "        for doc in relevant_set:\n",
    "            qrel[doc] = 1\n",
    "        \n",
    "        nqrel = {}\n",
    "        for doc in relevant_set:\n",
    "            nqrel[doc] = 2\n",
    "\n",
    "        fqrel[f'{idx}-query'] = qrel\n",
    "        fnqrel[f'{idx}-query'] = nqrel\n",
    "\n",
    "        for i in range(1, top_k_search+1):\n",
    "            run = {}\n",
    "            for doc, score in zip(retrieved_set_1[:i], [1 - 0.05 * x for x in range(10)]):\n",
    "                run[doc] = score\n",
    "            frun[str(i)][f'{idx}-query'] = run\n",
    "        \n",
    "        \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    ndcgs = []\n",
    "    for i in range(1, top_k_search+1):\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(fqrel, {f'recall.{i}'})\n",
    "        results = evaluator.evaluate(frun[str(i)])\n",
    "        avg_recall = np.mean([res[f'recall_{i}'] for res in list(results.values())])\n",
    "        # print(f\"Avg Recall@{i}: {avg_recall}\")\n",
    "        recalls.append(avg_recall.item())\n",
    "\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(fqrel, {f'P.{i}'})\n",
    "        results = evaluator.evaluate(frun[str(i)])\n",
    "        avg_precision = np.mean([res[f'P_{i}'] for res in list(results.values())])\n",
    "        # print(f\"Avg Precision@{i}: {avg_precision}\")\n",
    "        precisions.append(avg_precision.item())\n",
    "\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(fnqrel, {f'ndcg_cut.{i}', })\n",
    "        results = evaluator.evaluate(frun[str(i)])\n",
    "        avg_ndcg = np.mean([res[f'ndcg_cut_{i}'] for res in list(results.values())])\n",
    "        # print(f\"Avg NDCG@{i}: {avg_ndcg}\")\n",
    "        ndcgs.append(avg_ndcg.item())\n",
    "    \n",
    "\n",
    "    dataset['retrieved_documents'] = docs_retrieved\n",
    "    print(\"Recall@10\", recalls[-1])\n",
    "    print(\"Precision@10\", precisions[-1])\n",
    "    print(\"NDCG@10\", ndcgs[-1])\n",
    "    return recalls, precisions, ndcgs, dataset[['id', 'retrieved_documents']]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from collections.abc import Callable, Awaitable\n",
    "import numpy as np\n",
    "\n",
    "def eval_vir_framework_ind(dataset: pd.DataFrame, strategy: Callable, name: str, top_k_search=10):\n",
    "    print('='*30)\n",
    "    print(f\"{name}\")\n",
    "    print('='*30)\n",
    "\n",
    "    fnqrel = {}\n",
    "    fqrel = {}\n",
    "    frun = defaultdict(dict)\n",
    "\n",
    "    docs_retrieved = []\n",
    "\n",
    "    for idx, item in tqdm(dataset.iterrows()):\n",
    "        articles = ast.literal_eval(item['indicators'])  # ground truth: list of relevant articles\n",
    "        question = item[\"question\"]  # the question for search\n",
    "        possible_answer = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "        relevant_set = list(set(articles))\n",
    "\n",
    "        retrieved_set_1 = strategy(question, possible_answer, item['id'])\n",
    "        docs_retrieved.append(retrieved_set_1)\n",
    "        qrel = {}\n",
    "        for doc in relevant_set:\n",
    "            qrel[doc] = 1\n",
    "        \n",
    "        nqrel = {}\n",
    "        for doc in relevant_set:\n",
    "            nqrel[doc] = 2\n",
    "\n",
    "        fqrel[f'{idx}-query'] = qrel\n",
    "        fnqrel[f'{idx}-query'] = nqrel\n",
    "\n",
    "        for i in range(1, top_k_search+1):\n",
    "            run = {}\n",
    "            for doc, score in zip(retrieved_set_1[:i], [1 - 0.05 * x for x in range(10)]):\n",
    "                run[doc] = score\n",
    "            frun[str(i)][f'{idx}-query'] = run\n",
    "        \n",
    "        \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    ndcgs = []\n",
    "    for i in range(1, top_k_search+1):\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(fqrel, {f'recall.{i}'})\n",
    "        results = evaluator.evaluate(frun[str(i)])\n",
    "        avg_recall = np.mean([res[f'recall_{i}'] for res in list(results.values())])\n",
    "        # print(f\"Avg Recall@{i}: {avg_recall}\")\n",
    "        recalls.append(avg_recall.item())\n",
    "\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(fqrel, {f'P.{i}'})\n",
    "        results = evaluator.evaluate(frun[str(i)])\n",
    "        avg_precision = np.mean([res[f'P_{i}'] for res in list(results.values())])\n",
    "        # print(f\"Avg Precision@{i}: {avg_precision}\")\n",
    "        precisions.append(avg_precision.item())\n",
    "\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(fnqrel, {f'ndcg_cut.{i}', })\n",
    "        results = evaluator.evaluate(frun[str(i)])\n",
    "        avg_ndcg = np.mean([res[f'ndcg_cut_{i}'] for res in list(results.values())])\n",
    "        # print(f\"Avg NDCG@{i}: {avg_ndcg}\")\n",
    "        ndcgs.append(avg_ndcg.item())\n",
    "    \n",
    "\n",
    "    dataset['retrieved_documents'] = docs_retrieved\n",
    "    print(\"Recall@10\", recalls[-1])\n",
    "    print(\"Precision@10\", precisions[-1])\n",
    "    print(\"NDCG@10\", ndcgs[-1])\n",
    "    return recalls, precisions, ndcgs, dataset[['id', 'retrieved_documents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split 3 laws\n",
    "- split 3 inds\n",
    "- split 4 inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_law(df_split_3, strategy_1_law, '3-law')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_1_split_3_laws.csv', index=False)\n",
    "\n",
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_1_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_1_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_1_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_1_split_4_ind.csv', index=False)\n",
    "##########################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_law(df_split_3, strategy_2_law, '3-law')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_2_split_3_laws.csv', index=False)\n",
    "\n",
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_2_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_2_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_2_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_2_split_4_ind.csv', index=False)\n",
    "##########################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_law(df_split_3, strategy_3_law, '3-law')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_3_split_3_laws.csv', index=False)\n",
    "\n",
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_3_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_3_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_3_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_3_split_4_ind.csv', index=False)\n",
    "##########################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_law(df_split_3, strategy_4_law, '3-law')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_4_split_3_laws.csv', index=False)\n",
    "\n",
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_4_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_4_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_4_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_4_split_4_ind.csv', index=False)\n",
    "##########################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_law(df_split_4, strategy_5_law, '4-law')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_5_split_4_laws.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_law(df_split_3, strategy_5_law, '3-law')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_5_split_3_laws.csv', index=False)\n",
    "\n",
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_5_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_5_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_5_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_5_split_4_ind.csv', index=False)\n",
    "##########################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_law(df_split_3, strategy_6_law, '3-law')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_6_split_3_laws.csv', index=False)\n",
    "\n",
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_6_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_6_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_6_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_6_split_4_ind.csv', index=False)\n",
    "##########################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_7_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_7_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_7_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_7_split_4_ind.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_8_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_8_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_8_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_8_split_4_ind.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_9_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_9_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_9_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_9_split_4_ind.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_10_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_10_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_10_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_10_split_4_ind.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_11_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_11_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_11_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_11_split_4_ind.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_3, strategy_12_ind, '3-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_12_split_3_ind.csv', index=False)\n",
    "\n",
    "df_split_4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "recalls, precisions, ndcgs, retrieved = eval_vir_framework_ind(df_split_4, strategy_12_ind, '4-ind')\n",
    "print(recalls)\n",
    "print(precisions)\n",
    "print(ndcgs)\n",
    "retrieved.to_csv('../results/vir/vir_strat_12_split_4_ind.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proiect-NLP-Rutier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
