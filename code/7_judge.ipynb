{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_merge_df():\n",
    "    splits = [\"1_train\", \"1_test\", \"2\"]\n",
    "\n",
    "    specs = [\n",
    "        # (model_display_name, base_path_with_{split})\n",
    "        (\"o4-mini (best reasoning)\", \"../results/qa/qa_strat_6_split_{}.csv\"),\n",
    "        (\"Mistral (worst)\",     \"../results/qa_vllm/qa_strat_4_split_{}_vllm.csv\"),\n",
    "        (\"Gemma (best open)\",         \"../results/qa_vllm2/qa_strat_4_split_{}_vllm.csv\"),\n",
    "    ]\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for model_name, base in specs:\n",
    "        for split in splits:\n",
    "            base_path = base.format(split)\n",
    "            df = pd.read_csv(base_path)\n",
    "\n",
    "            sub = df[[\"id\", \"output_prompt\", \"exact_match\", \"input_prompt\"]].copy()\n",
    "            sub.columns = [\"id\", \"output_prompt\", \"exact_match\", \"input_prompt\"]\n",
    "            sub.insert(0, \"model\", model_name)\n",
    "            sub.insert(1, \"split\", split)\n",
    "\n",
    "            frames.append(sub)\n",
    "\n",
    "    agg = pd.concat(frames, ignore_index=True)\n",
    "    return agg\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def load_inputs():\n",
    "    df1 = load_merge_df()\n",
    "    qa_train = pd.read_csv(\"../data/dataset_V3/split_1_train.csv\", index_col=False)\n",
    "    qa_test  = pd.read_csv(\"../data/dataset_V3/split_1_test.csv\", index_col=False)\n",
    "    qa_test2  = pd.read_csv(\"../data/dataset_V3/split_2.csv\", index_col=False)\n",
    "    qa_refs = pd.concat([qa_train, qa_test, qa_test2], ignore_index=True)\n",
    "    qa_refs = qa_refs[[\"id\", \"question\", \"answers\" ,\"explanation\"]]\n",
    "    merged_df = pd.merge(df1, qa_refs, on=[\"id\"], how=\"inner\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eee6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_prompt_2 = \"\"\"Vei primi:\n",
    "- O întrebare grilă (răspuns multiplu) din testele auto în limba română.\n",
    "- Documentele legale folosite de o persoană pentru a răspunde.\n",
    "- Explicația și răspunsul final oferit de acea persoană.\n",
    "- Explicația oficială și răspunsul corect.\n",
    "\n",
    "Scopul tău este să identifici și să marchezi următoarele tipuri de erori (True/False), comparând <RaspunsOferit> cu <ExplicatieOficiala> și <DocumenteReferinta>. Nu inventa explicații noi și nu analiza concepte generale care nu apar în documente. Marchează <True> doar dacă există dovezi clare în text.\n",
    "\n",
    "Categoriile de erori:\n",
    "\n",
    "1. [Supra-interpretare]  \n",
    "Persoana introduce în raționament reguli, condiții sau consecințe care nu sunt prevăzute explicit în documentele legale.  \n",
    "Apare atunci când candidatul extrapolează dintr-o regulă reală pentru a susține o concluzie fără bază legală: extinde aplicabilitatea unei sancțiuni, inventează o condiție suplimentară sau tratează o formulare generală ca interdicție absolută.  \n",
    "**Tipare frecvente:** combinarea nejustificată a mai multor articole, adăugarea de distanțe/valori inexistente, presupunerea unor sancțiuni automate.  \n",
    "**Exemple:** a considerat că prezența unui polițist implică automat obligația de a opri; a extrapolat principiul general al siguranței la o interdicție neprevăzută; a impus o viteză minimă neprevăzută în text.  \n",
    "\n",
    "2. [Neglijare]  \n",
    "Persoana nu ia în considerare informații relevante sau excepții cruciale din documente ori din.  \n",
    "A consultat textele corecte dar a ignorat condiții esențiale (excepții, restricții, situații speciale).  \n",
    "**Tipare frecvente:** omiterea excepțiilor legale (tramvaie, situații speciale), ignorarea condițiilor cumulative, omisiunea diferențierii între categorii de vehicule sau drumuri.  \n",
    "**Exemple:** a ignorat excepția care permite depășirea tramvaielor fără refugiu; a neglijat condițiile pentru priorități în zone rezidențiale; a citit corect un articol dar a omis altul complementar.\n",
    "\n",
    "3. [Interpretare eronată]  \n",
    "Persoana citește sau aplică greșit prevederile legale existente, fără a adăuga reguli noi sau a confunda concepte.  \n",
    "Greșeala apare din interpretarea incorectă a textului: aplicare literală fără context, ignorarea priorității între reguli, citire selectivă.  \n",
    "**Tipare frecvente:** aplicarea marcajului continuu ca interdicție absolută, neînțelegerea priorității între semnal și indicator, interpretarea eronată a duratelor de suspendare.  \n",
    "**Exemple:** a considerat că marcajul continuu interzice orice depășire; a ignorat faptul că indicatorul „înainte sau la dreapta” rămâne valabil la semnal verde; a aplicat sancțiuni din altă categorie.\n",
    "\n",
    "4. [Valori]  \n",
    "Persoana aplică greșit o valoare numerică, un prag legal sau un standard tehnic prevăzut în textele normative.  \n",
    "Eroarea nu este doar de lectură, ci de aplicare incorectă a pragurilor sau standardelor (viteze, distanțe, mase, procente, puncte etc.) sau folosirea valorii corecte în regimul juridic greșit.  \n",
    "**Tipare frecvente:** confundarea limitelor de viteză între drumuri expres/E, aplicarea unei distanțe greșite (25m vs 50m), folosirea valorilor valabile pentru alte categorii de vehicule, inversarea punctelor sau aplicarea valorilor corecte în context greșit.  \n",
    "**Exemple:** a considerat că limita pe drumurile expres este 100 km/h (valoare pentru drumuri E); a tratat „la mai puțin de 25 m” ca „la mai puțin de 50 m”; a atribuit 9 puncte în loc de 6; a aplicat masa maximă admisă din categoria N la vehicule B.  \n",
    "**Diferență față de „Lipsă de atenție la detalii”**: la Valori, persoana aplică activ o valoare greșită; la Lipsă de atenție, valoarea e corectă în document dar a fost ignorată sau citită superficial.\n",
    "\n",
    "5. [Recomandări]  \n",
    "Persoana confundă caracterul juridic al unei prevederi: tratează o recomandare ca pe o obligație legală strictă sau, invers, minimalizează o recomandare relevantă pentru răspunsul corect.  \n",
    "Eroarea apare prin neînțelegerea expresiilor „de regulă”, „se recomandă”, „în mod normal”, care nu au același statut juridic ca obligațiile imperative.  \n",
    "**Tipare frecvente:** interpretarea „de regulă” ca regulă absolută; eliminarea variantelor formulate ca recomandări; ignorarea recomandărilor relevante pentru răspuns; tratarea recomandărilor ca neimportante în fața obligațiilor.  \n",
    "**Exemple:** a tratat recomandarea de a folosi luminile ziua pe drumurile naționale ca obligație legală strictă; a ignorat recomandarea de semnalizare în lipsa altor vehicule; a considerat „de regulă” ca o normă fără excepții.\n",
    "\n",
    "Instrucțiuni (versiune îmbunătățită):\n",
    "- Obiectiv: explică *de ce* <RaspunsOferit> este greșit (mecanismul erorii) raportat la <DocumenteReferinta>, NU „față de <ExplicatieOficiala>”.\n",
    "\n",
    "- Ordinea de lucru:\n",
    "  1) Extrage regulile/condițiile relevante din <DocumenteReferinta>.\n",
    "  2) Extrage afirmațiile-cheie din <RaspunsOferit>.\n",
    "  3) Compară (2) cu (1) și identifică mecanismul erorii (categoria potrivită).\n",
    "  4) Folosește <ExplicatieOficiala> DOAR pentru orientare (ca să înțelegi care e concluzia corectă și ce articole sunt relevante), NU ca probă în explicație.\n",
    "\n",
    "- Evidență necesară pentru <True>:\n",
    "  Marchează <True> la o categorie numai dacă poți formula o frază scurtă în care:\n",
    "  a) indici concret ce susține <RaspunsOferit>, și\n",
    "  b) arăți scurt ce prevăd textele din <DocumenteReferinta> (număr articol/condiție/valoare/direcție) care contrazic, restrâng sau nuanțează acea susținere.\n",
    "  Dacă nu există astfel de dovezi în <DocumenteReferinta>, marchează <False> chiar dacă <ExplicatieOficiala> indică altceva.\n",
    "\n",
    "- Interdicții explicite:\n",
    "  • NU cita, NU rezuma și NU invoca <ExplicatieOficiala> în <explicatie>.\n",
    "  • NU scrie „conform/după/în explicația oficială…”, „nu a urmat <ExplicatieOficiala>”, „răspunsul corect este X în <ExplicatieOficiala>”.\n",
    "  • NU inventa reguli, articole sau condiții care nu apar în <DocumenteReferinta>.\n",
    "\n",
    "- Stilul explicațiilor:\n",
    "  • 1–2 fraze per criteriu, neutre și factuale, fără meta-comentarii.\n",
    "  • Folosește, când e posibil, referințe minimale la articol/alin./lit. din <DocumenteReferinta> (ex.: „art. 120 lit. b) interzice…”, „Regulament-123 h) condiționează de…”).\n",
    "  • Evită formulări vagi de tipul „textul sugerează”, „pare că”, „probabil”.\n",
    "\n",
    "- Când pui <False>:\n",
    "  • Documentele nu acoperă situația sau nu contrazic clar <RaspunsOferit>.\n",
    "  • Diferența e vizibilă doar în <ExplicatieOficiala>, nu și în <DocumenteReferinta>.\n",
    "  • Există ambiguitate rezonabilă între text și răspuns.\n",
    "\n",
    "- Format de ieșire:\n",
    "  • Răspunde STRICT în XML, o singură secțiune <verdict>, fără text suplimentar.\n",
    "  • NU genera niciodată JSON.\n",
    "  • Pentru fiecare <criteriu>: <categorie>, <explicatie> (1–2 fraze bazate pe <DocumenteReferinta>), <valoare>True/False</valoare> (fix aceste forme).\n",
    "\n",
    "- NU FOLOSI NICIODATA: \"conform explicatiei oficiale\"\n",
    "  \n",
    "- Exemple rapide (doar ca ghid intern; NU le include în output):\n",
    "  ✔ Acceptat: „A tratat interdicția ca absolută, deși art. 120 prevede excepția …; eroare de Generalizare.”\n",
    "  ✘ Respins: „Persoana nu a luat în considerare că răspunsul corect este C, conform explicației oficiale.”, \"Persoana a omis să observe că explicația oficială menționează explicit\"\n",
    "  Exemplele respinse arata faptul ca nu trebuie sa compari in mod direct raspunsul si explicatia oficiala. Persoana nu a avut acces la explicatia oficiala cand a dat raspunsul.\n",
    "\n",
    "Iată detaliile de analizat:\n",
    "\n",
    "<Intrebare>\n",
    "{question}\n",
    "</Intrebare>\n",
    "\n",
    "<VarianteRaspuns>\n",
    "{options_refs}\n",
    "</VarianteRaspuns>\n",
    "\n",
    "<VarianteCorecte>\n",
    "{options_correct}\n",
    "</VarianteCorecte>\n",
    "\n",
    "<DocumenteReferinta>\n",
    "{documents_ref}\n",
    "</DocumenteReferinta>\n",
    "\n",
    "<RaspunsOferit>\n",
    "{answer}\n",
    "</RaspunsOferit>\n",
    "\n",
    "<RaspunsCorect>\n",
    "{is_correct}\n",
    "</RaspunsCorect>\n",
    "\n",
    "<ExplicatieOficiala>\n",
    "{answer_ref}\n",
    "</ExplicatieOficiala>\n",
    "\n",
    "Returnează rezultatul în structura:\n",
    "\n",
    "<verdict>\n",
    "  <criteriu id=\"1\">\n",
    "    <categorie>Supra-interpretare</categorie>\n",
    "    <explicatie>...</explicatie>\n",
    "    <valoare>True/False</valoare>\n",
    "  </criteriu>\n",
    "  <criteriu id=\"2\">\n",
    "    <categorie>Neglijare</categorie>\n",
    "    <explicatie>...</explicatie>\n",
    "    <valoare>True/False</valoare>\n",
    "  </criteriu>\n",
    "  <criteriu id=\"3\">\n",
    "    <categorie>Interpretare eronată</categorie>\n",
    "    <explicatie>...</explicatie>\n",
    "    <valoare>True/False</valoare>\n",
    "  </criteriu>\n",
    "  <criteriu id=\"4\">\n",
    "    <categorie>Valori</categorie>\n",
    "    <explicatie>...</explicatie>\n",
    "    <valoare>True/False</valoare>\n",
    "  </criteriu>\n",
    "  <criteriu id=\"5\">\n",
    "    <categorie>Recomandări</categorie>\n",
    "    <explicatie>...</explicatie>\n",
    "    <valoare>True/False</valoare>\n",
    "  </criteriu>\n",
    "</verdict>\n",
    "\n",
    "===========\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e91edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(item):\n",
    "    # print(item)\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "    correct_answers = [entry['answer_text'][0] for entry in ast.literal_eval(item['answers']) if entry['is_correct']]\n",
    "\n",
    "    documents = item[\"input_prompt\"]\n",
    "    documents = documents.split(\"Aceastea sunt legile relevante, dar nu neaparat toate sunt relevante:\")[1]\n",
    "    documents = documents.split(\"===================\")[0]\n",
    "    input_prompt = llm_judge_prompt_2.format(\n",
    "        question=item[\"question\"],\n",
    "        options_refs=answers,\n",
    "        options_correct=correct_answers,\n",
    "        documents_ref=documents,\n",
    "        answer=item[\"output_prompt\"],\n",
    "        is_correct=\"Raspunsul final este corect\" if item[\"exact_match\"] else \"Raspunsul final este incorect\",\n",
    "        answer_ref=item[\"explanation\"]\n",
    "    )\n",
    "    return input_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b4f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [18:09<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      model    split        id  \\\n",
      "0  o4-mini (best reasoning)  1_train  0641a4e4   \n",
      "1  o4-mini (best reasoning)  1_train  376f1dc6   \n",
      "2  o4-mini (best reasoning)  1_train  7f68bad6   \n",
      "3  o4-mini (best reasoning)  1_train  e0f85457   \n",
      "4  o4-mini (best reasoning)  1_train  ee8de379   \n",
      "\n",
      "                                              result  \n",
      "0  ```xml\\n<verdict>\\n  <criteriu id=\"1\">\\n    <c...  \n",
      "1  ```xml\\n<verdict>\\n  <criteriu id=\"1\">\\n    <c...  \n",
      "2  ```xml\\n<verdict>\\n  <criteriu id=\"1\">\\n    <c...  \n",
      "3  ```xml\\n<verdict>\\n  <criteriu id=\"1\">\\n    <c...  \n",
      "4  ```xml\\n<verdict>\\n  <criteriu id=\"1\">\\n    <c...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:8889/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"x\"\n",
    "\n",
    "SAVE_PATH = \"./llm_judge_out_2/gemma-3-27b-it.csv\"\n",
    "\n",
    "def call_llm(sys_prompt):\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"google/gemma-3-27b-it\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": sys_prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=2000,\n",
    "            seed=25\n",
    "            # seed=46 \n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "# def mass_qa_runner(data: pd.DataFrame, save_path: str = SAVE_PATH):\n",
    "#     # Load existing results if they exist\n",
    "#     if os.path.exists(save_path):\n",
    "#         results_df = pd.read_csv(save_path)\n",
    "#     else:\n",
    "#         results_df = pd.DataFrame(columns=[\"model\", \"split\", \"id\", \"result\"])\n",
    "\n",
    "#     # Use tuple (model, split, id) as unique identifier\n",
    "#     processed_keys = set(\n",
    "#         tuple(row) for row in results_df[[\"model\", \"split\", \"id\"]].to_numpy()\n",
    "#     )\n",
    "\n",
    "#     for real_idx, (idx, item) in tqdm.tqdm(enumerate(data.iterrows()), total=len(data)):\n",
    "#         key = (item[\"model\"], item[\"split\"], item[\"id\"])\n",
    "\n",
    "#         if key in processed_keys:\n",
    "#             continue  # Skip already processed\n",
    "\n",
    "#         prompt = judge(item)  # assumes you have a judge() function\n",
    "#         result = call_llm(prompt)\n",
    "\n",
    "#         # Append to results if successful\n",
    "#         if result is not None:\n",
    "#             new_row = {\n",
    "#                 \"model\": item[\"model\"],\n",
    "#                 \"split\": item[\"split\"],\n",
    "#                 \"id\": item[\"id\"],\n",
    "#                 \"result\": result,\n",
    "#             }\n",
    "#             results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "#             # Save intermediate progress\n",
    "#             results_df.to_csv(save_path, index=False)\n",
    "\n",
    "#             # Update processed set\n",
    "#             processed_keys.add(key)\n",
    "\n",
    "#     return results_df\n",
    "\n",
    "\n",
    "def process_row(idx, item, processed_keys):\n",
    "    key = (item[\"model\"], item[\"split\"], item[\"id\"])\n",
    "    if key in processed_keys:\n",
    "        return None\n",
    "\n",
    "    prompt = judge(item)\n",
    "    result = call_llm(prompt)\n",
    "\n",
    "    if result is None:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"model\": item[\"model\"],\n",
    "        \"split\": item[\"split\"],\n",
    "        \"id\": item[\"id\"],\n",
    "        \"result\": result,\n",
    "    }\n",
    "\n",
    "def run_parallel(data, save_path=SAVE_PATH, max_workers=64):\n",
    "    # Load existing results if they exist\n",
    "    if os.path.exists(save_path):\n",
    "        results_df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        results_df = pd.DataFrame(columns=[\"model\", \"split\", \"id\", \"result\"])\n",
    "\n",
    "    # Use tuple (model, split, id) as unique identifier\n",
    "    processed_keys = set(\n",
    "        tuple(row) for row in results_df[[\"model\", \"split\", \"id\"]].to_numpy()\n",
    "    )\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_row, idx, item, processed_keys): (idx, item)\n",
    "            for idx, item in data.iterrows()\n",
    "        }\n",
    "\n",
    "        for future in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
    "            row = future.result()\n",
    "            if row is not None:\n",
    "                results_df = pd.concat([results_df, pd.DataFrame([row])], ignore_index=True)\n",
    "                results_df.to_csv(save_path, index=False)  # save progress incrementally\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Example usage\n",
    "merged_df = load_inputs()\n",
    "merged_df = merged_df[merged_df[\"exact_match\"] == False]\n",
    "result_df = run_parallel(merged_df)\n",
    "print(result_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32851275",
   "metadata": {},
   "source": [
    "# K Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- Load Data ---\n",
    "SAVE_PATH = \"./llm_gemma_redhat_judge_7.csv\"\n",
    "df = pd.read_csv(SAVE_PATH)\n",
    "\n",
    "# --- Split 'result' column by newlines into multiple rows ---\n",
    "expanded_rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.isna(row['result']):\n",
    "        continue\n",
    "    for sentence in str(row['result']).split('\\n'):\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:  # avoid empty lines\n",
    "            expanded_rows.append({\n",
    "                'model': row['model'],\n",
    "                'split': row['split'],\n",
    "                'id': row['id'],\n",
    "                'result': sentence\n",
    "            })\n",
    "\n",
    "df_expanded = pd.DataFrame(expanded_rows)\n",
    "print(f\"Original rows: {len(df)}, Expanded rows: {len(df_expanded)}\")\n",
    "print(df_expanded.head())\n",
    "\n",
    "# --- Load SentenceTransformer model ---\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "# --- Embed the 'result' column ---\n",
    "texts = df_expanded['result'].tolist()\n",
    "embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# --- Run KMeans ---\n",
    "num_clusters = 15\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "df_expanded['cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# --- Inspect cluster distribution ---\n",
    "print(\"\\nCluster counts:\")\n",
    "print(df_expanded['cluster'].value_counts())\n",
    "\n",
    "# --- Print sentences grouped by cluster ---\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "for c in sorted(df_expanded['cluster'].unique()):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    cluster_sentences = df_expanded[df_expanded['cluster'] == c]['result']\n",
    "    for i, sentence in enumerate(cluster_sentences, 1):\n",
    "        print(f\"{i}. {sentence}\")\n",
    "\n",
    "# --- Optionally, save clustered results ---\n",
    "df_expanded.to_csv(\"./llm_gemma_redhat_judge_7_clustered_by_line.csv\", index=False)\n",
    "print(\"\\nClustered CSV saved to llm_gemma_redhat_judge_7_clustered_by_line.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
