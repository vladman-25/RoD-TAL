{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6ef339",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [ ] analize metrici vir/qa per categorie indicator??\n",
    "- [ ] numar de reasoning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c35b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1 = '../data/dataset_V3/split_1.csv'\n",
    "file2 = '../data/dataset_V3/split_2.csv'\n",
    "file3 = '../data/dataset_V3/split_3.csv'\n",
    "file4 = '../data/dataset_V3/split_4.csv'\n",
    "\n",
    "df1 = pd.read_csv(file1, index_col='id')\n",
    "df2 = pd.read_csv(file2, index_col='id')\n",
    "df3 = pd.read_csv(file3, index_col='id')\n",
    "df4 = pd.read_csv(file4, index_col='id')\n",
    "\n",
    "df_all = pd.concat([df1, df2, df3, df4])\n",
    "\n",
    "file1 = '../data/dataset_V3/corpus.csv'\n",
    "file2 = '../data/dataset_V3/corpus_indicators.csv'\n",
    "\n",
    "\n",
    "df_corpus = pd.read_csv(file1, index_col='id')\n",
    "df_corpus_indicators = pd.read_csv(file2, index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b94be",
   "metadata": {},
   "source": [
    "## Dsitrubtion of number of legal articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef967fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "# Convert string to list, then count number of articles\n",
    "num_articles = (\n",
    "    df_all['legislation']\n",
    "    .apply(lambda x: len(ast.literal_eval(x)))\n",
    ")\n",
    "\n",
    "# Exclude entries where the number of articles is zero\n",
    "num_articles = num_articles[num_articles > 0]\n",
    "\n",
    "# ---- Polished Plot ----\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 25,\n",
    "    \"ytick.labelsize\": 25,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7), constrained_layout=True)\n",
    "bars = ax.hist(\n",
    "    num_articles, \n",
    "    bins=range(1, num_articles.max() + 2), \n",
    "    edgecolor='black', \n",
    "    align='left', \n",
    "    color='#3572b0'\n",
    ")\n",
    "\n",
    "# Annotate bars with values\n",
    "for patch, value in zip(bars[2], bars[0]):\n",
    "    if value > 0:\n",
    "        ax.annotate(\n",
    "            int(value), \n",
    "            (patch.get_x() + patch.get_width() / 2, value),\n",
    "            ha='center', va='bottom', fontsize=25\n",
    "        )\n",
    "\n",
    "ax.set_title('Distribution of Number of Legal Articles')\n",
    "ax.set_xlabel('Number of Legal Articles')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# Clean spines and add grid\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "plt.xticks(range(1, num_articles.max() + 1))\n",
    "plt.savefig('../plots/new_font/distribution_legal_articles.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22cbea",
   "metadata": {},
   "source": [
    "## Distrubtion of number of indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088595f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "def safe_len(x):\n",
    "    if pd.isna(x):\n",
    "        return 0\n",
    "    try:\n",
    "        return len(ast.literal_eval(x))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "num_articles = df_all['indicators'].apply(safe_len)\n",
    "num_articles = num_articles[num_articles > 0]\n",
    "bins = range(1, num_articles.max() + 2)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 25,\n",
    "    \"ytick.labelsize\": 25,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7), constrained_layout=True)\n",
    "bars = ax.hist(\n",
    "    num_articles,\n",
    "    bins=bins,\n",
    "    edgecolor='black',\n",
    "    align='left',\n",
    "    color='#3572b0'\n",
    ")\n",
    "\n",
    "# Annotate bars\n",
    "for patch, value in zip(bars[2], bars[0]):\n",
    "    if value > 0:\n",
    "        ax.annotate(\n",
    "            int(value),\n",
    "            (patch.get_x() + patch.get_width() / 2, value),\n",
    "            ha='center', va='bottom', fontsize=25\n",
    "        )\n",
    "\n",
    "ax.set_title('Distribution of Number of Indicators')\n",
    "ax.set_xlabel('Number of Indicators')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "plt.xticks(range(1, num_articles.max() + 1))\n",
    "plt.savefig('../plots/new_font/distribution_indicators.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31421347",
   "metadata": {},
   "source": [
    "## Distribution of Questions per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file1 = '../data/dataset_V3/categories_stats.csv'\n",
    "df_stats = pd.read_csv(file1)\n",
    "\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "\n",
    "df_stats['category_name_en'] = df_stats['category_name'].map(translations)\n",
    "\n",
    "# Sort by question count for better readability\n",
    "df_sorted = df_stats.sort_values(\"question_count\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 25,\n",
    "    \"ytick.labelsize\": 25,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8), constrained_layout=True)\n",
    "bars = ax.barh(\n",
    "    df_sorted['category_name_en'],\n",
    "    df_sorted['question_count'],\n",
    "    color=\"#3572b0\"\n",
    ")\n",
    "\n",
    "# Annotate bars with counts\n",
    "for i, (bar, value) in enumerate(zip(bars, df_sorted['question_count'])):\n",
    "    ax.text(value + 2, bar.get_y() + bar.get_height() / 2, str(value),\n",
    "            va='center', ha='left', fontsize=25)\n",
    "\n",
    "ax.set_xlabel('Number of Questions')\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('Distribution of Questions per Category')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "plt.savefig('../plots/new_font/distribution_questions_per_category.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c0a76",
   "metadata": {},
   "source": [
    "## Distribution of visual question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_all is already defined as in your code\n",
    "\n",
    "# Drop NaNs and get counts, then sort for readability\n",
    "category_counts = df_all['category'].dropna().value_counts().sort_values()\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8), constrained_layout=True)\n",
    "bars = ax.barh(category_counts.index, category_counts.values, color=\"#3572b0\")\n",
    "\n",
    "# Add value annotations\n",
    "for bar in bars:\n",
    "    ax.text(\n",
    "        bar.get_width() + max(category_counts.values) * 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        str(int(bar.get_width())),\n",
    "        va='center',\n",
    "        fontsize=13\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Category')\n",
    "ax.set_title('Distribution of Visual Questions by Category')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "plt.savefig('../plots/final_font/distribution_category.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc331fc",
   "metadata": {},
   "source": [
    "## Distribution of Categories in Traffic Signs categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assume df_corpus_indicators is loaded as in your code\n",
    "indicator_translation = {\n",
    "    \"Indicatoare de avertizare\": \"Warning signs\",\n",
    "    \"Indicatoare de prioritate\": \"Priority signs\",\n",
    "    \"Indicatoare de interzicere sau restrictie\": \"Prohibition or restriction signs\",\n",
    "    \"Indicatoare de obligare\": \"Mandatory signs\",\n",
    "    \"Indicatoare de orientare\": \"Directional signs\",\n",
    "    \"Indicatoare de informare\": \"Information signs\",\n",
    "    \"Panouri aditionale\": \"Additional panels\",\n",
    "    \"Semnale luminoase la trecerile la nivel…\": \"Traffic lights at level crossings\",\n",
    "    \"Marcaje Longitudinale\": \"Longitudinal markings\",\n",
    "    \"Marcaje Transversale\": \"Transversal markings\",\n",
    "    \"Indicatoare rutiere temporare\": \"Temporary traffic signs\",\n",
    "}\n",
    "\n",
    "\n",
    "# Drop NaNs and count the occurrences of each category\n",
    "df_corpus_indicators['category_en'] = df_corpus_indicators['category'].map(indicator_translation)\n",
    "\n",
    "# Drop NaNs and count the occurrences of each category (sorted for clarity)\n",
    "category_counts = df_corpus_indicators['category_en'].dropna().value_counts().sort_values()\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 25,\n",
    "    \"ytick.labelsize\": 25,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8), constrained_layout=True)\n",
    "bars = ax.barh(category_counts.index, category_counts.values, color=\"#3572b0\")\n",
    "\n",
    "# Annotate each bar with its count\n",
    "for bar in bars:\n",
    "    ax.text(\n",
    "        bar.get_width() + max(category_counts.values) * 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        str(int(bar.get_width())),\n",
    "        va='center',\n",
    "        fontsize=25\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Category')\n",
    "ax.set_title('Distribution of Categories in Traffic Signs (English)')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "plt.savefig('../plots/new_font/distribution_corpus_indicators_category_horizontal.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4ad0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4669e638",
   "metadata": {},
   "source": [
    "## cstegories side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7848d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the splits\n",
    "split_paths = [\n",
    "    '../data/dataset_V3/split_1.csv',\n",
    "    '../data/dataset_V3/split_2.csv',\n",
    "    '../data/dataset_V3/split_3.csv',\n",
    "    '../data/dataset_V3/split_4.csv'\n",
    "]\n",
    "dfs = [pd.read_csv(path) for path in split_paths]\n",
    "\n",
    "# Load categories and get id->name mapping (English)\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "\n",
    "# Get sorted category ids and names for plotting order\n",
    "cat_ids = cat_stats['question_category_id'].tolist()\n",
    "cat_names = [cat_map[cid] for cid in cat_ids]\n",
    "\n",
    "split_counts = []\n",
    "for df in dfs:\n",
    "    counts = df['question_category_id'].value_counts().reindex(cat_ids, fill_value=0)\n",
    "    split_counts.append(counts.values)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "bar_width = 0.18\n",
    "index = np.arange(len(cat_names))\n",
    "colors = [\"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\"]  # You can customize\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8), constrained_layout=True)\n",
    "\n",
    "for i, counts in enumerate(split_counts):\n",
    "    ax.bar(index + i*bar_width, counts, width=bar_width, color=colors[i], label=f'Split {i+1}')\n",
    "\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Number of Entries')\n",
    "ax.set_title('Distribution by Category for Each Split')\n",
    "ax.set_xticks(index + bar_width*1.5)\n",
    "ax.set_xticklabels(cat_names, rotation=30, ha='right')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('../plots/final_font/distribution_by_category_per_split.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b429c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load splits 3 and 4\n",
    "split3 = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "split4 = pd.read_csv('../data/dataset_V3/split_4.csv')\n",
    "\n",
    "# Get all unique categories from both splits, sorted by total frequency for nice plotting\n",
    "categories = pd.concat([split3['category'], split4['category']]).dropna().value_counts().index.tolist()\n",
    "\n",
    "# Count occurrences in each split, aligning to the full set of categories\n",
    "counts3 = split3['category'].value_counts().reindex(categories, fill_value=0)\n",
    "counts4 = split4['category'].value_counts().reindex(categories, fill_value=0)\n",
    "\n",
    "# Bar plot parameters\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(categories))\n",
    "colors = [\"#81b29a\", \"#f2cc8f\"]  # Blue and orange\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 25,\n",
    "    \"ytick.labelsize\": 25,\n",
    "    \"legend.fontsize\": 25,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8), constrained_layout=True)\n",
    "\n",
    "bars1 = ax.bar(index - bar_width/2, counts3.values, width=bar_width, color=colors[0], label='Split 3')\n",
    "bars2 = ax.bar(index + bar_width/2, counts4.values, width=bar_width, color=colors[1], label='Split 4')\n",
    "\n",
    "# Optional: add count annotations\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{int(height)}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "                    xytext=(0,3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=25)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{int(height)}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "                    xytext=(0,3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=25)\n",
    "\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Number of Entries')\n",
    "ax.set_title('Distribution of Visual Categories for Splits 3 and 4')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(categories, ha='right')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('../plots/new_font/distribution_category_split3_split4.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af64c074",
   "metadata": {},
   "source": [
    "## token bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset_V3/corpus.csv\", index_col=\"id\")\n",
    "laws = {}\n",
    "for idx, row in df.iterrows():\n",
    "    laws[idx] = f\"{row['title_metadata']} | {row['content']}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "\n",
    "def special_bins(length):\n",
    "    if length < 300:\n",
    "        # Bins of 100 until 299\n",
    "        bin_start = (length // 100) * 100\n",
    "        bin_end = bin_start + 99\n",
    "    elif 300 <= length <= 384:\n",
    "        bin_start, bin_end = 300, 384\n",
    "    elif 385 <= length <= 499:\n",
    "        bin_start, bin_end = 385, 499\n",
    "    else:\n",
    "        # Standard bins of 100 after 500\n",
    "        bin_start = (length // 100) * 100\n",
    "        bin_end = bin_start + 99\n",
    "    return (bin_start, bin_end)\n",
    "\n",
    "def text_length_distribution(text_dict, tokenizer):\n",
    "    lengths = [len(tokenizer.encode(content, add_special_tokens=True))\n",
    "               for content in text_dict.values()]\n",
    "    bins = defaultdict(int)\n",
    "    for length in lengths:\n",
    "        bins[special_bins(length)] += 1\n",
    "    sorted_bins = dict(sorted(bins.items()))\n",
    "    return sorted_bins\n",
    "\n",
    "dist = text_length_distribution(laws, tokenizer)\n",
    "for bin_range, count in dist.items():\n",
    "    print(f\"Tokens {bin_range[0]}–{bin_range[1]}: {count} texts\")\n",
    "\n",
    "####\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [f\"{start}-{end}\" for (start, end) in dist.keys()]\n",
    "counts = list(dist.values())\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 22,\n",
    "    \"xtick.labelsize\": 22,\n",
    "    \"ytick.labelsize\": 22,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7), constrained_layout=True)\n",
    "bars = ax.bar(labels, counts, color=\"#3572b0\", edgecolor='black')\n",
    "\n",
    "# Annotate bars\n",
    "for bar, value in zip(bars, counts):\n",
    "    if value > 0:\n",
    "        ax.annotate(\n",
    "            str(value),\n",
    "            (bar.get_x() + bar.get_width() / 2, value),\n",
    "            ha='center', va='bottom', fontsize=22\n",
    "        )\n",
    "\n",
    "ax.set_xlabel('Token Length (bins)')\n",
    "ax.set_ylabel('Number of Texts')\n",
    "ax.set_title('Distribution of Law Document Lengths (Tokens)')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Fix overlapping labels: rotate and align x-tick labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=35, ha='right', rotation_mode='anchor')\n",
    "\n",
    "# Add padding to the bottom so labels aren't cut off\n",
    "plt.subplots_adjust(bottom=0.22)\n",
    "\n",
    "plt.savefig('../plots/new_font/token_length_distribution.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "split_paths = [\n",
    "    '../data/dataset_V3/split_1.csv',\n",
    "    '../data/dataset_V3/split_2.csv',\n",
    "    '../data/dataset_V3/split_3.csv',\n",
    "    '../data/dataset_V3/split_4.csv'\n",
    "]\n",
    "dfs = [pd.read_csv(p, index_col=\"id\") for p in split_paths]\n",
    "df_corpus = pd.read_csv(\"../data/dataset_V3/corpus.csv\", index_col=\"id\")\n",
    "\n",
    "used_legislation_ids = set()\n",
    "for df in dfs:\n",
    "    for items in df['legislation'].dropna():\n",
    "        ids = ast.literal_eval(items)\n",
    "        used_legislation_ids.update(ids)\n",
    "\n",
    "filtered_corpus = df_corpus.loc[df_corpus.index.astype(str).isin(used_legislation_ids)]\n",
    "\n",
    "laws = {}\n",
    "for idx, row in filtered_corpus.iterrows():\n",
    "    laws[idx] = f\"{row['title_metadata']} | {row['content']}\"\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "\n",
    "def special_bins(length):\n",
    "    if length < 300:\n",
    "        bin_start = (length // 100) * 100\n",
    "        bin_end = bin_start + 99\n",
    "    elif 300 <= length <= 384:\n",
    "        bin_start, bin_end = 300, 384\n",
    "    elif 385 <= length <= 499:\n",
    "        bin_start, bin_end = 385, 499\n",
    "    else:\n",
    "        bin_start = (length // 100) * 100\n",
    "        bin_end = bin_start + 99\n",
    "    return (bin_start, bin_end)\n",
    "\n",
    "def text_length_distribution(text_dict, tokenizer):\n",
    "    lengths = [len(tokenizer.encode(content, add_special_tokens=True))\n",
    "               for content in text_dict.values()]\n",
    "    bins = defaultdict(int)\n",
    "    for length in lengths:\n",
    "        bins[special_bins(length)] += 1\n",
    "    sorted_bins = dict(sorted(bins.items()))\n",
    "    return sorted_bins\n",
    "\n",
    "dist = text_length_distribution(laws, tokenizer)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [f\"{start}-{end}\" for (start, end) in dist.keys()]\n",
    "counts = list(dist.values())\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 22,\n",
    "    \"xtick.labelsize\": 22,\n",
    "    \"ytick.labelsize\": 22,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7), constrained_layout=True)\n",
    "bars = ax.bar(labels, counts, color=\"#3572b0\", edgecolor='black')\n",
    "\n",
    "# Annotate bars\n",
    "for bar, value in zip(bars, counts):\n",
    "    if value > 0:\n",
    "        ax.annotate(\n",
    "            str(value),\n",
    "            (bar.get_x() + bar.get_width() / 2, value),\n",
    "            ha='center', va='bottom', fontsize=22\n",
    "        )\n",
    "\n",
    "ax.set_xlabel('Token Length (bins)')\n",
    "ax.set_ylabel('Number of Documents')\n",
    "ax.set_title('Distribution of Token Lengths for Used Legal Articles')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=35, ha='right', rotation_mode='anchor')\n",
    "plt.subplots_adjust(bottom=0.22)\n",
    "\n",
    "plt.savefig('../plots/new_font/token_length_distribution_used_legislation.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630c6d2",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d962e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate token lengths for each used article\n",
    "token_lengths = {}\n",
    "for doc_id, text in laws.items():\n",
    "    token_len = len(tokenizer.encode(text, add_special_tokens=True))\n",
    "    token_lengths[str(doc_id)] = token_len  # Ensure IDs are strings for comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_with_long_article = 0\n",
    "for df in dfs:\n",
    "    for items in df['legislation'].dropna():\n",
    "        ids = ast.literal_eval(items)\n",
    "        if any(token_lengths.get(str(id_), 0) > 384 for id_ in ids):\n",
    "            question_with_long_article += 1\n",
    "\n",
    "print(f\"Number of questions referencing at least one article with token length > 384: {question_with_long_article}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_questions = sum(len(df) for df in dfs)\n",
    "percentage = (question_with_long_article / total_questions) * 100\n",
    "print(f\"({question_with_long_article}/{total_questions}) = {percentage:.2f}% of questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "split3 = pd.read_csv('../results/qa/qa_strat_3_split_1_train.csv')\n",
    "split4 = pd.read_csv('../results/qa/qa_strat_4_split_1_train.csv')\n",
    "\n",
    "# Add suffixes to distinguish columns after merge\n",
    "merged = split3.merge(split4, on='id', suffixes=('_3', '_4'))\n",
    "\n",
    "# Normalize 'exact_match' to bool (in case they're strings)\n",
    "merged['exact_match_3'] = merged['exact_match_3'].astype(str) == 'True'\n",
    "merged['exact_match_4'] = merged['exact_match_4'].astype(str) == 'True'\n",
    "\n",
    "# Find switches\n",
    "false_to_true = merged[(~merged['exact_match_3']) & (merged['exact_match_4'])]\n",
    "true_to_false = merged[(merged['exact_match_3']) & (~merged['exact_match_4'])]\n",
    "\n",
    "print(f\"Count switched from False to True: {false_to_true.shape[0]}\")\n",
    "# print(false_to_true[['id', 'input_prompt_3', 'output_prompt_3', 'qa_result_3', 'correct_answers_3', 'exact_match_3', 'exact_match_4']])\n",
    "\n",
    "print(f\"\\nCount switched from True to False: {true_to_false.shape[0]}\")\n",
    "# print(true_to_false[['id', 'input_prompt_3', 'output_prompt_3', 'qa_result_3', 'correct_answers_3', 'exact_match_3', 'exact_match_4']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6030e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "split3 = pd.read_csv('../results/vqa/vqa_strat_8_split_3.csv')\n",
    "actual = pd.read_csv('../data/dataset_V3/split_3.csv')\n",
    "\n",
    "# Merge on 'id' (add suffixes to avoid column name collisions)\n",
    "merged = pd.merge(split3, actual[['id', 'category', 'image']], on='id')\n",
    "\n",
    "# Filter for rows where exact_march is False\n",
    "filtered = merged[merged['exact_match'] == False]\n",
    "\n",
    "# For each unique actual category, sample (up to) 3 entries and print as before\n",
    "for category, group in filtered.groupby('category'):\n",
    "    print(f\"Category: {category}\")\n",
    "    # Sample up to 3 entries from each group\n",
    "    sample = group.sample(n=min(3, len(group)), random_state=42)\n",
    "    for idx, row in sample.iterrows():\n",
    "        print('Question ID:')\n",
    "        print(row['id'])\n",
    "        print('Image ID:')\n",
    "        print(row['image'])\n",
    "        print(\"Input:\")\n",
    "        qa = row['input_prompt'].split(\"Aceasta este intrebarea:\")[-1]\n",
    "        qa = qa.split(\"Aceastea sunt legile relevante\")[0]\n",
    "        print(qa)\n",
    "        print(\"Output:\")\n",
    "        print(row['output_prompt'])\n",
    "        print(\"Correct answers:\")\n",
    "        print(row['correct_answers'])\n",
    "        print(\"-\" * 40)\n",
    "    print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57069198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "split1 = pd.read_csv('../results/qa/qa_strat_1_split_1_train.csv')\n",
    "split4 = pd.read_csv('../results/qa/qa_strat_4_split_1_train.csv')\n",
    "\n",
    "# Merge on 'id'\n",
    "merged = split1.merge(split4, on='id', suffixes=('_1', '_4'))\n",
    "\n",
    "# Ensure exact_match columns are booleans\n",
    "merged['exact_match_1'] = merged['exact_match_1'].astype(str) == 'True'\n",
    "merged['exact_match_4'] = merged['exact_match_4'].astype(str) == 'True'\n",
    "\n",
    "# Find switches from False to True\n",
    "false_to_true = merged[(~merged['exact_match_1']) & (merged['exact_match_4'])]\n",
    "\n",
    "print(f\"Count switched from False to True: {false_to_true.shape[0]}\")\n",
    "print()\n",
    "\n",
    "# Print input and output for each\n",
    "for idx, row in false_to_true.iterrows():\n",
    "    print('Question ID:')\n",
    "    print(row['id'])\n",
    "    print(\"Input:\")\n",
    "    qa = row['input_prompt_1'].split(\"Aceasta este intrebarea:\")[-1]\n",
    "    qa = qa.split(\"Aceastea sunt legile relevante\")[0]\n",
    "    print(qa)\n",
    "    print(\"Output 3:\")\n",
    "    print(row['output_prompt_1'])\n",
    "    print(\"Output 4:\")\n",
    "    print(row['output_prompt_4'])\n",
    "    print(\"-\" * 40)\n",
    "    print(\"-\" * 40)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "split1 = pd.read_csv('../results/qa/qa_strat_1_split_1_test.csv')\n",
    "split4 = pd.read_csv('../results/qa/qa_strat_4_split_1_test.csv')\n",
    "\n",
    "# Merge on 'id'\n",
    "merged = split1.merge(split4, on='id', suffixes=('_1', '_4'))\n",
    "\n",
    "# Ensure exact_match columns are booleans\n",
    "merged['exact_match_1'] = merged['exact_match_1'].astype(str) == 'True'\n",
    "merged['exact_match_4'] = merged['exact_match_4'].astype(str) == 'True'\n",
    "\n",
    "# Find switches from False to True\n",
    "false_to_true = merged[(~merged['exact_match_1']) & (merged['exact_match_4'])]\n",
    "\n",
    "print(f\"Count switched from False to True: {false_to_true.shape[0]}\")\n",
    "print()\n",
    "\n",
    "# Print input and output for each\n",
    "for idx, row in false_to_true.iterrows():\n",
    "    print('Question ID:')\n",
    "    print(row['id'])\n",
    "    print(\"Input:\")\n",
    "    qa = row['input_prompt_1'].split(\"Aceasta este intrebarea:\")[-1]\n",
    "    qa = qa.split(\"Aceastea sunt legile relevante\")[0]\n",
    "    print(qa)\n",
    "    print(\"Output 3:\")\n",
    "    print(row['output_prompt_1'])\n",
    "    print(\"Output 4:\")\n",
    "    print(row['output_prompt_4'])\n",
    "    print(\"-\" * 40)\n",
    "    print(\"-\" * 40)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393a6f0",
   "metadata": {},
   "source": [
    "# analize de metrici pe clase / categorie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7089ba5",
   "metadata": {},
   "source": [
    "- [ ] analize de metrici pe clase / categorie\n",
    "\n",
    "1. IR - analize recall over split & q category\n",
    "1. QA - analize EM over split & q category\n",
    "1. VIR - 1. analize recall law over split & q category 2. analize recall law ind split & q category\n",
    "1. VQA - 1. analize EM law over split & q category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b847e8f",
   "metadata": {},
   "source": [
    "2 splits - '../results/ir/ir_strat_x_train.csv' & '../results/ir/ir_strat_x_test'\n",
    "calculate recall@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytrec_eval\n",
    "from collections import defaultdict\n",
    "import ast \n",
    "# Read results\n",
    "df_res = pd.read_csv('../results/ir/ir_strat_1_train.csv')\n",
    "df_res = df_res[['id', 'retrieved_documents']]\n",
    "print(df_res.head())\n",
    "\n",
    "# Read references\n",
    "df_ref = pd.read_csv('../data/dataset_V3/split_1_train.csv')\n",
    "df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "print(df_ref.head())\n",
    "\n",
    "# Convert to dict for fast lookup\n",
    "ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "\n",
    "fqrel = {}\n",
    "frun = {}\n",
    "\n",
    "top_k_search = 10  # or whatever value you want\n",
    "\n",
    "for idx in df_res['id']:\n",
    "    # Assume legislation and retrieved_documents are comma-separated string lists\n",
    "    relevant_set = ast.literal_eval(ref_dict[idx])\n",
    "    retrieved_set = ast.literal_eval(res_dict[idx])\n",
    "    # Build qrel (binary relevance for pytrec_eval)\n",
    "    fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "    # Build run: assign dummy score (descending order)\n",
    "    run = {doc: float(top_k_search - rank) for rank, doc in enumerate(retrieved_set[:top_k_search])}\n",
    "    frun[str(idx)] = run\n",
    "\n",
    "# Define metrics, e.g. recall at 10\n",
    "metrics = {'recall_10'}\n",
    "\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(fqrel, metrics)\n",
    "results = evaluator.evaluate(frun)\n",
    "\n",
    "# Compute average recall@10\n",
    "recalls = [res['recall_10'] for res in results.values()]\n",
    "avg_recall = np.mean(recalls)\n",
    "print('Average recall@10:', avg_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8783c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytrec_eval\n",
    "from collections import defaultdict\n",
    "import ast \n",
    "\n",
    "# Read results\n",
    "df_res = pd.read_csv('../results/ir/ir_strat_1_train.csv')\n",
    "df_res = df_res[['id', 'retrieved_documents']]\n",
    "\n",
    "# Read references\n",
    "df_ref = pd.read_csv('../data/dataset_V3/split_1_train.csv')\n",
    "df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "\n",
    "# Convert to dict for fast lookup\n",
    "ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "fqrel = {}\n",
    "frun = {}\n",
    "\n",
    "top_k_search = 10\n",
    "\n",
    "# Build qrel and run for pytrec_eval\n",
    "for idx in df_res['id']:\n",
    "    relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "    retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "    fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "    run = {doc: float(top_k_search - rank) for rank, doc in enumerate(retrieved_set[:top_k_search])}\n",
    "    frun[str(idx)] = run\n",
    "\n",
    "# Define metrics\n",
    "metrics = {'recall_10'}\n",
    "\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(fqrel, metrics)\n",
    "results = evaluator.evaluate(frun)\n",
    "\n",
    "# Compute recall@10 for each query and collect by category\n",
    "cat_recalls = defaultdict(list)\n",
    "\n",
    "for query_id, result in results.items():\n",
    "    # Both query_id and cat_dict keys are strings, but just in case, match types\n",
    "    cat = cat_dict[query_id] if query_id in cat_dict else cat_dict[query_id]\n",
    "    recall = result['recall_10']\n",
    "    cat_recalls[cat].append(recall)\n",
    "\n",
    "\n",
    "# Load categories and get id->name mapping (English)\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "\n",
    "# Print average recall per category\n",
    "print(\"Average recall@10 per question_category_id:\")\n",
    "for cat, recalls in cat_recalls.items():\n",
    "    avg = np.mean(recalls)\n",
    "    print(f'Category {cat_map[cat]}: Average recall@10 = {avg:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_recall_per_category(res_path, ref_path, cat_map, top_k=10):\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pytrec_eval\n",
    "    from collections import defaultdict\n",
    "\n",
    "    df_res = pd.read_csv(res_path)\n",
    "    df_res = df_res[['id', 'retrieved_documents']]\n",
    "\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "\n",
    "    ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "    res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "    cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "    fqrel = {}\n",
    "    frun = {}\n",
    "\n",
    "    for idx in df_res['id']:\n",
    "        relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "        retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "        fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "        run = {doc: float(top_k - rank) for rank, doc in enumerate(retrieved_set[:top_k])}\n",
    "        frun[str(idx)] = run\n",
    "\n",
    "    metrics = {'recall_10'}\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(fqrel, metrics)\n",
    "    results = evaluator.evaluate(frun)\n",
    "\n",
    "    cat_recalls = defaultdict(list)\n",
    "    for query_id, result in results.items():\n",
    "        cat = cat_dict[query_id]\n",
    "        recall = result['recall_10']\n",
    "        cat_recalls[cat].append(recall)\n",
    "\n",
    "    # Aggregate averages by category_id\n",
    "    avg_recall_per_cat = {}\n",
    "    for cat, recalls in cat_recalls.items():\n",
    "        avg_recall_per_cat[cat] = np.mean(recalls)\n",
    "\n",
    "    # Remap to English names for plot\n",
    "    avg_recall_per_cat_en = {cat_map[cat]: avg for cat, avg in avg_recall_per_cat.items() if cat in cat_map}\n",
    "    return avg_recall_per_cat_en\n",
    "\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "\n",
    "avg_recall_train = compute_avg_recall_per_category(\n",
    "    '../results/ir/ir_strat_1_train.csv',\n",
    "    '../data/dataset_V3/split_1_train.csv',\n",
    "    cat_map\n",
    ")\n",
    "avg_recall_test = compute_avg_recall_per_category(\n",
    "    '../results/ir/ir_strat_1_test.csv',\n",
    "    '../data/dataset_V3/split_1_test.csv',\n",
    "    cat_map\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure all categories are present in both for plotting order\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "\n",
    "train_recalls = [avg_recall_train.get(cat, 0) for cat in all_cats]\n",
    "test_recalls = [avg_recall_test.get(cat, 0) for cat in all_cats]\n",
    "\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(all_cats))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8), constrained_layout=True)\n",
    "bars1 = ax.bar(index - bar_width/2, train_recalls, width=bar_width, color=\"#3572b0\", label='Train')\n",
    "bars2 = ax.bar(index + bar_width/2, test_recalls, width=bar_width, color=\"#3572b0\", label='Test', hatch='///', edgecolor='black')\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "                    xytext=(0,3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average Recall@10')\n",
    "ax.set_title('Average Recall@10 per Category with Strategy 1 (Split 1 Train vs Split 1 Test)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('../plots/final_font/ir_recall_per_category_train_test_st1.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_recall_per_category(res_path, ref_path, cat_map, top_k=10):\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pytrec_eval\n",
    "    from collections import defaultdict\n",
    "\n",
    "    df_res = pd.read_csv(res_path)\n",
    "    df_res = df_res[['id', 'retrieved_documents']]\n",
    "\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "\n",
    "    ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "    res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "    cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "    fqrel = {}\n",
    "    frun = {}\n",
    "\n",
    "    for idx in df_res['id']:\n",
    "        relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "        retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "        fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "        run = {doc: float(top_k - rank) for rank, doc in enumerate(retrieved_set[:top_k])}\n",
    "        frun[str(idx)] = run\n",
    "\n",
    "    metrics = {'recall_10'}\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(fqrel, metrics)\n",
    "    results = evaluator.evaluate(frun)\n",
    "\n",
    "    cat_recalls = defaultdict(list)\n",
    "    for query_id, result in results.items():\n",
    "        cat = cat_dict[query_id]\n",
    "        recall = result['recall_10']\n",
    "        cat_recalls[cat].append(recall)\n",
    "\n",
    "    # Aggregate averages by category_id\n",
    "    avg_recall_per_cat = {}\n",
    "    for cat, recalls in cat_recalls.items():\n",
    "        avg_recall_per_cat[cat] = np.mean(recalls)\n",
    "\n",
    "    # Remap to English names for plot\n",
    "    avg_recall_per_cat_en = {cat_map[cat]: avg for cat, avg in avg_recall_per_cat.items() if cat in cat_map}\n",
    "    return avg_recall_per_cat_en\n",
    "import pandas as pd\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "\n",
    "avg_recall_train = compute_avg_recall_per_category(\n",
    "    '../results/ir/ir_strat_3_train.csv',\n",
    "    '../data/dataset_V3/split_1_train.csv',\n",
    "    cat_map\n",
    ")\n",
    "avg_recall_test = compute_avg_recall_per_category(\n",
    "    '../results/ir/ir_strat_3_test.csv',\n",
    "    '../data/dataset_V3/split_1_test.csv',\n",
    "    cat_map\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure all categories are present in both for plotting order\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "\n",
    "train_recalls = [avg_recall_train.get(cat, 0) for cat in all_cats]\n",
    "test_recalls = [avg_recall_test.get(cat, 0) for cat in all_cats]\n",
    "\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(all_cats))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8), constrained_layout=True)\n",
    "bars1 = ax.bar(index - bar_width/2, train_recalls, width=bar_width, color=\"#3572b0\", label='Train')\n",
    "bars2 = ax.bar(index + bar_width/2, test_recalls, width=bar_width, color=\"#3572b0\", label='Test', hatch='///', edgecolor='black')\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "                    xytext=(0,3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average Recall@10')\n",
    "ax.set_title('Average Recall@10 per Category with Strategy 3 (Split 1 Train vs Split 1 Test)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('../plots/final_font/ir_recall_per_category_train_test_st3.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f68b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_recall_per_category(res_path, ref_path, cat_map, top_k=10):\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pytrec_eval\n",
    "    from collections import defaultdict\n",
    "\n",
    "    df_res = pd.read_csv(res_path)\n",
    "    df_res = df_res[['id', 'retrieved_documents']]\n",
    "\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "\n",
    "    ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "    res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "    cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "    fqrel = {}\n",
    "    frun = {}\n",
    "\n",
    "    for idx in df_res['id']:\n",
    "        relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "        retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "        fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "        run = {doc: float(top_k - rank) for rank, doc in enumerate(retrieved_set[:top_k])}\n",
    "        frun[str(idx)] = run\n",
    "\n",
    "    metrics = {'recall_10'}\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(fqrel, metrics)\n",
    "    results = evaluator.evaluate(frun)\n",
    "\n",
    "    cat_recalls = defaultdict(list)\n",
    "    for query_id, result in results.items():\n",
    "        cat = cat_dict[query_id]\n",
    "        recall = result['recall_10']\n",
    "        cat_recalls[cat].append(recall)\n",
    "\n",
    "    # Aggregate averages by category_id\n",
    "    avg_recall_per_cat = {}\n",
    "    for cat, recalls in cat_recalls.items():\n",
    "        avg_recall_per_cat[cat] = np.mean(recalls)\n",
    "\n",
    "    # Remap to English names for plot\n",
    "    avg_recall_per_cat_en = {cat_map[cat]: avg for cat, avg in avg_recall_per_cat.items() if cat in cat_map}\n",
    "    return avg_recall_per_cat_en\n",
    "\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "\n",
    "avg_recall_train = compute_avg_recall_per_category(\n",
    "    '../results/ir/ir_strat_6_train.csv',\n",
    "    '../data/dataset_V3/split_1_train.csv',\n",
    "    cat_map\n",
    ")\n",
    "avg_recall_test = compute_avg_recall_per_category(\n",
    "    '../results/ir/ir_strat_6_test.csv',\n",
    "    '../data/dataset_V3/split_1_test.csv',\n",
    "    cat_map\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure all categories are present in both for plotting order\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "\n",
    "train_recalls = [avg_recall_train.get(cat, 0) for cat in all_cats]\n",
    "test_recalls = [avg_recall_test.get(cat, 0) for cat in all_cats]\n",
    "\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(all_cats))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8), constrained_layout=True)\n",
    "bars1 = ax.bar(index - bar_width/2, train_recalls, width=bar_width, color=\"#3572b0\", label='Train')\n",
    "bars2 = ax.bar(index + bar_width/2, test_recalls, width=bar_width, color=\"#3572b0\", label='Test', hatch='///', edgecolor='black')\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "                    xytext=(0,3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average Recall@10')\n",
    "ax.set_title('Average Recall@10 per Category with Strategy 6 (Split 1 Train vs Split 1 Test)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('../plots/final_font/ir_recall_per_category_train_test_st6.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_recall_per_category(res_path, ref_path, cat_map, top_k=10):\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pytrec_eval\n",
    "    from collections import defaultdict\n",
    "\n",
    "    df_res = pd.read_csv(res_path)\n",
    "    df_res = df_res[['id', 'retrieved_documents']]\n",
    "\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "\n",
    "    ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "    res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "    cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "    fqrel = {}\n",
    "    frun = {}\n",
    "\n",
    "    for idx in df_res['id']:\n",
    "        relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "        retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "        fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "        run = {doc: float(top_k - rank) for rank, doc in enumerate(retrieved_set[:top_k])}\n",
    "        frun[str(idx)] = run\n",
    "\n",
    "    metrics = {'recall_10'}\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(fqrel, metrics)\n",
    "    results = evaluator.evaluate(frun)\n",
    "\n",
    "    cat_recalls = defaultdict(list)\n",
    "    for query_id, result in results.items():\n",
    "        cat = cat_dict[query_id]\n",
    "        recall = result['recall_10']\n",
    "        cat_recalls[cat].append(recall)\n",
    "\n",
    "    # Aggregate averages by category_id\n",
    "    avg_recall_per_cat = {}\n",
    "    for cat, recalls in cat_recalls.items():\n",
    "        avg_recall_per_cat[cat] = np.mean(recalls)\n",
    "\n",
    "    # Remap to English names for plot\n",
    "    avg_recall_per_cat_en = {cat_map[cat]: avg for cat, avg in avg_recall_per_cat.items() if cat in cat_map}\n",
    "    return avg_recall_per_cat_en\n",
    "\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "\n",
    "avg_recall_train = compute_avg_recall_per_category(\n",
    "    '../results/ir/ir_strat_8_train.csv',\n",
    "    '../data/dataset_V3/split_1_train.csv',\n",
    "    cat_map\n",
    ")\n",
    "avg_recall_test = compute_avg_recall_per_category(\n",
    "    '../results/ir/ir_strat_8_test.csv',\n",
    "    '../data/dataset_V3/split_1_test.csv',\n",
    "    cat_map\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure all categories are present in both for plotting order\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "\n",
    "train_recalls = [avg_recall_train.get(cat, 0) for cat in all_cats]\n",
    "test_recalls = [avg_recall_test.get(cat, 0) for cat in all_cats]\n",
    "\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(all_cats))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 16,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8), constrained_layout=True)\n",
    "bars1 = ax.bar(index - bar_width/2, train_recalls, width=bar_width, color=\"#3572b0\", label='Train')\n",
    "bars2 = ax.bar(index + bar_width/2, test_recalls, width=bar_width, color=\"#3572b0\", label='Test', hatch='///', edgecolor='black')\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "                    xytext=(0,3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average Recall@10')\n",
    "ax.set_title('Average Recall@10 per Category with Strategy 8 (Split 1 Train vs Split 1 Test)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('../plots/final_font/ir_recall_per_category_train_test_st8.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_recall_per_category(res_path, ref_path, cat_map, top_k=10):\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pytrec_eval\n",
    "    from collections import defaultdict\n",
    "\n",
    "    df_res = pd.read_csv(res_path)\n",
    "    df_res = df_res[['id', 'retrieved_documents']]\n",
    "\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "\n",
    "    ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "    res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "    cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "    fqrel = {}\n",
    "    frun = {}\n",
    "\n",
    "    for idx in df_res['id']:\n",
    "        relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "        retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "        fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "        run = {doc: float(top_k - rank) for rank, doc in enumerate(retrieved_set[:top_k])}\n",
    "        frun[str(idx)] = run\n",
    "\n",
    "    metrics = {'recall_10'}\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(fqrel, metrics)\n",
    "    results = evaluator.evaluate(frun)\n",
    "\n",
    "    cat_recalls = defaultdict(list)\n",
    "    for query_id, result in results.items():\n",
    "        cat = cat_dict[query_id]\n",
    "        recall = result['recall_10']\n",
    "        cat_recalls[cat].append(recall)\n",
    "\n",
    "    # Aggregate averages by category_id\n",
    "    avg_recall_per_cat = {}\n",
    "    for cat, recalls in cat_recalls.items():\n",
    "        avg_recall_per_cat[cat] = np.mean(recalls)\n",
    "\n",
    "    # Remap to English names for plot\n",
    "    avg_recall_per_cat_en = {cat_map[cat]: avg for cat, avg in avg_recall_per_cat.items() if cat in cat_map}\n",
    "    return avg_recall_per_cat_en\n",
    "\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "\n",
    "avg_recall_train = compute_avg_recall_per_category(\n",
    "    '../results/vir/vir_strat_1_split_3_laws.csv',\n",
    "    '../data/dataset_V3/split_3.csv',\n",
    "    cat_map\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure all categories are present in both for plotting order\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "\n",
    "train_recalls = [avg_recall_train.get(cat, 0) for cat in all_cats]\n",
    "\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(all_cats))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 25,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8), constrained_layout=True)\n",
    "bars1 = ax.bar(index - bar_width/2, train_recalls, width=bar_width, color=\"#81b29a\", label='Split 3')\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "                    xytext=(0,3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average Recall@10')\n",
    "ax.set_title('Average Recall@10 per Category with Strategy 1 on split 3')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('../plots/new_font_err3/vir_recall_per_category_split_3_law_st1.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ded41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_recall_per_category(res_path, ref_path, cat_map, top_k=10):\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pytrec_eval\n",
    "    from collections import defaultdict\n",
    "\n",
    "    df_res = pd.read_csv(res_path)\n",
    "    df_res = df_res[['id', 'retrieved_documents']]\n",
    "\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "\n",
    "    ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "    res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "    cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "    fqrel = {}\n",
    "    frun = {}\n",
    "\n",
    "    for idx in df_res['id']:\n",
    "        relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "        retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "        fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "        run = {doc: float(top_k - rank) for rank, doc in enumerate(retrieved_set[:top_k])}\n",
    "        frun[str(idx)] = run\n",
    "\n",
    "    metrics = {'recall_10'}\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(fqrel, metrics)\n",
    "    results = evaluator.evaluate(frun)\n",
    "\n",
    "    cat_recalls = defaultdict(list)\n",
    "    for query_id, result in results.items():\n",
    "        cat = cat_dict[query_id]\n",
    "        recall = result['recall_10']\n",
    "        cat_recalls[cat].append(recall)\n",
    "\n",
    "    # Aggregate averages by category_id\n",
    "    avg_recall_per_cat = {}\n",
    "    for cat, recalls in cat_recalls.items():\n",
    "        avg_recall_per_cat[cat] = np.mean(recalls)\n",
    "\n",
    "    # Remap to English names for plot\n",
    "    avg_recall_per_cat_en = {cat_map[cat]: avg for cat, avg in avg_recall_per_cat.items() if cat in cat_map}\n",
    "    return avg_recall_per_cat_en\n",
    "\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "\n",
    "avg_recall_train = compute_avg_recall_per_category(\n",
    "    '../results/vir/vir_strat_5_split_3_laws.csv',\n",
    "    '../data/dataset_V3/split_3.csv',\n",
    "    cat_map\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure all categories are present in both for plotting order\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "\n",
    "train_recalls = [avg_recall_train.get(cat, 0) for cat in all_cats]\n",
    "\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(all_cats))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 25,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8), constrained_layout=True)\n",
    "bars1 = ax.bar(index - bar_width/2, train_recalls, width=bar_width, color=\"#81b29a\", label='Split 3')\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "                    xytext=(0,3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average Recall@10')\n",
    "ax.set_title('Average Recall@10 per Category with Strategy 5 on split 3')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.savefig('../plots/new_font_err3/vir_recall_per_category_split_3_law_st5.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c854217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_recall_per_category(res_path, ref_path, cat_map, top_k=10):\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pytrec_eval\n",
    "    from collections import defaultdict\n",
    "\n",
    "    df_res = pd.read_csv(res_path)\n",
    "    df_res = df_res[['id', 'retrieved_documents']]\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "    ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "    res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "    cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "    fqrel, frun = {}, {}\n",
    "    for idx in df_res['id']:\n",
    "        relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "        retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "        fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "        run = {doc: float(top_k - rank) for rank, doc in enumerate(retrieved_set[:top_k])}\n",
    "        frun[str(idx)] = run\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(fqrel, {'recall_10'})\n",
    "    results = evaluator.evaluate(frun)\n",
    "\n",
    "    cat_recalls = defaultdict(list)\n",
    "    for query_id, result in results.items():\n",
    "        cat = cat_dict[query_id]\n",
    "        recall = result['recall_10']\n",
    "        cat_recalls[cat].append(recall)\n",
    "    avg_recall_per_cat = {cat: np.mean(recalls) for cat, recalls in cat_recalls.items()}\n",
    "    return avg_recall_per_cat\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "\n",
    "recall_by_strat = []   # List of dicts for each strat\n",
    "# strat_names = [f\"Strat {i}\" for i in range(1,9)]\n",
    "strat_names = [\n",
    "    \"Question based (Q)\",\n",
    "    \"Question + Answer Choices (QA)\",\n",
    "    \"QA + ReRanker jina\",\n",
    "    \"QA + ReRanker bert-msmarco\",\n",
    "    \"QA rephrased using GPT 4o-mini\",\n",
    "    \"Finetuned Retriever\",\n",
    "    \"Finetuned Retriever + ReRanker jina\",\n",
    "    \"Augmented Finetuned Retriever\"\n",
    "]\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\"\n",
    "]\n",
    "\n",
    "for i in range(1, 9):\n",
    "    res_path = f'../results/ir/ir_strat_{i}_train.csv'\n",
    "    ref_path = '../data/dataset_V3/split_1_train.csv'\n",
    "    avg_recalls = compute_avg_recall_per_category(res_path, ref_path, cat_map)\n",
    "    # Map to category English names, fill missing with 0, and keep order of all_cats\n",
    "    avg_recalls_ordered = [avg_recalls.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    recall_by_strat.append(avg_recalls_ordered)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "bar_width = 0.11\n",
    "index = np.arange(len(all_cats))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 13,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 8), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "for strat_idx, (recalls, color, strat_name) in enumerate(zip(recall_by_strat, colors, strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*4/2, recalls, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average Recall@10')\n",
    "\n",
    "plt.suptitle('Average Recall@10 per Category for All Strategies (Split 1 Train)', fontsize=25, y=1.07)  # Title way up\n",
    "\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.11),   # Still above plot, but below the suptitle\n",
    "    ncol=4,\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "plt.savefig('../plots/new_font_err/ir_recall_per_category_all_strategies.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d869213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_recall_per_category(res_path, ref_path, cat_map, top_k=10):\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pytrec_eval\n",
    "    from collections import defaultdict\n",
    "\n",
    "    df_res = pd.read_csv(res_path)\n",
    "    df_res = df_res[['id', 'retrieved_documents']]\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "    ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "    res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "    cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "    fqrel, frun = {}, {}\n",
    "    for idx in df_res['id']:\n",
    "        relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "        retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "        fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "        run = {doc: float(top_k - rank) for rank, doc in enumerate(retrieved_set[:top_k])}\n",
    "        frun[str(idx)] = run\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(fqrel, {'recall_10'})\n",
    "    results = evaluator.evaluate(frun)\n",
    "\n",
    "    cat_recalls = defaultdict(list)\n",
    "    for query_id, result in results.items():\n",
    "        cat = cat_dict[query_id]\n",
    "        recall = result['recall_10']\n",
    "        cat_recalls[cat].append(recall)\n",
    "    avg_recall_per_cat = {cat: np.mean(recalls) for cat, recalls in cat_recalls.items()}\n",
    "    return avg_recall_per_cat\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "\n",
    "strat_names = [\n",
    "    \"Question based (Q)\",\n",
    "    \"Question + Answer Choices (QA)\",\n",
    "    \"QA + ReRanker jina\",\n",
    "    \"QA + ReRanker bert-msmarco\",\n",
    "    \"QA rephrased using GPT 4o-mini\",\n",
    "    \"Finetuned Retriever\",\n",
    "    \"Finetuned Retriever + ReRanker jina\",\n",
    "    \"Augmented Finetuned Retriever\"\n",
    "]\n",
    "\n",
    "recall_by_strat = []   # List of dicts for each strat\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\"\n",
    "]\n",
    "\n",
    "for i in range(1, 9):\n",
    "    res_path = f'../results/ir/ir_strat_{i}_test.csv'\n",
    "    ref_path = '../data/dataset_V3/split_1_test.csv'\n",
    "    avg_recalls = compute_avg_recall_per_category(res_path, ref_path, cat_map)\n",
    "    # Map to category English names, fill missing with 0, and keep order of all_cats\n",
    "    avg_recalls_ordered = [avg_recalls.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    recall_by_strat.append(avg_recalls_ordered)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "bar_width = 0.11\n",
    "index = np.arange(len(all_cats))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 13,\n",
    "})\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 8), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "for strat_idx, (recalls, color, strat_name) in enumerate(zip(recall_by_strat, colors, strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*4/2, recalls, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average Recall@10')\n",
    "\n",
    "plt.suptitle('Average Recall@10 per Category for All Strategies (Split 1 Test)', fontsize=25, y=1.07)  # Title way up\n",
    "\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.11),   # Still above plot, but below the suptitle\n",
    "    ncol=4,\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "plt.savefig('../plots/new_font_err/ir_recall_per_category_all_strategies_test.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4120907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_recall_per_category(res_path, ref_path, cat_map, top_k=10):\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pytrec_eval\n",
    "    from collections import defaultdict\n",
    "\n",
    "    df_res = pd.read_csv(res_path)\n",
    "    df_res = df_res[['id', 'retrieved_documents']]\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    df_ref = df_ref[['id', 'legislation', 'question_category_id']]\n",
    "    ref_dict = dict(zip(df_ref['id'], df_ref['legislation']))\n",
    "    res_dict = dict(zip(df_res['id'], df_res['retrieved_documents']))\n",
    "    cat_dict = dict(zip(df_ref['id'], df_ref['question_category_id']))\n",
    "\n",
    "    fqrel, frun = {}, {}\n",
    "    for idx in df_res['id']:\n",
    "        relevant_set = ast.literal_eval(ref_dict[idx]) if isinstance(ref_dict[idx], str) else ref_dict[idx]\n",
    "        retrieved_set = ast.literal_eval(res_dict[idx]) if isinstance(res_dict[idx], str) else res_dict[idx]\n",
    "        fqrel[str(idx)] = {doc: 1 for doc in relevant_set}\n",
    "        run = {doc: float(top_k - rank) for rank, doc in enumerate(retrieved_set[:top_k])}\n",
    "        frun[str(idx)] = run\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(fqrel, {'recall_10'})\n",
    "    results = evaluator.evaluate(frun)\n",
    "\n",
    "    cat_recalls = defaultdict(list)\n",
    "    for query_id, result in results.items():\n",
    "        cat = cat_dict[query_id]\n",
    "        recall = result['recall_10']\n",
    "        cat_recalls[cat].append(recall)\n",
    "    avg_recall_per_cat = {cat: np.mean(recalls) for cat, recalls in cat_recalls.items()}\n",
    "    return avg_recall_per_cat\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_map = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "\n",
    "recall_by_strat = []   # List of dicts for each strat\n",
    "# strat_names = [f\"Strat {i}\" for i in range(1,9)]\n",
    "strat_names = [\n",
    "    \"Question + Answer Choices (QA)\",\n",
    "    \"Caption + QA\",\n",
    "    \"R[OpenAI o4-mini + Image + QA]\",\n",
    "    \"R[OpenAI o4-mini + Image + Caption + QA]\",\n",
    "    \"R[OpenAI o4-mini + Image + QA] + QA\",\n",
    "    \"R[OpenAI o4-mini + Image + Caption + QA] + QA\"\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\"\n",
    "]\n",
    "\n",
    "for i in range(1, 7):\n",
    "    res_path = f'../results/vir/vir_strat_{i}_split_3_laws.csv'\n",
    "    ref_path = '../data/dataset_V3/split_3.csv'\n",
    "    avg_recalls = compute_avg_recall_per_category(res_path, ref_path, cat_map)\n",
    "    # Map to category English names, fill missing with 0, and keep order of all_cats\n",
    "    avg_recalls_ordered = [avg_recalls.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    recall_by_strat.append(avg_recalls_ordered)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "bar_width = 0.11\n",
    "index = np.arange(len(all_cats))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 12,\n",
    "})\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 8), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "for strat_idx, (recalls, color, strat_name) in enumerate(zip(recall_by_strat, colors, strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*4/2, recalls, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average Recall@10')\n",
    "\n",
    "plt.suptitle('Average Recall@10 per Category for All Strategies (Split 3)', fontsize=25, y=1.07)  # Title way up\n",
    "\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.11),   # Still above plot, but below the suptitle\n",
    "    ncol=4,\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "plt.savefig('../plots/new_font_err/vir_recall_per_category_all_strategies_split_3.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../results/qa/qa_strat_1_split_1_test.csv')\n",
    "df = df[['id', 'qa_result', 'correct_answers', 'exact_match']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Strategy names for the legend\n",
    "qa_strat_names = [\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT (without RAG)\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + Ideal RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI GPT-4o-mini + RAG + better prompt (without CoT)\",\n",
    "    \"OpenAI o4-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI o4-mini + CoT + better prompt (without RAG)\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 8):\n",
    "    df = pd.read_csv(f'../results/qa/qa_strat_{i}_split_1_test.csv')\n",
    "    # Group by question_category_id and compute mean exact match\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    # Need category per question (assume you have question_category_id in your df)\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_1_test to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_1_test.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    # Map and align to all_cats\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.12\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 12,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8), constrained_layout=True)\n",
    "plt.suptitle(\n",
    "    'QA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.07\n",
    ")\n",
    "bars = []\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, qa_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*3, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "# Optional: Annotate values (may be cluttered)\n",
    "# for bar_set in bars:\n",
    "#     for bar in bar_set:\n",
    "#         height = bar.get_height()\n",
    "#         if height > 0:\n",
    "#             ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "#                         xytext=(0,3), textcoords=\"offset points\",\n",
    "#                         ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "# ax.set_title('QA Accuracy (Exact Match) per Strategy by Question Category')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.11), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/qa_accuracy_exact_match_per_category_strategies_1_test.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Strategy names for the legend\n",
    "qa_strat_names = [\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT (without RAG)\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + Ideal RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI GPT-4o-mini + RAG + better prompt (without CoT)\",\n",
    "    \"OpenAI o4-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI o4-mini + CoT + better prompt (without RAG)\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 8):\n",
    "    df = pd.read_csv(f'../results/qa/qa_strat_{i}_split_1_train.csv')\n",
    "    # Group by question_category_id and compute mean exact match\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    # Need category per question (assume you have question_category_id in your df)\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_1_test to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_1_train.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    # Map and align to all_cats\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.12\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 12,\n",
    "})\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8), constrained_layout=True)\n",
    "plt.suptitle(\n",
    "    'QA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.07\n",
    ")\n",
    "bars = []\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, qa_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*3, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "# Optional: Annotate values (may be cluttered)\n",
    "# for bar_set in bars:\n",
    "#     for bar in bar_set:\n",
    "#         height = bar.get_height()\n",
    "#         if height > 0:\n",
    "#             ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "#                         xytext=(0,3), textcoords=\"offset points\",\n",
    "#                         ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "# ax.set_title('QA Accuracy (Exact Match) per Strategy by Question Category')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.11), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/qa_accuracy_exact_match_per_category_strategies_1_train.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Strategy names for the legend\n",
    "qa_strat_names = [\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT (without RAG)\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + Ideal RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI GPT-4o-mini + RAG + better prompt (without CoT)\",\n",
    "    \"OpenAI o4-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI o4-mini + CoT + better prompt (without RAG)\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 8):\n",
    "    df = pd.read_csv(f'../results/qa/qa_strat_{i}_split_2.csv')\n",
    "    # Group by question_category_id and compute mean exact match\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    # Need category per question (assume you have question_category_id in your df)\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_1_test to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_2.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    # Map and align to all_cats\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.12\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 12,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8), constrained_layout=True)\n",
    "plt.suptitle(\n",
    "    'QA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.07\n",
    ")\n",
    "bars = []\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, qa_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*3, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "# Optional: Annotate values (may be cluttered)\n",
    "# for bar_set in bars:\n",
    "#     for bar in bar_set:\n",
    "#         height = bar.get_height()\n",
    "#         if height > 0:\n",
    "#             ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height), \n",
    "#                         xytext=(0,3), textcoords=\"offset points\",\n",
    "#                         ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "# ax.set_title('QA Accuracy (Exact Match) per Strategy by Question Category')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.11), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/qa_accuracy_exact_match_per_category_strategies_2.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../results/vqa/vqa_strat_1_split_3.csv')\n",
    "df = df[['id', 'qa_result', 'correct_answers', 'exact_match']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f5bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_3.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_3 to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_3.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\", \"#faa307\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "    \"legend.fontsize\": 14,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 9), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "# Center bars around each category group\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vqa_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*4, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    'VQA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.09\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/vqa_accuracy_exact_match_per_category_strategies_sp3.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4897e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_4.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_3 to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_4.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\", \"#faa307\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "    \"legend.fontsize\": 14,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 9), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "# Center bars around each category group\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vqa_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*4, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    'VQA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.09\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/vqa_accuracy_exact_match_per_category_strategies_sp4.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adcc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the same for these:\n",
    "\n",
    "df = pd.read_csv('../results/qa_vllm/qa_strat_1_split_1_vllm.csv')\n",
    "df = df[['id', 'qa_result', 'correct_answers', 'exact_match']]\n",
    "df.head()\n",
    "\n",
    "# with strat 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vllm_strat_names = [\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT + RAG\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT (without RAG)\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT + Ideal RAG\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT + RAG + better prompt\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + RAG + better prompt (without CoT)\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 6):\n",
    "    df = pd.read_csv(f'../results/qa_vllm/qa_strat_{i}_split_1_train_vllm.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_1 to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_1_train.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.14\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\", \"#b56576\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 19,\n",
    "    \"ytick.labelsize\": 19,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 9), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vllm_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*2, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    'Mistral QA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.09\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/mistral_qa_accuracy_exact_match_per_category_strategies_1_train.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vllm_strat_names = [\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT + RAG\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT (without RAG)\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT + Ideal RAG\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT + RAG + better prompt\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + RAG + better prompt (without CoT)\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 6):\n",
    "    df = pd.read_csv(f'../results/qa_vllm/qa_strat_{i}_split_1_test_vllm.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_1 to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_1_test.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.14\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\", \"#b56576\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 19,\n",
    "    \"ytick.labelsize\": 19,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 9), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vllm_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*2, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    'Mistral QA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.09\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/mistral_qa_accuracy_exact_match_per_category_strategies_1_test.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vllm_strat_names = [\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT + RAG\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT (without RAG)\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT + Ideal RAG\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + CoT + RAG + better prompt\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503 + RAG + better prompt (without CoT)\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 6):\n",
    "    df = pd.read_csv(f'../results/qa_vllm/qa_strat_{i}_split_2_vllm.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_1 to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_2.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.14\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\", \"#b56576\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 19,\n",
    "    \"ytick.labelsize\": 19,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "fig, ax = plt.subplots(figsize=(20, 9), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vllm_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*2, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    'Mistral QA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.09\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/mistral_qa_accuracy_exact_match_per_category_strategies_2.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ef9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vllm_strat_names = [\n",
    "    \"Gemma 3 27B Instruct + CoT + RAG\",\n",
    "    \"Gemma 3 27B Instruct + CoT (without RAG)\",\n",
    "    \"Gemma 3 27B Instruct + CoT + Ideal RAG\",\n",
    "    \"Gemma 3 27B Instruct + CoT + RAG + better prompt\",\n",
    "    \"Gemma 3 27B Instruct + RAG + better prompt (without CoT)\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 6):\n",
    "    df = pd.read_csv(f'../results/qa_vllm2/qa_strat_{i}_split_1_train_vllm.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_1 to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_1_train.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.14\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\", \"#b56576\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 19,\n",
    "    \"ytick.labelsize\": 19,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 9), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vllm_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*2, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    'Gemma QA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.09\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/gemma_qa_accuracy_exact_match_per_category_strategies_1_train.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7909f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vllm_strat_names = [\n",
    "    \"Gemma 3 27B Instruct + CoT + RAG\",\n",
    "    \"Gemma 3 27B Instruct + CoT (without RAG)\",\n",
    "    \"Gemma 3 27B Instruct + CoT + Ideal RAG\",\n",
    "    \"Gemma 3 27B Instruct + CoT + RAG + better prompt\",\n",
    "    \"Gemma 3 27B Instruct + RAG + better prompt (without CoT)\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 6):\n",
    "    df = pd.read_csv(f'../results/qa_vllm2/qa_strat_{i}_split_1_test_vllm.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_1 to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_1_test.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.14\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\", \"#b56576\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 19,\n",
    "    \"ytick.labelsize\": 19,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 9), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vllm_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*2, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    'Gemma QA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.09\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/gemma_qa_accuracy_exact_match_per_category_strategies_1_test.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vllm_strat_names = [\n",
    "    \"Gemma 3 27B Instruct + CoT + RAG\",\n",
    "    \"Gemma 3 27B Instruct + CoT (without RAG)\",\n",
    "    \"Gemma 3 27B Instruct + CoT + Ideal RAG\",\n",
    "    \"Gemma 3 27B Instruct + CoT + RAG + better prompt\",\n",
    "    \"Gemma 3 27B Instruct + RAG + better prompt (without CoT)\"\n",
    "]\n",
    "\n",
    "# Category mapping as before\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Read and compute per-category accuracy for each strategy\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 6):\n",
    "    df = pd.read_csv(f'../results/qa_vllm2/qa_strat_{i}_split_2_vllm.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # If missing, merge with split_1 to get category\n",
    "        df_meta = pd.read_csv('../data/dataset_V3/split_2.csv')[['id','question_category_id']]\n",
    "        df = df.merge(df_meta, on='id', how='left')\n",
    "    accs = df.groupby('question_category_id')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.14\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\", \"#b56576\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 19,\n",
    "    \"ytick.labelsize\": 19,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 9), constrained_layout=True)\n",
    "\n",
    "bars = []\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vllm_strat_names)):\n",
    "    bars.append(\n",
    "        ax.bar(index + bar_width*strat_idx - bar_width*2, accs, width=bar_width, color=color, label=strat_name)\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    'Gemma QA Accuracy (Exact Match) per Strategy by Question Category',\n",
    "    fontsize=25,\n",
    "    y=1.09\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/gemma_qa_accuracy_exact_match_per_category_strategies_2.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "# Get categories from split_4.csv\n",
    "meta = pd.read_csv('../data/dataset_V3/split_4.csv')[['id', 'category']]\n",
    "categories = sorted(meta['category'].dropna().unique())\n",
    "categories_cap = [cat.capitalize() for cat in categories]  # Capitalize\n",
    "\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_4.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    # Merge to get 'category'\n",
    "    df = df.merge(meta, on='id', how='left')\n",
    "    df['category_cap'] = df['category'].str.capitalize()\n",
    "    accs = df.groupby('category_cap')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat, 0) for cat in categories_cap]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(categories_cap))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\", \"#faa307\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 12,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7), constrained_layout=True)\n",
    "\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vqa_strat_names)):\n",
    "    ax.bar(index + bar_width*strat_idx - bar_width*4, accs, width=bar_width, color=color, label=strat_name)\n",
    "\n",
    "plt.suptitle(\n",
    "    'VQA Accuracy (Exact Match) per Strategy by Secondary category',\n",
    "    fontsize=25,\n",
    "    y=1.10\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(categories_cap, rotation=0, ha='center')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.18), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err/vqa_accuracy_exact_match_per_category_strategies_sp4_secondary.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a341dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "# Get categories from split_4.csv\n",
    "meta = pd.read_csv('../data/dataset_V3/split_3.csv')[['id', 'category']]\n",
    "categories = sorted(meta['category'].dropna().unique())\n",
    "categories_cap = [cat.capitalize() for cat in categories]  # Capitalize\n",
    "\n",
    "accuracies_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_3.csv')\n",
    "    df['exact_match_bool'] = df['exact_match'].apply(lambda x: str(x).lower() == 'true')\n",
    "    # Merge to get 'category'\n",
    "    df = df.merge(meta, on='id', how='left')\n",
    "    df['category_cap'] = df['category'].str.capitalize()\n",
    "    accs = df.groupby('category_cap')['exact_match_bool'].mean().to_dict()\n",
    "    accs_ordered = [accs.get(cat, 0) for cat in categories_cap]\n",
    "    accuracies_by_strat.append(accs_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(categories_cap))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\", \"#faa307\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 19,\n",
    "    \"ytick.labelsize\": 19,\n",
    "    \"legend.fontsize\": 12,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7), constrained_layout=True)\n",
    "\n",
    "for strat_idx, (accs, color, strat_name) in enumerate(zip(accuracies_by_strat, colors, vqa_strat_names)):\n",
    "    ax.bar(index + bar_width*strat_idx - bar_width*4, accs, width=bar_width, color=color, label=strat_name)\n",
    "\n",
    "plt.suptitle(\n",
    "    'VQA Accuracy (Exact Match) per Strategy by Secondary category',\n",
    "    fontsize=25,\n",
    "    y=1.10\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Accuracy (Exact Match)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(categories_cap, rotation=0, ha='center')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.18), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/final_font/vqa_accuracy_exact_match_per_category_strategies_sp3_secondary.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "qa_strat_names = [\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT (without RAG)\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + Ideal RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI GPT-4o-mini + RAG + better prompt (without CoT)\",\n",
    "    \"OpenAI o4-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI o4-mini + CoT + better prompt (without RAG)\"\n",
    "]\n",
    "\n",
    "counts_by_strat = []\n",
    "for i in range(1, 8):\n",
    "    df = pd.read_csv(f'../results/qa/qa_strat_{i}_split_1_test.csv')\n",
    "    over, under, exact = 0, 0, 0\n",
    "    for _, row in df.iterrows():\n",
    "        # Make sure the lists are parsed, regardless of string format\n",
    "        try:\n",
    "            pred = ast.literal_eval(row['qa_result']) if isinstance(row['qa_result'], str) else row['qa_result']\n",
    "            gold = ast.literal_eval(row['correct_answers']) if isinstance(row['correct_answers'], str) else row['correct_answers']\n",
    "        except Exception:\n",
    "            continue  # skip broken rows\n",
    "        pred_count = len(pred) if isinstance(pred, list) else 0\n",
    "        gold_count = len(gold) if isinstance(gold, list) else 0\n",
    "        if pred_count > gold_count:\n",
    "            over += 1\n",
    "        elif pred_count < gold_count:\n",
    "            under += 1\n",
    "        else:\n",
    "            exact += 1\n",
    "    total = over + under + exact\n",
    "    # Save percentages\n",
    "    counts_by_strat.append([over/total, under/total, exact/total])\n",
    "\n",
    "counts_by_strat = np.array(counts_by_strat)  # shape (n_strats, 3)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(qa_strat_names))\n",
    "labels = [\"More than gold\", \"Less than gold\", \"Exact match\"]\n",
    "\n",
    "colors = [\"#e07a5f\", \"#81b29a\", \"#3572b0\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7), constrained_layout=True)\n",
    "for j in range(3):\n",
    "    ax.bar(index + bar_width*j - bar_width, counts_by_strat[:,j], width=bar_width, color=colors[j], label=labels[j])\n",
    "\n",
    "plt.suptitle(\n",
    "    'Tendency of Model to Select More, Less, or Exactly the Required Answers (QA, Split 1 Test)',\n",
    "    fontsize=17,\n",
    "    y=1.10\n",
    ")\n",
    "ax.set_xlabel('QA Strategy')\n",
    "ax.set_ylabel('Fraction of Questions')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(qa_strat_names, rotation=20, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.12), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err2/qa_tendency_over_under_exact_1_test.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "qa_strat_names = [\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT (without RAG)\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + Ideal RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI GPT-4o-mini + RAG + better prompt (without CoT)\",\n",
    "    \"OpenAI o4-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI o4-mini + CoT + better prompt (without RAG)\"\n",
    "]\n",
    "\n",
    "counts_by_strat = []\n",
    "for i in range(1, 8):\n",
    "    df = pd.read_csv(f'../results/qa/qa_strat_{i}_split_1_train.csv')\n",
    "    over, under, exact = 0, 0, 0\n",
    "    for _, row in df.iterrows():\n",
    "        # Make sure the lists are parsed, regardless of string format\n",
    "        try:\n",
    "            pred = ast.literal_eval(row['qa_result']) if isinstance(row['qa_result'], str) else row['qa_result']\n",
    "            gold = ast.literal_eval(row['correct_answers']) if isinstance(row['correct_answers'], str) else row['correct_answers']\n",
    "        except Exception:\n",
    "            continue  # skip broken rows\n",
    "        pred_count = len(pred) if isinstance(pred, list) else 0\n",
    "        gold_count = len(gold) if isinstance(gold, list) else 0\n",
    "        if pred_count > gold_count:\n",
    "            over += 1\n",
    "        elif pred_count < gold_count:\n",
    "            under += 1\n",
    "        else:\n",
    "            exact += 1\n",
    "    total = over + under + exact\n",
    "    # Save percentages\n",
    "    counts_by_strat.append([over/total, under/total, exact/total])\n",
    "\n",
    "counts_by_strat = np.array(counts_by_strat)  # shape (n_strats, 3)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(qa_strat_names))\n",
    "labels = [\"More than gold\", \"Less than gold\", \"Exact match\"]\n",
    "\n",
    "colors = [\"#e07a5f\", \"#81b29a\", \"#3572b0\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7), constrained_layout=True)\n",
    "for j in range(3):\n",
    "    ax.bar(index + bar_width*j - bar_width, counts_by_strat[:,j], width=bar_width, color=colors[j], label=labels[j])\n",
    "\n",
    "plt.suptitle(\n",
    "    'Tendency of Model to Select More, Less, or Exactly the Required Answers (QA, Split 1 Train)',\n",
    "    fontsize=17,\n",
    "    y=1.10\n",
    ")\n",
    "ax.set_xlabel('QA Strategy')\n",
    "ax.set_ylabel('Fraction of Questions')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(qa_strat_names, rotation=20, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.12), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err2/qa_tendency_over_under_exact_1_train.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "qa_strat_names = [\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT (without RAG)\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + Ideal RAG\",\n",
    "    \"OpenAI GPT-4o-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI GPT-4o-mini + RAG + better prompt (without CoT)\",\n",
    "    \"OpenAI o4-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI o4-mini + CoT + better prompt (without RAG)\"\n",
    "]\n",
    "\n",
    "counts_by_strat = []\n",
    "for i in range(1, 8):\n",
    "    df = pd.read_csv(f'../results/qa/qa_strat_{i}_split_2.csv')\n",
    "    over, under, exact = 0, 0, 0\n",
    "    for _, row in df.iterrows():\n",
    "        # Make sure the lists are parsed, regardless of string format\n",
    "        try:\n",
    "            pred = ast.literal_eval(row['qa_result']) if isinstance(row['qa_result'], str) else row['qa_result']\n",
    "            gold = ast.literal_eval(row['correct_answers']) if isinstance(row['correct_answers'], str) else row['correct_answers']\n",
    "        except Exception:\n",
    "            continue  # skip broken rows\n",
    "        pred_count = len(pred) if isinstance(pred, list) else 0\n",
    "        gold_count = len(gold) if isinstance(gold, list) else 0\n",
    "        if pred_count > gold_count:\n",
    "            over += 1\n",
    "        elif pred_count < gold_count:\n",
    "            under += 1\n",
    "        else:\n",
    "            exact += 1\n",
    "    total = over + under + exact\n",
    "    # Save percentages\n",
    "    counts_by_strat.append([over/total, under/total, exact/total])\n",
    "\n",
    "counts_by_strat = np.array(counts_by_strat)  # shape (n_strats, 3)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(qa_strat_names))\n",
    "labels = [\"More than gold\", \"Less than gold\", \"Exact match\"]\n",
    "\n",
    "colors = [\"#e07a5f\", \"#81b29a\", \"#3572b0\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7), constrained_layout=True)\n",
    "for j in range(3):\n",
    "    ax.bar(index + bar_width*j - bar_width, counts_by_strat[:,j], width=bar_width, color=colors[j], label=labels[j])\n",
    "\n",
    "plt.suptitle(\n",
    "    'Tendency of Model to Select More, Less, or Exactly the Required Answers (QA, Split 2)',\n",
    "    fontsize=17,\n",
    "    y=1.10\n",
    ")\n",
    "ax.set_xlabel('QA Strategy')\n",
    "ax.set_ylabel('Fraction of Questions')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(qa_strat_names, rotation=20, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.12), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err2/qa_tendency_over_under_exact_2.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "counts_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_4.csv')\n",
    "    over, under, exact = 0, 0, 0\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            pred = ast.literal_eval(row['qa_result']) if isinstance(row['qa_result'], str) else row['qa_result']\n",
    "            gold = ast.literal_eval(row['correct_answers']) if isinstance(row['correct_answers'], str) else row['correct_answers']\n",
    "        except Exception:\n",
    "            continue\n",
    "        pred_count = len(pred) if isinstance(pred, list) else 0\n",
    "        gold_count = len(gold) if isinstance(gold, list) else 0\n",
    "        if pred_count > gold_count:\n",
    "            over += 1\n",
    "        elif pred_count < gold_count:\n",
    "            under += 1\n",
    "        else:\n",
    "            exact += 1\n",
    "    total = over + under + exact\n",
    "    counts_by_strat.append([over/total, under/total, exact/total])\n",
    "\n",
    "counts_by_strat = np.array(counts_by_strat)  # shape (n_strats, 3)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.23\n",
    "index = np.arange(len(vqa_strat_names))\n",
    "labels = [\"More than gold\", \"Less than gold\", \"Exact match\"]\n",
    "colors = [\"#e07a5f\", \"#81b29a\", \"#3572b0\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 7), constrained_layout=True)\n",
    "for j in range(3):\n",
    "    ax.bar(index + bar_width*j - bar_width, counts_by_strat[:,j], width=bar_width, color=colors[j], label=labels[j])\n",
    "\n",
    "plt.suptitle(\n",
    "    'Tendency of VQA Model to Select More, Less, or Exactly the Required Answers (Split 4)',\n",
    "    fontsize=25,\n",
    "    y=1.11\n",
    ")\n",
    "ax.set_xlabel('VQA Strategy')\n",
    "ax.set_ylabel('Fraction of Questions')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(vqa_strat_names, rotation=23, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.12), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err2/vqa_tendency_over_under_exact_4.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec1d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "counts_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_3.csv')\n",
    "    over, under, exact = 0, 0, 0\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            pred = ast.literal_eval(row['qa_result']) if isinstance(row['qa_result'], str) else row['qa_result']\n",
    "            gold = ast.literal_eval(row['correct_answers']) if isinstance(row['correct_answers'], str) else row['correct_answers']\n",
    "        except Exception:\n",
    "            continue\n",
    "        pred_count = len(pred) if isinstance(pred, list) else 0\n",
    "        gold_count = len(gold) if isinstance(gold, list) else 0\n",
    "        if pred_count > gold_count:\n",
    "            over += 1\n",
    "        elif pred_count < gold_count:\n",
    "            under += 1\n",
    "        else:\n",
    "            exact += 1\n",
    "    total = over + under + exact\n",
    "    counts_by_strat.append([over/total, under/total, exact/total])\n",
    "\n",
    "counts_by_strat = np.array(counts_by_strat)  # shape (n_strats, 3)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.23\n",
    "index = np.arange(len(vqa_strat_names))\n",
    "labels = [\"More than gold\", \"Less than gold\", \"Exact match\"]\n",
    "colors = [\"#e07a5f\", \"#81b29a\", \"#3572b0\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 7), constrained_layout=True)\n",
    "for j in range(3):\n",
    "    ax.bar(index + bar_width*j - bar_width, counts_by_strat[:,j], width=bar_width, color=colors[j], label=labels[j])\n",
    "\n",
    "plt.suptitle(\n",
    "    'Tendency of VQA Model to Select More, Less, or Exactly the Required Answers (Split 3)',\n",
    "    fontsize=25,\n",
    "    y=1.11\n",
    ")\n",
    "ax.set_xlabel('VQA Strategy')\n",
    "ax.set_ylabel('Fraction of Questions')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(vqa_strat_names, rotation=23, ha='right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.12), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err2/vqa_tendency_over_under_exact_3.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "qa_strat_names = [\n",
    "    \"OpenAI o4-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI o4-mini + CoT + better prompt (without RAG)\"\n",
    "]\n",
    "\n",
    "# Load categories mapping\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "reasoning_avg_by_strat = []\n",
    "for i in range(6, 8):\n",
    "    df = pd.read_csv(f'../results/qa/qa_strat_{i}_split_1_test.csv')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # Merge with meta to get categories\n",
    "        meta = pd.read_csv('../data/dataset_V3/split_1_test.csv')[['id','question_category_id']]\n",
    "        df = df.merge(meta, on='id', how='left')\n",
    "    # Count occurrences of [REASONING] (case-insensitive)\n",
    "    df['reasoning_count'] = df['output_prompt'].astype(str).apply(lambda x: len(re.findall(r'\\[REASONING\\]', x, flags=re.IGNORECASE)))\n",
    "    # Group by category and take the average\n",
    "    avg_per_cat = df.groupby('question_category_id')['reasoning_count'].mean().to_dict()\n",
    "    # Map and order by all_cats\n",
    "    avg_per_cat_ordered = [avg_per_cat.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    reasoning_avg_by_strat.append(avg_per_cat_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.33\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\"#3572b0\", \"#81b29a\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8), constrained_layout=True)\n",
    "for strat_idx, (avgs, color, strat_name) in enumerate(zip(reasoning_avg_by_strat, colors, qa_strat_names)):\n",
    "    ax.bar(index + bar_width*strat_idx - bar_width/2, avgs, width=bar_width, color=color, label=strat_name)\n",
    "\n",
    "plt.suptitle(\n",
    "    'Average Number of reasoning steps in Output per Category (Split 1 Test)',\n",
    "    fontsize=25,\n",
    "    y=1.10\n",
    ")\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average # of reasoning steps')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, max(max(reasoning_avg_by_strat)) * 1.25)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err2/qa_reasoning_avg_occurrences_per_category_1_test.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afafb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "qa_strat_names = [\n",
    "    \"OpenAI o4-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI o4-mini + CoT + better prompt (without RAG)\"\n",
    "]\n",
    "\n",
    "# Load categories mapping\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "reasoning_avg_by_strat = []\n",
    "for i in range(6, 8):\n",
    "    df = pd.read_csv(f'../results/qa/qa_strat_{i}_split_1_train.csv')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # Merge with meta to get categories\n",
    "        meta = pd.read_csv('../data/dataset_V3/split_1_train.csv')[['id','question_category_id']]\n",
    "        df = df.merge(meta, on='id', how='left')\n",
    "    # Count occurrences of [REASONING] (case-insensitive)\n",
    "    df['reasoning_count'] = df['output_prompt'].astype(str).apply(lambda x: len(re.findall(r'\\[REASONING\\]', x, flags=re.IGNORECASE)))\n",
    "    # Group by category and take the average\n",
    "    avg_per_cat = df.groupby('question_category_id')['reasoning_count'].mean().to_dict()\n",
    "    # Map and order by all_cats\n",
    "    avg_per_cat_ordered = [avg_per_cat.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    reasoning_avg_by_strat.append(avg_per_cat_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.33\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\"#3572b0\", \"#81b29a\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "fig, ax = plt.subplots(figsize=(16, 8), constrained_layout=True)\n",
    "for strat_idx, (avgs, color, strat_name) in enumerate(zip(reasoning_avg_by_strat, colors, qa_strat_names)):\n",
    "    ax.bar(index + bar_width*strat_idx - bar_width/2, avgs, width=bar_width, color=color, label=strat_name)\n",
    "\n",
    "plt.suptitle(\n",
    "    'Average Number of reasoning steps in Output per Category (Split 1 Train)',\n",
    "    fontsize=25,\n",
    "    y=1.10\n",
    ")\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average # of reasoning steps')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, max(max(reasoning_avg_by_strat)) * 1.25)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err2/qa_reasoning_avg_occurrences_per_category_1_train.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad02b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "qa_strat_names = [\n",
    "    \"OpenAI o4-mini + CoT + RAG + better prompt\",\n",
    "    \"OpenAI o4-mini + CoT + better prompt (without RAG)\"\n",
    "]\n",
    "\n",
    "# Load categories mapping\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "reasoning_avg_by_strat = []\n",
    "for i in range(6, 8):\n",
    "    df = pd.read_csv(f'../results/qa/qa_strat_{i}_split_2.csv')\n",
    "    if 'question_category_id' not in df.columns:\n",
    "        # Merge with meta to get categories\n",
    "        meta = pd.read_csv('../data/dataset_V3/split_2.csv')[['id','question_category_id']]\n",
    "        df = df.merge(meta, on='id', how='left')\n",
    "    # Count occurrences of [REASONING] (case-insensitive)\n",
    "    df['reasoning_count'] = df['output_prompt'].astype(str).apply(lambda x: len(re.findall(r'\\[REASONING\\]', x, flags=re.IGNORECASE)))\n",
    "    # Group by category and take the average\n",
    "    avg_per_cat = df.groupby('question_category_id')['reasoning_count'].mean().to_dict()\n",
    "    # Map and order by all_cats\n",
    "    avg_per_cat_ordered = [avg_per_cat.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    reasoning_avg_by_strat.append(avg_per_cat_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.33\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\"#3572b0\", \"#81b29a\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8), constrained_layout=True)\n",
    "for strat_idx, (avgs, color, strat_name) in enumerate(zip(reasoning_avg_by_strat, colors, qa_strat_names)):\n",
    "    ax.bar(index + bar_width*strat_idx - bar_width/2, avgs, width=bar_width, color=color, label=strat_name)\n",
    "\n",
    "plt.suptitle(\n",
    "    'Average Number of reasoning steps in Output per Category (Split 2)',\n",
    "    fontsize=25,\n",
    "    y=1.10\n",
    ")\n",
    "ax.set_xlabel('Question Category')\n",
    "ax.set_ylabel('Average # of reasoning steps')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, max(max(reasoning_avg_by_strat)) * 1.25)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.13), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err2/qa_reasoning_avg_occurrences_per_category_2.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "# Get categories from split_4.csv (and capitalize)\n",
    "meta = pd.read_csv('../data/dataset_V3/split_4.csv')[['id', 'category']]\n",
    "categories = sorted(meta['category'].dropna().unique())\n",
    "categories_cap = [cat.capitalize() for cat in categories]\n",
    "\n",
    "reasoning_avg_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_4.csv')\n",
    "    # Merge to get 'category'\n",
    "    df = df.merge(meta, on='id', how='left')\n",
    "    df['category_cap'] = df['category'].str.capitalize()\n",
    "    # Count occurrences of [REASONING] (case-insensitive)\n",
    "    df['reasoning_count'] = df['output_prompt'].astype(str).apply(lambda x: len(re.findall(r'\\[REASONING\\]', x, flags=re.IGNORECASE)))\n",
    "    # Group by category and take the average\n",
    "    avg_per_cat = df.groupby('category_cap')['reasoning_count'].mean().to_dict()\n",
    "    # Map and order by categories_cap\n",
    "    avg_per_cat_ordered = [avg_per_cat.get(cat, 0) for cat in categories_cap]\n",
    "    reasoning_avg_by_strat.append(avg_per_cat_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(categories_cap))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\", \"#faa307\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 13,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7), constrained_layout=True)\n",
    "\n",
    "for strat_idx, (avgs, color, strat_name) in enumerate(zip(reasoning_avg_by_strat, colors, vqa_strat_names)):\n",
    "    ax.bar(index + bar_width*strat_idx - bar_width*4, avgs, width=bar_width, color=color, label=strat_name)\n",
    "\n",
    "plt.suptitle(\n",
    "    'Average Number of reasoning steps in Output per Category (VQA Strategies, Split 4)',\n",
    "    fontsize=17,\n",
    "    y=1.12\n",
    ")\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Average # of [REASONING] Occurrences')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(categories_cap, rotation=0, ha='center')\n",
    "ax.set_ylim(0, max(max(reasoning_avg_by_strat)) * 1.25)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/final_font/vqa_reasoning_avg_occurrences_per_category_strategies_secondary_sp4.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ee819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "# Get categories from split_4.csv (and capitalize)\n",
    "meta = pd.read_csv('../data/dataset_V3/split_3.csv')[['id', 'category']]\n",
    "categories = sorted(meta['category'].dropna().unique())\n",
    "categories_cap = [cat.capitalize() for cat in categories]\n",
    "\n",
    "reasoning_avg_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_3.csv')\n",
    "    # Merge to get 'category'\n",
    "    df = df.merge(meta, on='id', how='left')\n",
    "    df['category_cap'] = df['category'].str.capitalize()\n",
    "    # Count occurrences of [REASONING] (case-insensitive)\n",
    "    df['reasoning_count'] = df['output_prompt'].astype(str).apply(lambda x: len(re.findall(r'\\[REASONING\\]', x, flags=re.IGNORECASE)))\n",
    "    # Group by category and take the average\n",
    "    avg_per_cat = df.groupby('category_cap')['reasoning_count'].mean().to_dict()\n",
    "    # Map and order by categories_cap\n",
    "    avg_per_cat_ordered = [avg_per_cat.get(cat, 0) for cat in categories_cap]\n",
    "    reasoning_avg_by_strat.append(avg_per_cat_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(categories_cap))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\", \"#faa307\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7), constrained_layout=True)\n",
    "\n",
    "for strat_idx, (avgs, color, strat_name) in enumerate(zip(reasoning_avg_by_strat, colors, vqa_strat_names)):\n",
    "    ax.bar(index + bar_width*strat_idx - bar_width*4, avgs, width=bar_width, color=color, label=strat_name)\n",
    "\n",
    "plt.suptitle(\n",
    "    'Average Number of reasoning steps in Output per Category (VQA Strategies, Split 3)',\n",
    "    fontsize=17,\n",
    "    y=1.12\n",
    ")\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Average # of [REASONING] Occurrences')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(categories_cap, rotation=0, ha='center')\n",
    "ax.set_ylim(0, max(max(reasoning_avg_by_strat)) * 1.25)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/final_font/vqa_reasoning_avg_occurrences_per_category_strategies_secondary_sp3.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "# Load category mapping (question_category_id to English)\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Get question_category_id from split_4\n",
    "meta = pd.read_csv('../data/dataset_V3/split_4.csv')[['id', 'question_category_id']]\n",
    "\n",
    "reasoning_avg_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_4.csv')\n",
    "    # Merge to get 'question_category_id'\n",
    "    df = df.merge(meta, on='id', how='left')\n",
    "    # Count occurrences of [REASONING] (case-insensitive)\n",
    "    df['reasoning_count'] = df['output_prompt'].astype(str).apply(lambda x: len(re.findall(r'\\[REASONING\\]', x, flags=re.IGNORECASE)))\n",
    "    # Group by category and take the average\n",
    "    avg_per_cat = df.groupby('question_category_id')['reasoning_count'].mean().to_dict()\n",
    "    # Map and order by all_cats\n",
    "    avg_per_cat_ordered = [avg_per_cat.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    reasoning_avg_by_strat.append(avg_per_cat_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\", \"#faa307\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 19,\n",
    "    \"ytick.labelsize\": 19,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(21, 8), constrained_layout=True)\n",
    "\n",
    "for strat_idx, (avgs, color, strat_name) in enumerate(zip(reasoning_avg_by_strat, colors, vqa_strat_names)):\n",
    "    ax.bar(index + bar_width*strat_idx - bar_width*4, avgs, width=bar_width, color=color, label=strat_name)\n",
    "\n",
    "plt.suptitle(\n",
    "    'Average Number of [REASONING] Occurrences in Output Prompt per Primary Category (VQA, Split 4)',\n",
    "    fontsize=25,\n",
    "    y=1.12\n",
    ")\n",
    "ax.set_xlabel('Primary Question Category')\n",
    "ax.set_ylabel('Average # of [REASONING] Occurrences')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, max(max(reasoning_avg_by_strat)) * 1.25)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err3/vqa_reasoning_avg_occurrences_per_question_category_strategies_sp4.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "vqa_strat_names = [\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + CoT (without RAG)\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + Ideal RAG\",\n",
    "    \"OpenAI o4-mini + Caption + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + QA + CoT + RAG\",\n",
    "    \"OpenAI o4-mini + Image + Caption + QA + RAG\"\n",
    "]\n",
    "\n",
    "# Load category mapping (question_category_id to English)\n",
    "cat_stats = pd.read_csv('../data/dataset_V3/categories_stats.csv')\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "cat_stats['category_name_en'] = cat_stats['category_name'].map(translations)\n",
    "cat_id_to_name = cat_stats.set_index('question_category_id')['category_name_en'].to_dict()\n",
    "all_cats = cat_stats['category_name_en'].dropna().tolist()\n",
    "cat_id_list = cat_stats['question_category_id'].tolist()\n",
    "\n",
    "# Get question_category_id from split_4\n",
    "meta = pd.read_csv('../data/dataset_V3/split_3.csv')[['id', 'question_category_id']]\n",
    "\n",
    "reasoning_avg_by_strat = []\n",
    "for i in range(1, 10):\n",
    "    df = pd.read_csv(f'../results/vqa/vqa_strat_{i}_split_3.csv')\n",
    "    # Merge to get 'question_category_id'\n",
    "    df = df.merge(meta, on='id', how='left')\n",
    "    # Count occurrences of [REASONING] (case-insensitive)\n",
    "    df['reasoning_count'] = df['output_prompt'].astype(str).apply(lambda x: len(re.findall(r'\\[REASONING\\]', x, flags=re.IGNORECASE)))\n",
    "    # Group by category and take the average\n",
    "    avg_per_cat = df.groupby('question_category_id')['reasoning_count'].mean().to_dict()\n",
    "    # Map and order by all_cats\n",
    "    avg_per_cat_ordered = [avg_per_cat.get(cat_id, 0) for cat_id in cat_id_list]\n",
    "    reasoning_avg_by_strat.append(avg_per_cat_ordered)\n",
    "\n",
    "# Plot\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(all_cats))\n",
    "colors = [\n",
    "    \"#3572b0\", \"#e07a5f\", \"#81b29a\", \"#f2cc8f\",\n",
    "    \"#6d597a\", \"#b56576\", \"#3d5a80\", \"#43aa8b\", \"#faa307\"\n",
    "]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 25,\n",
    "    \"axes.titlesize\": 25,\n",
    "    \"axes.labelsize\": 25,\n",
    "    \"xtick.labelsize\": 19,\n",
    "    \"ytick.labelsize\": 19,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(21, 8), constrained_layout=True)\n",
    "\n",
    "for strat_idx, (avgs, color, strat_name) in enumerate(zip(reasoning_avg_by_strat, colors, vqa_strat_names)):\n",
    "    ax.bar(index + bar_width*strat_idx - bar_width*4, avgs, width=bar_width, color=color, label=strat_name)\n",
    "\n",
    "plt.suptitle(\n",
    "    'Average number of reasoning steps (VQA, Split 3)',\n",
    "    fontsize=25,\n",
    "    y=1.12\n",
    ")\n",
    "ax.set_xlabel('Primary Question Category')\n",
    "ax.set_ylabel('Average # of reasoning steps')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(all_cats, rotation=30, ha='right')\n",
    "ax.set_ylim(0, max(max(reasoning_avg_by_strat)) * 1.25)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, frameon=False)\n",
    "\n",
    "plt.savefig('../plots/new_font_err3/vqa_reasoning_avg_occurrences_per_question_category_strategies_sp3.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51444657",
   "metadata": {},
   "source": [
    "# Reviews analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "corpus_path = \"../data/dataset_V3/corpus.csv\"\n",
    "df = pd.read_csv(corpus_path)\n",
    "\n",
    "# Group by title_metadata and count occurrences\n",
    "counts = df.groupby(\"title_metadata\").size().reset_index(name=\"count\")\n",
    "\n",
    "# Sort for better visualization\n",
    "counts = counts.sort_values(\"count\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(counts[\"title_metadata\"], counts[\"count\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"title_metadata\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Counts of title_metadata in corpus\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "corpus_path = \"../data/dataset_V3/corpus.csv\"\n",
    "df = pd.read_csv(corpus_path)\n",
    "\n",
    "# Group by title_metadata and count occurrences\n",
    "counts = df.groupby(\"title_metadata\").size().reset_index(name=\"count\")\n",
    "\n",
    "# Sort by count descending\n",
    "counts = counts.sort_values(\"count\", ascending=False)\n",
    "\n",
    "# Print as list\n",
    "for _, row in counts.iterrows():\n",
    "    print(f\"{row['title_metadata']}: {row['count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load dataset\n",
    "corpus_path = \"../data/dataset_V3/corpus.csv\"\n",
    "df = pd.read_csv(corpus_path)\n",
    "\n",
    "# Group by title_metadata and count occurrences\n",
    "counts = df.groupby(\"title_metadata\").size().reset_index(name=\"count\")\n",
    "\n",
    "# Sort by count descending\n",
    "counts = counts.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Ensure pandas shows full text\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Display as table\n",
    "display(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72cc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"read ../data/dataset_V3/split_1.csv 2 3 and 4, and merge them and keep only questions which have question_category_id == 7.\n",
    "\n",
    "now, take legislation column from each, which is a list of strings. count all the unique values from all the entries and output the length\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Paths to your split CSVs\n",
    "base_path = \"../data/dataset_V3\"\n",
    "files = [f\"{base_path}/split_{i}.csv\" for i in [1, 2, 3, 4]]\n",
    "\n",
    "# Read and merge\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Collect all legislation values (lists of strings)\n",
    "unique_legislation = set()\n",
    "\n",
    "for entry in merged[\"legislation\"].dropna():\n",
    "    # Convert string representation of list to actual list\n",
    "    try:\n",
    "        laws = ast.literal_eval(entry) if isinstance(entry, str) else entry\n",
    "        unique_legislation.update(laws)\n",
    "    except Exception:\n",
    "        pass  # skip malformed entries\n",
    "\n",
    "# Output the number of unique values\n",
    "print(\"Number of unique legislation entries:\", len(unique_legislation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_legislation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b46707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load dataset\n",
    "corpus_path = \"../data/dataset_V3/corpus.csv\"\n",
    "df = pd.read_csv(corpus_path)\n",
    "\n",
    "# Extract doc_source from id (everything before the first '-')\n",
    "df[\"doc_source\"] = df[\"id\"].str.split(\"-\", n=1).str[0]\n",
    "\n",
    "# Group by title_metadata and count\n",
    "counts = df.groupby(\"title_metadata\").size().reset_index(name=\"count\")\n",
    "\n",
    "# Now coalesce RCA, ITP, PENAL into a single group\n",
    "coalesce_sources = [\"RCA\", \"ITP\", \"PENAL\"]\n",
    "\n",
    "# Sum their counts\n",
    "coalesce_sum = df[df[\"doc_source\"].isin(coalesce_sources)].groupby(\"title_metadata\").size().sum()\n",
    "\n",
    "# Add as a new row\n",
    "counts = pd.concat(\n",
    "    [\n",
    "        counts,\n",
    "        pd.DataFrame([{\"title_metadata\": \"RCA+ITP+PENAL\", \"count\": coalesce_sum}])\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Sort by count descending\n",
    "counts = counts.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Ensure pandas shows full text\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Display table\n",
    "display(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load dataset\n",
    "corpus_path = \"../data/dataset_V3/corpus.csv\"\n",
    "df = pd.read_csv(corpus_path)\n",
    "\n",
    "# Extract doc_source from id (everything before the first '-')\n",
    "df[\"doc_source\"] = df[\"id\"].str.split(\"-\", n=1).str[0]\n",
    "\n",
    "# Group by title_metadata and count\n",
    "counts = df.groupby(\"title_metadata\").size().reset_index(name=\"count\")\n",
    "\n",
    "# Sources to coalesce\n",
    "coalesce_sources = [\"RCA\", \"ITP\", \"PENAL\"]\n",
    "\n",
    "# Aggregate counts at the doc_source level for those sources\n",
    "agg_counts = (\n",
    "    df[df[\"doc_source\"].isin(coalesce_sources)]\n",
    "    .groupby(\"doc_source\")\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .rename(columns={\"doc_source\": \"title_metadata\"})\n",
    ")\n",
    "\n",
    "# Remove rows that came from those sources\n",
    "counts = counts[~counts[\"title_metadata\"].isin(df.loc[df[\"doc_source\"].isin(coalesce_sources), \"title_metadata\"])]\n",
    "\n",
    "# Add aggregated rows back\n",
    "counts = pd.concat([counts, agg_counts], ignore_index=True)\n",
    "\n",
    "# Sort by count descending\n",
    "counts = counts.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Ensure pandas shows full text\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Display\n",
    "display(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5c194",
   "metadata": {},
   "source": [
    "# hallucination / citings / etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "take experiment cot+rag+better prompt (same scenario) for\n",
    "- o4-mini (best reasoning) - ./results/qa/qa_strat_6_split_{}. 1_train, 1_test, 2\n",
    "- gemma 3 (best open) - ./results/qa_vllm/qa_strat_4_split_{}_vllm. 1_train, 1_test, 2\n",
    "- Mistral (worst) - ./results/qa_vllm/qa_strat_4_split_{}_vllm. 1_train, 1_test, 2\n",
    "\n",
    "we need aggregate model, split, id, output_prompt, exact_match\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f18ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-cell script to read the 9 result DataFrames, normalize, and concatenate\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# --- Config (edit here if paths change) ---\n",
    "splits = [\"1_train\", \"1_test\", \"2\"]\n",
    "\n",
    "specs = [\n",
    "    # (model_display_name, base_path_with_{split})\n",
    "    (\"o4-mini (best reasoning)\", \"../results/qa/qa_strat_6_split_{}\"),\n",
    "    (\"Mistral (worst)\",     \"../results/qa_vllm/qa_strat_4_split_{}_vllm\"),\n",
    "    (\"Gemma (best open)\",         \"../results/qa_vllm2/qa_strat_4_split_{}_vllm\"),\n",
    "]\n",
    "\n",
    "# --- Helpers ---\n",
    "def _guess_file(path_str: str) -> Path:\n",
    "    \"\"\"\n",
    "    Given a base path that may be a file without extension or a directory,\n",
    "    try to resolve to a concrete file we can read.\n",
    "    Tries common extensions and common filenames inside directories.\n",
    "    \"\"\"\n",
    "    p = Path(path_str)\n",
    "    # If path already points to a file, use it\n",
    "    if p.is_file():\n",
    "        return p\n",
    "\n",
    "    # Try with common single-file extensions\n",
    "    for ext in (\".parquet\", \".csv\", \".jsonl\", \".json\"):\n",
    "        q = Path(path_str + ext)\n",
    "        if q.exists() and q.is_file():\n",
    "            return q\n",
    "\n",
    "    # If it's a directory, try common filenames; else try first matching file by extension\n",
    "    if p.exists() and p.is_dir():\n",
    "        preferred_names = [\n",
    "            \"predictions.parquet\", \"results.parquet\", \"df.parquet\", \"data.parquet\",\n",
    "            \"predictions.csv\", \"results.csv\", \"df.csv\", \"data.csv\",\n",
    "            \"predictions.jsonl\", \"results.jsonl\", \"df.jsonl\", \"data.jsonl\",\n",
    "            \"predictions.json\", \"results.json\", \"df.json\", \"data.json\",\n",
    "        ]\n",
    "        for name in preferred_names:\n",
    "            q = p / name\n",
    "            if q.exists() and q.is_file():\n",
    "                return q\n",
    "        # Fallback: first file by extension priority\n",
    "        for ext in (\"*.parquet\", \"*.csv\", \"*.jsonl\", \"*.json\"):\n",
    "            matches = sorted(p.glob(ext))\n",
    "            if matches:\n",
    "                return matches[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not resolve a file for base path: {path_str}\")\n",
    "\n",
    "def _read_df(resolved: Path) -> pd.DataFrame:\n",
    "    if resolved.suffix == \".parquet\":\n",
    "        return pd.read_parquet(resolved)\n",
    "    if resolved.suffix == \".csv\":\n",
    "        return pd.read_csv(resolved)\n",
    "    if resolved.suffix in (\".jsonl\",):\n",
    "        return pd.read_json(resolved, lines=True)\n",
    "    if resolved.suffix in (\".json\",):\n",
    "        # try records; if not, fallback to auto-infer\n",
    "        try:\n",
    "            with open(resolved, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                return pd.DataFrame(data)\n",
    "            # try nested under \"data\" or \"results\"\n",
    "            for key in (\"data\", \"results\"):\n",
    "                if key in data and isinstance(data[key], list):\n",
    "                    return pd.DataFrame(data[key])\n",
    "            # last resort\n",
    "            return pd.json_normalize(data)\n",
    "        except Exception:\n",
    "            return pd.read_json(resolved)\n",
    "    raise ValueError(f\"Unsupported file type: {resolved}\")\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates) -> str:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # try case-insensitive\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in lower:\n",
    "            return lower[c.lower()]\n",
    "    raise KeyError(f\"None of the candidate columns found: {candidates}\\nAvailable: {list(df.columns)}\")\n",
    "\n",
    "# Candidate column names to normalize\n",
    "ID_CANDS          = [\"id\", \"example_id\", \"sample_id\", \"question_id\", \"qid\", \"row_id\"]\n",
    "OUTPUT_PROMPT_CANDS = [\"output_prompt\", \"final_prompt\", \"prompt_out\", \"model_output\", \"output\", \"response\", \"answer\"]\n",
    "EXACT_MATCH_CANDS = [\"exact_match\", \"em\", \"is_exact_match\", \"match_exact\", \"exact\"]\n",
    "\n",
    "# --- Load, normalize, concat ---\n",
    "frames = []\n",
    "provenance = []  # keep track for debugging\n",
    "\n",
    "for model_name, base in specs:\n",
    "    for split in splits:\n",
    "        base_path = base.format(split)\n",
    "        resolved = _guess_file(base_path)\n",
    "        df = _read_df(resolved)\n",
    "\n",
    "        id_col     = _pick_col(df, ID_CANDS)\n",
    "        out_col    = _pick_col(df, OUTPUT_PROMPT_CANDS)\n",
    "        exact_col  = _pick_col(df, EXACT_MATCH_CANDS)\n",
    "\n",
    "        sub = df[[id_col, out_col, exact_col]].copy()\n",
    "        sub.columns = [\"id\", \"output_prompt\", \"exact_match\"]\n",
    "        sub.insert(0, \"model\", model_name)\n",
    "        sub.insert(1, \"split\", split)\n",
    "\n",
    "        frames.append(sub)\n",
    "        provenance.append({\"model\": model_name, \"split\": split, \"path\": str(resolved), \"rows\": len(sub)})\n",
    "\n",
    "# Final aggregated DataFrame\n",
    "agg = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Ensure exact_match is boolean if possible\n",
    "if agg[\"exact_match\"].dtype == object:\n",
    "    # try to coerce strings like \"true\"/\"false\"/\"1\"/\"0\"\n",
    "    agg[\"exact_match\"] = agg[\"exact_match\"].map(\n",
    "        lambda x: True if str(x).strip().lower() in {\"1\",\"true\",\"t\",\"yes\",\"y\"} \n",
    "        else False if str(x).strip().lower() in {\"0\",\"false\",\"f\",\"no\",\"n\"} \n",
    "        else x\n",
    "    )\n",
    "\n",
    "# Reorder columns exactly as requested\n",
    "agg = agg[[\"model\", \"split\", \"id\", \"output_prompt\", \"exact_match\"]]\n",
    "\n",
    "# Display a brief summary and show the first few rows for sanity check (comment out if unwanted)\n",
    "print(\"Loaded pieces:\")\n",
    "for p in provenance:\n",
    "    print(f\"- {p['model']} | {p['split']} | {p['rows']} rows | {p['path']}\")\n",
    "print(\"\\nAggregated shape:\", agg.shape)\n",
    "display(agg.head(10))\n",
    "\n",
    "# If you want to save the result, uncomment:\n",
    "# agg.to_parquet(\"./results/qa_aggregated.parquet\", index=False)\n",
    "agg.to_csv(\"../citations-analytics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ed7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-cell: add `citations_number` by counting specified regex occurrences in `output_prompt` (case-insensitive)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory from the previous step.\")\n",
    "if \"output_prompt\" not in agg.columns:\n",
    "    raise KeyError(\"`agg` must have an `output_prompt` column.\")\n",
    "\n",
    "# Make sure output_prompt is string\n",
    "texts = agg[\"output_prompt\"].fillna(\"\").astype(str)\n",
    "\n",
    "# --- Define the patterns exactly as requested (case-insensitive) ---\n",
    "# For the {number} placeholder we use (\\d+). Hyphens and spaces are matched literally as provided.\n",
    "patterns = [\n",
    "    r\"\\bRegulament-(\\d+)\\b\",\n",
    "    r\"\\bReg-(\\d+)\\b\",\n",
    "    r\"\\bReg\\.\\s*(\\d+)\\b\",\n",
    "    r\"\\breg\\.-\\s*(\\d+)\\b\",        # covers \"reg.-{number}\"\n",
    "    r\"\\bRegulament\\s+(\\d+)\\b\",\n",
    "    r\"\\breg\\.\\s*(\\d+)\\b\",\n",
    "    r\"\\bRegulamentul-(\\d+)\\b\",\n",
    "    r\"\\bRegulamentul\\s+(\\d+)\\b\",\n",
    "\n",
    "    r\"\\boug-(\\d+)\\b\",\n",
    "    r\"\\boug\\s+(\\d+)\\b\",\n",
    "    r\"ordonanței de urgen\",       # literal substring, as provided\n",
    "\n",
    "    r\"\\bITP-(\\d+)\\b\",\n",
    "\n",
    "    r\"\\bRCA-(\\d+)\\b\",\n",
    "\n",
    "    r\"\\bPENAL-(\\d+)\\b\",\n",
    "\n",
    "    r\"\\bCodul\\s+penal\\b\",\n",
    "    r\"\\bCod\\s+penal\\b\",\n",
    "]\n",
    "\n",
    "compiled = [re.compile(p, flags=re.IGNORECASE | re.UNICODE) for p in patterns]\n",
    "\n",
    "def count_all_patterns(text: str) -> int:\n",
    "    total = 0\n",
    "    for rx in compiled:\n",
    "        # Use finditer to count non-overlapping matches of each pattern\n",
    "        total += sum(1 for _ in rx.finditer(text))\n",
    "    return total\n",
    "\n",
    "agg[\"citations_number\"] = texts.apply(count_all_patterns)\n",
    "\n",
    "# Optional: quick peek\n",
    "print(\"Added `citations_number`. Example rows:\")\n",
    "display(agg[[\"model\", \"split\", \"id\", \"output_prompt\", \"citations_number\"]].head(10))\n",
    "agg.to_csv(\"../citations-analytics-number.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory.\")\n",
    "if \"citations_number\" not in agg.columns:\n",
    "    raise KeyError(\"`agg` must have a `citations_number` column.\")\n",
    "\n",
    "# --- Plot distribution of citations per model ---\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for model in agg[\"model\"].unique():\n",
    "    subset = agg.loc[agg[\"model\"] == model, \"citations_number\"]\n",
    "    plt.hist(\n",
    "        subset,\n",
    "        bins=range(0, subset.max() + 2),\n",
    "        alpha=0.5,\n",
    "        label=model,\n",
    "        density=True\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Citations number\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of citations per model\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-model, per-split: number of correct (exact_match=True) / total, formatted as ratio and fraction\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory.\")\n",
    "if \"exact_match\" not in agg.columns:\n",
    "    raise KeyError(\"`agg` must have an `exact_match` column.\")\n",
    "\n",
    "# Group by model and split\n",
    "summary = (\n",
    "    agg.groupby([\"model\", \"split\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x == True).sum()),\n",
    "           total=(\"exact_match\", \"size\")\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "\n",
    "summary[\"ratio\"] = summary[\"correct\"] / summary[\"total\"]\n",
    "\n",
    "# Format as \"correct / total\" and decimal ratio\n",
    "summary[\"performance\"] = summary[\"correct\"].astype(str) + \" / \" + summary[\"total\"].astype(str)\n",
    "summary[\"ratio_formatted\"] = summary[\"ratio\"].map(lambda x: f\"{x:.3f}\")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory.\")\n",
    "if \"citations_number\" not in agg.columns:\n",
    "    raise KeyError(\"`agg` must have a `citations_number` column.\")\n",
    "\n",
    "# Count how many examples fall into each citations_number bin, per model\n",
    "counts = (\n",
    "    agg.groupby([\"model\", \"citations_number\"])\n",
    "       .size()\n",
    "       .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Pivot into wide format for bar plotting\n",
    "pivot = counts.pivot(index=\"citations_number\", columns=\"model\", values=\"count\").fillna(0)\n",
    "\n",
    "# Plot grouped bar chart\n",
    "ax = pivot.plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(12,6),\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Citations number\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of citations per model\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter: average citations vs exact-match accuracy (per model & split) with trend line\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Preconditions ---\n",
    "required_cols = {\"model\", \"split\", \"citations_number\", \"exact_match\", \"id\"}\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory.\")\n",
    "missing = required_cols - set(agg.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"`agg` is missing required columns: {missing}\")\n",
    "\n",
    "# --- Aggregate per (model, split) ---\n",
    "summary = (\n",
    "    agg.groupby([\"model\", \"split\"], as_index=False)\n",
    "       .agg(\n",
    "           avg_citations=(\"citations_number\", \"mean\"),\n",
    "           accuracy=(\"exact_match\", lambda x: (x == True).mean()),\n",
    "           total=(\"id\", \"size\")\n",
    "       )\n",
    ")\n",
    "\n",
    "# --- Scatter plot ---\n",
    "plt.figure(figsize=(9,6))\n",
    "x = summary[\"avg_citations\"].to_numpy()\n",
    "y = summary[\"accuracy\"].to_numpy()\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Annotate each point with model | split and n\n",
    "for _, row in summary.iterrows():\n",
    "    label = f\"{row['model']} | {row['split']} (n={int(row['total'])})\"\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        (row[\"avg_citations\"], row[\"accuracy\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(6, 6),\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "# --- Simple linear regression line and R^2 ---\n",
    "if len(summary) >= 2 and np.isfinite(x).all() and np.isfinite(y).all():\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    x_line = np.linspace(x.min(), x.max(), 100)\n",
    "    y_line = slope * x_line + intercept\n",
    "    plt.plot(x_line, y_line)\n",
    "\n",
    "    # Pearson r and R^2\n",
    "    r = np.corrcoef(x, y)[0, 1] if x.size > 1 else np.nan\n",
    "    r2 = r**2 if np.isfinite(r) else np.nan\n",
    "    plt.title(f\"Avg citations vs Exact-match accuracy\\nLinear trend (R² = {r2:.3f})\")\n",
    "else:\n",
    "    plt.title(\"Avg citations vs Exact-match accuracy\")\n",
    "\n",
    "plt.xlabel(\"Average citations per answer\")\n",
    "plt.ylabel(\"Exact-match accuracy (0–1)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: also print the summary table for reference\n",
    "summary.sort_values([\"model\", \"split\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc438035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory.\")\n",
    "if \"citations_number\" not in agg.columns or \"exact_match\" not in agg.columns:\n",
    "    raise KeyError(\"`agg` must have `citations_number` and `exact_match` columns.\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(\n",
    "    data=agg,\n",
    "    x=\"model\",\n",
    "    y=\"citations_number\",\n",
    "    hue=\"exact_match\",      # split correct vs wrong\n",
    "    showfliers=False\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Citations number\")\n",
    "plt.title(\"Distribution of citations in Correct vs Wrong answers\")\n",
    "plt.legend(title=\"Exact Match\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory.\")\n",
    "if \"citations_number\" not in agg.columns or \"exact_match\" not in agg.columns:\n",
    "    raise KeyError(\"`agg` must have `citations_number` and `exact_match` columns.\")\n",
    "\n",
    "# Combine model and split into one label for plotting\n",
    "agg[\"model_split\"] = agg[\"model\"] + \" | \" + agg[\"split\"]\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.boxplot(\n",
    "    data=agg,\n",
    "    x=\"model_split\",\n",
    "    y=\"citations_number\",\n",
    "    hue=\"exact_match\",\n",
    "    showfliers=False\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Model | Split\")\n",
    "plt.ylabel(\"Citations number\")\n",
    "plt.title(\"Distribution of citations in Correct vs Wrong answers (per Model & Split)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend(title=\"Exact Match\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d30190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory.\")\n",
    "if \"citations_number\" not in agg.columns or \"exact_match\" not in agg.columns:\n",
    "    raise KeyError(\"`agg` must have `citations_number` and `exact_match` columns.\")\n",
    "\n",
    "summary = (\n",
    "    agg.groupby([\"model\", \"split\"])\n",
    "       .agg(\n",
    "           avg_citations=(\"citations_number\", \"mean\"),\n",
    "           correct=(\"exact_match\", lambda x: (x == True).sum()),\n",
    "           total=(\"exact_match\", \"size\")\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "\n",
    "summary[\"accuracy\"] = summary[\"correct\"] / summary[\"total\"]\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory.\")\n",
    "if not {\"model\",\"split\",\"citations_number\",\"exact_match\"} <= set(agg.columns):\n",
    "    raise KeyError(\"agg must have columns: model, split, citations_number, exact_match\")\n",
    "\n",
    "# Compute accuracy per (model, split, citations_number)\n",
    "acc_by_cite = (\n",
    "    agg.groupby([\"model\",\"split\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x == True).sum()),\n",
    "           total=(\"exact_match\", \"size\")\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "acc_by_cite[\"accuracy\"] = acc_by_cite[\"correct\"] / acc_by_cite[\"total\"]\n",
    "\n",
    "# Plot: one subplot per model, lines for each split\n",
    "models = acc_by_cite[\"model\"].unique()\n",
    "n_models = len(models)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(6*n_models,5), sharey=True)\n",
    "\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, model in zip(axes, models):\n",
    "    sub = acc_by_cite[acc_by_cite[\"model\"] == model]\n",
    "    for split in sub[\"split\"].unique():\n",
    "        ss = sub[sub[\"split\"] == split]\n",
    "        ax.plot(\n",
    "            ss[\"citations_number\"], ss[\"accuracy\"], \n",
    "            marker=\"o\", label=split\n",
    "        )\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel(\"Citations number\")\n",
    "    ax.set_ylabel(\"Exact-match accuracy (0–1)\")\n",
    "    ax.set_xticks(sorted(sub[\"citations_number\"].unique()))\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend(title=\"Split\")\n",
    "\n",
    "plt.suptitle(\"Accuracy vs Citations count per Model & Split\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd22fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory from the previous step.\")\n",
    "if \"output_prompt\" not in agg.columns:\n",
    "    raise KeyError(\"`agg` must have an `output_prompt` column.\")\n",
    "\n",
    "# Ensure string\n",
    "texts = agg[\"output_prompt\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Helper: only accept pure numbers (no x/y)\n",
    "# We enforce this via a negative lookahead after the number so \\d+/\\d+ is ignored\n",
    "NUM = r\"(\\d+)(?!\\s*/\\s*\\d)\"\n",
    "\n",
    "# --- Compile patterns for counting (kept from your approach) ---\n",
    "count_patterns = [\n",
    "    r\"\\bRegulament-\" + NUM + r\"\\b\",\n",
    "    r\"\\bReg-\" + NUM + r\"\\b\",\n",
    "    r\"\\bReg\\.\\s*\" + NUM + r\"\\b\",\n",
    "    r\"\\breg\\.-\\s*\" + NUM + r\"\\b\",            # \"reg.-{number}\"\n",
    "    r\"\\bRegulament\\s+\" + NUM + r\"\\b\",\n",
    "    r\"\\breg\\.\\s*\" + NUM + r\"\\b\",\n",
    "    r\"\\bRegulamentul-\" + NUM + r\"\\b\",\n",
    "    r\"\\bRegulamentul\\s+\" + NUM + r\"\\b\",\n",
    "\n",
    "    r\"\\boug-\" + NUM + r\"\\b\",\n",
    "    r\"\\boug\\s+\" + NUM + r\"\\b\",\n",
    "    r\"ordonanței de urgen\",                  # literal substring, no number\n",
    "\n",
    "    r\"\\bITP-\" + NUM + r\"\\b\",\n",
    "\n",
    "    r\"\\bRCA-\" + NUM + r\"\\b\",\n",
    "\n",
    "    r\"\\bPENAL-\" + NUM + r\"\\b\",\n",
    "\n",
    "    r\"\\bCodul\\s+penal\\b\",\n",
    "    r\"\\bCod\\s+penal\\b\",\n",
    "]\n",
    "count_compiled = [re.compile(p, flags=re.IGNORECASE | re.UNICODE) for p in count_patterns]\n",
    "\n",
    "def count_all_patterns(text: str) -> int:\n",
    "    total = 0\n",
    "    for rx in count_compiled:\n",
    "        total += sum(1 for _ in rx.finditer(text))\n",
    "    return total\n",
    "\n",
    "agg[\"citations_number\"] = texts.apply(count_all_patterns)\n",
    "\n",
    "# --- Extraction patterns (with normalization rules) ---\n",
    "# Each entry: (compiled_regex, normalizer_fn(match)->str or None)\n",
    "extract_specs = []\n",
    "\n",
    "# Regulament family -> \"Regulament-{n}\"\n",
    "reg_variants = [\n",
    "    r\"\\bRegulament-\" + NUM + r\"\\b\",\n",
    "    r\"\\bReg-\" + NUM + r\"\\b\",\n",
    "    r\"\\bReg\\.\\s*\" + NUM + r\"\\b\",\n",
    "    r\"\\breg\\.-\\s*\" + NUM + r\"\\b\",\n",
    "    r\"\\bRegulament\\s+\" + NUM + r\"\\b\",\n",
    "    r\"\\breg\\.\\s*\" + NUM + r\"\\b\",\n",
    "    r\"\\bRegulamentul-\" + NUM + r\"\\b\",\n",
    "    r\"\\bRegulamentul\\s+\" + NUM + r\"\\b\",\n",
    "]\n",
    "for p in reg_variants:\n",
    "    rx = re.compile(p, flags=re.IGNORECASE | re.UNICODE)\n",
    "    extract_specs.append((rx, lambda m: f\"Regulament-{m.group(1)}\"))\n",
    "\n",
    "# OUG family -> \"OUG-{n}\"\n",
    "oug_variants = [\n",
    "    r\"\\bOUG-\" + NUM + r\"\\b\",\n",
    "    r\"\\bOUG\\s+\" + NUM + r\"\\b\",\n",
    "]\n",
    "for p in oug_variants:\n",
    "    rx = re.compile(p, flags=re.IGNORECASE | re.UNICODE)\n",
    "    extract_specs.append((rx, lambda m: f\"OUG-{m.group(1)}\"))\n",
    "\n",
    "# ITP-{n} remains as is\n",
    "itp_rx = re.compile(r\"\\bITP-\" + NUM + r\"\\b\", flags=re.IGNORECASE | re.UNICODE)\n",
    "extract_specs.append((itp_rx, lambda m: f\"ITP-{m.group(1)}\"))\n",
    "\n",
    "# RCA-{n} remains as is\n",
    "rca_rx = re.compile(r\"\\bRCA-\" + NUM + r\"\\b\", flags=re.IGNORECASE | re.UNICODE)\n",
    "extract_specs.append((rca_rx, lambda m: f\"RCA-{m.group(1)}\"))\n",
    "\n",
    "# Explicit PENAL-{n} remains as is\n",
    "penal_rx = re.compile(r\"\\bPENAL-\" + NUM + r\"\\b\", flags=re.IGNORECASE | re.UNICODE)\n",
    "extract_specs.append((penal_rx, lambda m: f\"PENAL-{m.group(1)}\"))\n",
    "\n",
    "# \"Codul penal\" / \"Cod penal\" followed by an optional separator and number -> PENAL-{n}\n",
    "# We require a number to include it in the list.\n",
    "cod_penal_rx = re.compile(\n",
    "    r\"\\bCod(?:ul)?\\s+penal(?:\\s*[-–]?\\s*\" + NUM + r\")\\b\",\n",
    "    flags=re.IGNORECASE | re.UNICODE\n",
    ")\n",
    "def cod_penal_norm(m: re.Match) -> str | None:\n",
    "    # The capturing group with the number is the last one\n",
    "    n = m.group(m.lastindex) if m.lastindex else None\n",
    "    return f\"PENAL-{n}\" if n else None\n",
    "extract_specs.append((cod_penal_rx, cod_penal_norm))\n",
    "\n",
    "def extract_citations(text: str) -> list[str]:\n",
    "    results = []\n",
    "    for rx, norm in extract_specs:\n",
    "        for m in rx.finditer(text):\n",
    "            val = norm(m)\n",
    "            if val:  # only include when we have a number (e.g., Codul penal with number)\n",
    "                results.append(val)\n",
    "    return results\n",
    "\n",
    "agg[\"citations_list\"] = texts.apply(extract_citations)\n",
    "\n",
    "# Peek\n",
    "print(\"Added `citations_number` and `citations_list`. Example rows:\")\n",
    "display(agg[[\"model\", \"split\", \"id\", \"output_prompt\", \"citations_number\", \"citations_list\"]].head(10))\n",
    "\n",
    "# Save\n",
    "agg.to_csv(\"../citations-analytics-list.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a53f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import re\n",
    "\n",
    "# --- LLMs you already set up ---\n",
    "llm_4o = ChatOpenAI(model_name=\"gpt-5-mini\", api_key=\"\", seed=25, temperature=0)\n",
    "# llm_o4 = ChatOpenAI(model_name=\"o4-mini\", api_key=\"\", seed=25, temperature=0, output_version=\"responses/v1\")\n",
    "\n",
    "MAX_CONCURRENT_REQUESTS = 100\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# ========= Structured output schema =========\n",
    "class CitationResponse(BaseModel):\n",
    "    citations: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"Lista finală, unică, de citări normalizate pentru textul dat.\"\n",
    "    )\n",
    "    # optional debug fields if you want to inspect model reasoning (kept empty on purpose)\n",
    "    notes: Optional[str] = Field(None, description=\"(Opțional) Observații scurte despre parsare, dacă este cazul.\")\n",
    "\n",
    "# ========= Prompt (Romanian) + examples =========\n",
    "SYSTEM_PROMPT = \"\"\"Ești un asistent care extrage articole citate dintr-un text în limba română.\n",
    "Întoarce DOAR o listă JSON de citări normalizate, fără explicații.\n",
    "\n",
    "REGULI DE NORMALIZARE (foarte important):\n",
    "1) OUG:\n",
    "   - Forme: \"OUG 195/2002 art. 6\", \"OUG-195/2002 art 6\", \"OUG 195 art. 6\", \"OUG 195/2002\".\n",
    "   - Dacă apare \"OUG x/y art z\" -> returnează \"OUG-z\".\n",
    "   - Dacă apare doar \"OUG n\" (fără articol) -> returnează \"OUG-n\".\n",
    "   - Ignoră date și alte forme care nu sunt articole.\n",
    "\n",
    "2) Regulament:\n",
    "   - Toate formele (\"Reg.\", \"Regulament\", \"Regulamentul\", \"Reg.-\", \"Reg. 123\", \"Regulament-123\", etc.)\n",
    "   - Inlcusiv cum este mai sus pentru OUG\n",
    "   - Normalizează la \"Regulament-{n}\".\n",
    "\n",
    "3) ITP, RCA:\n",
    "   - \"ITP-{n}\" și \"RCA-{n}\" rămân neschimbate.\n",
    "\n",
    "4) Cod penal:\n",
    "   - \"PENAL-{n}\" rămâne neschimbat.\n",
    "   - \"Codul penal\" / \"Cod penal\" cu articol (ex: \"Codul penal art. 193\") -> \"PENAL-{n}\".\n",
    "   - Dacă nu e indicat articolul, ignoră (nu adăuga un PENAL fără număr de articol).\n",
    "\n",
    "5) Articole multiple:\n",
    "   - Desparte \"art. 6, 7 și 8\" în articole separate.\n",
    "   - Extinde intervale \"art. 1-3\" -> 1, 2, 3.\n",
    "   - Acceptă formate cu sau fără punct: \"art\", \"art.\", \"alin.\", etc. (alin. poate apărea, dar noi extragem doar numărul articolului principal).\n",
    "\n",
    "6) Unicitate și ordine:\n",
    "   - Păstrează ordinea primei apariții în text.\n",
    "   - Elimină duplicatele.\n",
    "\n",
    "IEȘIRE:\n",
    "- Returnează JSON VALID cu schema:\n",
    "  {\n",
    "    \"citations\": [\"...\",\"...\"]\n",
    "  }\n",
    "- Fără alte texte.\n",
    "\"\"\"\n",
    "\n",
    "# ========= Build the chain (structured output) =========\n",
    "# Use o4-mini for function-like JSON reliability\n",
    "extractor_llm = llm_4o\n",
    "\n",
    "# ========= Normalizer (post-LLM sanity; optional but useful) =========\n",
    "def _dedupe_preserve_order(items: List[str]) -> List[str]:\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for it in items:\n",
    "        if it not in seen:\n",
    "            seen.add(it)\n",
    "            out.append(it)\n",
    "    return out\n",
    "\n",
    "def _strip_spaces(items: List[str]) -> List[str]:\n",
    "    return [re.sub(r\"\\s+\", \"\", x) if any(k in x for k in [\"OUG-\", \"PENAL-\", \"Regulament-\", \"ITP-\", \"RCA-\"]) else x for x in items]\n",
    "\n",
    "def normalize_llm_output(citations: List[str]) -> List[str]:\n",
    "    # Basic cleanup: trim, collapse spaces, uppercase keys\n",
    "    cleaned = []\n",
    "    for c in citations:\n",
    "        s = c.strip()\n",
    "        # Unify casing of prefixes\n",
    "        s = re.sub(r\"^(oug|Oug)\\-\", \"OUG-\", s)\n",
    "        s = re.sub(r\"^(penal|Penal|PENAL)\\-\", \"PENAL-\", s)\n",
    "        s = re.sub(r\"^(regulament|Regulament|REGULAMENT)\\-\", \"Regulament-\", s)\n",
    "        s = re.sub(r\"^(itp|Itp|ITP)\\-\", \"ITP-\", s)\n",
    "        s = re.sub(r\"^(rca|Rca|RCA)\\-\", \"RCA-\", s)\n",
    "        cleaned.append(s)\n",
    "    cleaned = _strip_spaces(cleaned)\n",
    "    cleaned = _dedupe_preserve_order(cleaned)\n",
    "    return cleaned\n",
    "\n",
    "# ========= Async strategy to extract per row =========\n",
    "async def strategy_extract_citations(item, text_column: str = \"output_prompt\"):\n",
    "    text = str(item[text_column]) if text_column in item and pd.notna(item[text_column]) else \"\"\n",
    "    msg_vars = {\"input_text\": text}\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=SYSTEM_PROMPT),\n",
    "        HumanMessage(content=f\"TEXT:\\n{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "    extract_chain = prompt | extractor_llm\n",
    "\n",
    "    async with semaphore:\n",
    "        resp = await extract_chain.ainvoke({})\n",
    "\n",
    "    citations = json.loads(resp.content)\n",
    "    citations = normalize_llm_output(citations[\"citations\"] or [])\n",
    "    # Return a tuple similar to your existing pipeline style if needed,\n",
    "    # but here we just return the list.\n",
    "    return citations\n",
    "\n",
    "# ========= Mass runner to add a column =========\n",
    "async def mass_extract_runner(data: pd.DataFrame, text_column: str = \"output_prompt\") -> pd.DataFrame:\n",
    "    tasks = []\n",
    "    for _, item in data.iterrows():\n",
    "        tasks.append(strategy_extract_citations(item, text_column=text_column))\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    data = data.copy()\n",
    "    data[\"citations_list_llm\"] = results\n",
    "    return data\n",
    "\n",
    "# ======== Example usage (async) ========\n",
    "# new_df = await mass_extract_runner(agg, text_column=\"output_prompt\")\n",
    "# display(new_df[[\"id\", \"output_prompt\", \"citations_list_llm\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = await mass_extract_runner(agg, text_column=\"output_prompt\")\n",
    "new_df.to_csv(\"../citations-analytics-list-llm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end cell: load citations + IR refs, merge with agg, filter to split_1 only,\n",
    "# and compute how many wrong/correct answers have incorrect citations (with percentages).\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory (from your earlier aggregation).\")\n",
    "req_cols = {\"id\", \"split\", \"exact_match\"}\n",
    "missing = req_cols - set(agg.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"`agg` is missing required columns: {missing}\")\n",
    "\n",
    "# --- 1) Read citations CSV ---\n",
    "citations_path = \"../citations-analytics-list-llm.csv\"\n",
    "cit_df = pd.read_csv(citations_path)\n",
    "\n",
    "# Normalize expected columns\n",
    "if \"id\" not in cit_df.columns:\n",
    "    raise KeyError(\"Citations CSV must include an `id` column.\")\n",
    "if \"citations_list_llm\" not in cit_df.columns:\n",
    "    raise KeyError(\"Citations CSV must include a `citations_list_llm` column containing a Python-list-like string.\")\n",
    "\n",
    "# Parse citation lists safely\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "cit_df[\"citations\"] = cit_df[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_df = cit_df[[\"id\", \"citations\"]]\n",
    "\n",
    "# Coerce id type to a common type (string) to ensure joins match\n",
    "cit_df[\"id\"] = cit_df[\"id\"].astype(str)\n",
    "\n",
    "# --- 2) Read IR reference files (train/test only for split_1) ---\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    if path.endswith(\".parquet\"):\n",
    "        df = pd.read_parquet(path)\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "    if \"id\" not in df.columns or \"retrieved_documents\" not in df.columns:\n",
    "        raise KeyError(f\"IR file at {path} must include `id` and `retrieved_documents` columns.\")\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df[[\"id\", \"retrieved_documents\"]]\n",
    "\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 3) Merge citations with refs ---\n",
    "merged = cit_df.merge(ir_refs, on=\"id\", how=\"inner\", suffixes=(\"_cit\",\"_ref\"))\n",
    "\n",
    "# --- 4) Attach split & exact_match from agg, then filter to split_1 only ---\n",
    "agg_tmp = agg[[\"id\", \"split\", \"exact_match\"]].copy()\n",
    "agg_tmp[\"id\"] = agg_tmp[\"id\"].astype(str)\n",
    "merged = merged.merge(agg_tmp, on=\"id\", how=\"left\")\n",
    "\n",
    "# Keep only split_1 (train/test)\n",
    "merged = merged[merged[\"split\"].isin([\"1_train\", \"1_test\"])].copy()\n",
    "\n",
    "# --- 5) Mark incorrect citations: any citation not found among retrieved refs ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        # If no citations were provided, treat as \"no incorrect citations\"\n",
    "        # (adjust to True if you want to flag missing citations as incorrect)\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "merged[\"incorrect_citation\"] = merged.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 6) Summary: counts + percentages for incorrect citations by correctness ---\n",
    "summary = (\n",
    "    merged.groupby(\"exact_match\")[\"incorrect_citation\"]\n",
    "          .value_counts(dropna=False)\n",
    "          .unstack(fill_value=0)\n",
    "          .rename(columns={False: \"no_incorrect\", True: \"has_incorrect\"})\n",
    ")\n",
    "summary[\"total\"] = summary[\"no_incorrect\"] + summary[\"has_incorrect\"]\n",
    "summary[\"pct_incorrect\"] = (summary[\"has_incorrect\"] / summary[\"total\"]).round(3)\n",
    "\n",
    "print(\"Incorrect citations vs correctness (split_1 only):\")\n",
    "display(summary)\n",
    "\n",
    "# --- Optional: per model & split breakdown (useful to see which systems err more) ---\n",
    "if \"model\" in agg.columns:\n",
    "    merged = merged.merge(agg[[\"id\",\"model\"]].drop_duplicates(), on=\"id\", how=\"left\")\n",
    "    by_m_s = (\n",
    "        merged.groupby([\"model\", \"split\", \"exact_match\"])[\"incorrect_citation\"]\n",
    "              .value_counts(dropna=False)\n",
    "              .unstack(fill_value=0)\n",
    "              .rename(columns={False: \"no_incorrect\", True: \"has_incorrect\"})\n",
    "              .reset_index()\n",
    "    )\n",
    "    by_m_s[\"total\"] = by_m_s[\"no_incorrect\"] + by_m_s[\"has_incorrect\"]\n",
    "    by_m_s[\"pct_incorrect\"] = (by_m_s[\"has_incorrect\"] / by_m_s[\"total\"]).round(3)\n",
    "    print(\"\\nPer model & split:\")\n",
    "    display(by_m_s.sort_values([\"model\",\"split\",\"exact_match\"]).reset_index(drop=True))\n",
    "else:\n",
    "    print(\"\\n`agg` has no `model` column; skipping per model & split breakdown.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised end-to-end code: ensure per-MODEL alignment so accuracy & hallucination differ by model.\n",
    "# Key fixes:\n",
    "#  - Keep/merge on ['id','model','split'] wherever possible (from the citations CSVs).\n",
    "#  - Avoid broadcasting the same citations to all models.\n",
    "#  - Validate merges and drop accidental duplicates.\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# Try to read with model/split if they exist; otherwise fall back to id-only.\n",
    "def read_citations_number(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Normalize columns\n",
    "    df.rename(columns={c: c.lower() for c in df.columns}, inplace=True)\n",
    "    must = [\"id\",\"citations_number\"]\n",
    "    for m in must:\n",
    "        if m not in df.columns:\n",
    "            raise KeyError(f\"`{path}` must contain column `{m}`.\")\n",
    "    keep = [\"id\",\"citations_number\"]\n",
    "    # Prefer per-model/split alignment if present\n",
    "    for c in (\"model\",\"split\"):\n",
    "        if c in df.columns:\n",
    "            keep.append(c)\n",
    "    df = df[keep].copy()\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def read_citations_list(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df.rename(columns={c: c.lower() for c in df.columns}, inplace=True)\n",
    "    # Accept common variants\n",
    "    if \"citations_list_llm\" in df.columns:\n",
    "        list_col = \"citations_list_llm\"\n",
    "    elif \"citations\" in df.columns:\n",
    "        list_col = \"citations\"\n",
    "    else:\n",
    "        raise KeyError(f\"`{path}` must contain `citations_list_llm` or `citations`.\")\n",
    "    must = [\"id\", list_col]\n",
    "    for m in must:\n",
    "        if m not in df.columns:\n",
    "            raise KeyError(f\"`{path}` must contain column `{m}`.\")\n",
    "    keep = [\"id\", list_col]\n",
    "    for c in (\"model\",\"split\"):\n",
    "        if c in df.columns:\n",
    "            keep.append(c)\n",
    "    df = df[keep].copy()\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"citations\"] = df[list_col].apply(_parse_list)\n",
    "    df.drop(columns=[list_col], inplace=True)\n",
    "    return df\n",
    "\n",
    "# --- 1) Read citations (number + list). KEEP model/split if available ---\n",
    "cit_num = read_citations_number(\"../citations-analytics-number.csv\")\n",
    "cit_list = read_citations_list(\"../citations-analytics-list-llm.csv\")\n",
    "\n",
    "# Decide merge keys: prefer ['id','model','split'] if both frames have them\n",
    "merge_keys = [\"id\"]\n",
    "for c in (\"model\",\"split\"):\n",
    "    if c in cit_num.columns and c in cit_list.columns:\n",
    "        merge_keys.append(c)\n",
    "\n",
    "cit = cit_num.merge(cit_list, on=merge_keys, how=\"inner\", validate=\"one_to_one\")\n",
    "# Deduplicate defensively\n",
    "cit = cit.drop_duplicates(subset=merge_keys).reset_index(drop=True)\n",
    "\n",
    "# --- 2) IR refs (id-level) ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# Merge IR refs by id (one-to-one expected). If multiple rows per id exist, collapse to unique set.\n",
    "if ir_refs.duplicated(subset=[\"id\"]).any():\n",
    "    ir_refs = ir_refs.groupby(\"id\", as_index=False)[\"retrieved_documents\"].agg(\n",
    "        lambda lists: list({item for sub in lists for item in sub})\n",
    "    )\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "# --- 3) Bring in agg (id, model, split, exact_match). Merge on the strongest key available. ---\n",
    "agg_ids = agg[[\"id\",\"model\",\"split\",\"exact_match\"]].copy()\n",
    "agg_ids[\"id\"] = agg_ids[\"id\"].astype(str)\n",
    "\n",
    "# Decide agg merge keys: prefer ['id','model','split'] if present in `cit`\n",
    "agg_merge_keys = [\"id\"]\n",
    "for c in (\"model\",\"split\"):\n",
    "    if c in cit.columns:\n",
    "        agg_merge_keys.append(c)\n",
    "\n",
    "cit = cit.merge(agg_ids, on=agg_merge_keys, how=\"left\", validate=\"one_to_one\")\n",
    "\n",
    "# --- 4) Keep only split_1 (train/test) ---\n",
    "if \"split\" in cit.columns:\n",
    "    cit = cit[cit[\"split\"].isin([\"1_train\",\"1_test\"])].copy()\n",
    "else:\n",
    "    raise KeyError(\"No `split` column available after merges; cannot filter to split_1.\")\n",
    "\n",
    "# --- 5) Mark incorrect citations (per row, i.e., per id+model+split) ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs or [])\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 6) Aggregate properly per model/split/citations_number ---\n",
    "# Ensures no accidental duplication by grouping on the right keys.\n",
    "stats = (\n",
    "    cit.groupby([\"model\",\"split\",\"citations_number\"], dropna=False)\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x == True).sum()),\n",
    "           total=(\"exact_match\", \"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x == True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "\n",
    "# Compute metrics\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "\n",
    "# Sanity checks: ensure metrics can differ by model\n",
    "# (optional prints you can uncomment when debugging)\n",
    "# print(stats.groupby(\"model\")[[\"accuracy\",\"pct_incorrect\"]].mean())\n",
    "\n",
    "# Final tidy output\n",
    "stats = stats.sort_values([\"model\",\"split\",\"citations_number\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Aggregated report per model/split/citations_number (fixed merging):\")\n",
    "display(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaff715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# --- 1. Citations number ---\n",
    "cit_num = pd.read_csv(\n",
    "    \"../citations-analytics-number.csv\",\n",
    "    usecols=[\"id\",\"citations_number\"]\n",
    ")\n",
    "cit_num[\"id\"] = cit_num[\"id\"].astype(str)\n",
    "\n",
    "# --- 2. Citations list ---\n",
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "cit_list[\"id\"] = cit_list[\"id\"].astype(str)\n",
    "cit_list[\"citations\"] = cit_list[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_list = cit_list[[\"id\",\"citations\"]]\n",
    "\n",
    "# --- 3. Merge both citation files ---\n",
    "cit = cit_num.merge(cit_list, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 4. Read IR refs ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 5. Merge citations with refs ---\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 6. Merge with agg (only id, model, split, exact_match) ---\n",
    "agg_ids = agg[[\"id\",\"model\",\"split\",\"exact_match\"]].copy()\n",
    "agg_ids[\"id\"] = agg_ids[\"id\"].astype(str)\n",
    "cit = cit.merge(agg_ids, on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 7. Keep only split_1 ---\n",
    "cit = cit[cit[\"split\"].isin([\"1_train\",\"1_test\"])].copy()\n",
    "\n",
    "cit = cit[cit[\"model\"] == \"o4-mini (best reasoning)\"]\n",
    "\n",
    "# --- 8. Mark incorrect citations ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 9. Aggregate report ---\n",
    "stats = (\n",
    "    cit.groupby([\"model\",\"split\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x==True).sum()),\n",
    "           total=(\"exact_match\",\"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "\n",
    "# --- Save / Display table ---\n",
    "stats_sorted = stats.sort_values([\"model\",\"split\",\"citations_number\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Aggregated report per model/split/citations_number:\")\n",
    "display(stats_sorted)\n",
    "\n",
    "# Save to CSV if needed\n",
    "out_path = \"../results/citations_accuracy_report.csv\"\n",
    "stats_sorted.to_csv(out_path, index=False)\n",
    "print(f\"Saved table to {out_path}\")\n",
    "\n",
    "\n",
    "# Plot: accuracy lines + % incorrect citation bars per model, grouped by citations_number and split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Preconditions ---\n",
    "req_cols = {\"model\",\"split\",\"citations_number\",\"accuracy\",\"pct_incorrect\"}\n",
    "if \"stats\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `stats` from the previous step.\")\n",
    "missing = req_cols - set(stats.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"`stats` is missing columns: {missing}\")\n",
    "\n",
    "models = stats[\"model\"].unique()\n",
    "n_models = len(models)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 5), sharey=False)\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, model in zip(axes, models):\n",
    "    sub = stats[stats[\"model\"] == model].copy()\n",
    "    # consistent x order\n",
    "    x_vals = sorted(sub[\"citations_number\"].unique())\n",
    "    x = np.array(x_vals, dtype=float)\n",
    "\n",
    "    # Draw % incorrect bars per split, offset left/right to avoid overlap\n",
    "    splits = list(sub[\"split\"].unique())\n",
    "    n_splits = len(splits)\n",
    "    bw = 0.35 / max(n_splits, 1)  # bar width per split\n",
    "    offsets = np.linspace(-bw*(n_splits-1)/2, bw*(n_splits-1)/2, n_splits) if n_splits > 1 else [0.0]\n",
    "\n",
    "    for off, split in zip(offsets, splits):\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_bars = [ssplit.loc[k, \"pct_incorrect\"] if k in ssplit.index else 0 for k in x_vals]\n",
    "        ax.bar(x + off, y_bars, width=bw, alpha=0.4, label=f\"{split} (% incorrect)\")\n",
    "\n",
    "    # Draw accuracy lines per split\n",
    "    for split in splits:\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_line = [ssplit.loc[k, \"accuracy\"] if k in ssplit.index else np.nan for k in x_vals]\n",
    "        ax.plot(x, y_line, marker=\"o\", label=f\"{split} (accuracy)\")\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel(\"Citations number\")\n",
    "    ax.set_ylabel(\"Accuracy (lines) / % Incorrect (bars)\")\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Accuracy vs Citations Count per Model & Split\\n(+ % Incorrect Citations)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# --- 1. Citations number ---\n",
    "cit_num = pd.read_csv(\n",
    "    \"../citations-analytics-number.csv\",\n",
    "    usecols=[\"id\",\"citations_number\"]\n",
    ")\n",
    "cit_num[\"id\"] = cit_num[\"id\"].astype(str)\n",
    "\n",
    "# --- 2. Citations list ---\n",
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "cit_list[\"id\"] = cit_list[\"id\"].astype(str)\n",
    "cit_list[\"citations\"] = cit_list[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_list = cit_list[[\"id\",\"citations\"]]\n",
    "\n",
    "# --- 3. Merge both citation files ---\n",
    "cit = cit_num.merge(cit_list, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 4. Read IR refs ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 5. Merge citations with refs ---\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 6. Merge with agg (only id, model, split, exact_match) ---\n",
    "agg_ids = agg[[\"id\",\"model\",\"split\",\"exact_match\"]].copy()\n",
    "agg_ids[\"id\"] = agg_ids[\"id\"].astype(str)\n",
    "cit = cit.merge(agg_ids, on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 7. Keep only split_1 ---\n",
    "cit = cit[cit[\"split\"].isin([\"1_train\",\"1_test\"])].copy()\n",
    "\n",
    "cit = cit[cit[\"model\"] == \"Mistral (worst)\"]\n",
    "\n",
    "# --- 8. Mark incorrect citations ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 9. Aggregate report ---\n",
    "stats = (\n",
    "    cit.groupby([\"model\",\"split\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x==True).sum()),\n",
    "           total=(\"exact_match\",\"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "\n",
    "# --- Save / Display table ---\n",
    "stats_sorted = stats.sort_values([\"model\",\"split\",\"citations_number\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Aggregated report per model/split/citations_number:\")\n",
    "display(stats_sorted)\n",
    "\n",
    "# Save to CSV if needed\n",
    "out_path = \"../results/citations_accuracy_report.csv\"\n",
    "stats_sorted.to_csv(out_path, index=False)\n",
    "print(f\"Saved table to {out_path}\")\n",
    "\n",
    "\n",
    "# Plot: accuracy lines + % incorrect citation bars per model, grouped by citations_number and split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Preconditions ---\n",
    "req_cols = {\"model\",\"split\",\"citations_number\",\"accuracy\",\"pct_incorrect\"}\n",
    "if \"stats\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `stats` from the previous step.\")\n",
    "missing = req_cols - set(stats.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"`stats` is missing columns: {missing}\")\n",
    "\n",
    "models = stats[\"model\"].unique()\n",
    "n_models = len(models)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 5), sharey=False)\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, model in zip(axes, models):\n",
    "    sub = stats[stats[\"model\"] == model].copy()\n",
    "    # consistent x order\n",
    "    x_vals = sorted(sub[\"citations_number\"].unique())\n",
    "    x = np.array(x_vals, dtype=float)\n",
    "\n",
    "    # Draw % incorrect bars per split, offset left/right to avoid overlap\n",
    "    splits = list(sub[\"split\"].unique())\n",
    "    n_splits = len(splits)\n",
    "    bw = 0.35 / max(n_splits, 1)  # bar width per split\n",
    "    offsets = np.linspace(-bw*(n_splits-1)/2, bw*(n_splits-1)/2, n_splits) if n_splits > 1 else [0.0]\n",
    "\n",
    "    for off, split in zip(offsets, splits):\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_bars = [ssplit.loc[k, \"pct_incorrect\"] if k in ssplit.index else 0 for k in x_vals]\n",
    "        ax.bar(x + off, y_bars, width=bw, alpha=0.4, label=f\"{split} (% incorrect)\")\n",
    "\n",
    "    # Draw accuracy lines per split\n",
    "    for split in splits:\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_line = [ssplit.loc[k, \"accuracy\"] if k in ssplit.index else np.nan for k in x_vals]\n",
    "        ax.plot(x, y_line, marker=\"o\", label=f\"{split} (accuracy)\")\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel(\"Citations number\")\n",
    "    ax.set_ylabel(\"Accuracy (lines) / % Incorrect (bars)\")\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Accuracy vs Citations Count per Model & Split\\n(+ % Incorrect Citations)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# --- 1. Citations number ---\n",
    "cit_num = pd.read_csv(\n",
    "    \"../citations-analytics-number.csv\",\n",
    "    usecols=[\"id\",\"citations_number\"]\n",
    ")\n",
    "cit_num[\"id\"] = cit_num[\"id\"].astype(str)\n",
    "\n",
    "# --- 2. Citations list ---\n",
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "cit_list[\"id\"] = cit_list[\"id\"].astype(str)\n",
    "cit_list[\"citations\"] = cit_list[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_list = cit_list[[\"id\",\"citations\"]]\n",
    "\n",
    "# --- 3. Merge both citation files ---\n",
    "cit = cit_num.merge(cit_list, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 4. Read IR refs ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 5. Merge citations with refs ---\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 6. Merge with agg (only id, model, split, exact_match) ---\n",
    "agg_ids = agg[[\"id\",\"model\",\"split\",\"exact_match\"]].copy()\n",
    "agg_ids[\"id\"] = agg_ids[\"id\"].astype(str)\n",
    "cit = cit.merge(agg_ids, on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 7. Keep only split_1 ---\n",
    "cit = cit[cit[\"split\"].isin([\"1_train\",\"1_test\"])].copy()\n",
    "\n",
    "# --- 8. Mark incorrect citations ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 9. Aggregate report with citations_number ---\n",
    "stats = (\n",
    "    cit.groupby([\"model\",\"split\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x==True).sum()),\n",
    "           total=(\"exact_match\",\"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"citation_hallucination_rate\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "\n",
    "# --- 10. Keep only requested columns and truncate ---\n",
    "final_stats = stats[[\"model\",\"split\",\"citations_number\",\"accuracy\",\"citation_hallucination_rate\"]].copy()\n",
    "final_stats[\"accuracy\"] = final_stats[\"accuracy\"].round(4)\n",
    "final_stats[\"citation_hallucination_rate\"] = final_stats[\"citation_hallucination_rate\"].round(4)\n",
    "\n",
    "print(\"Final report per model/split/citations_number:\")\n",
    "display(final_stats)\n",
    "\n",
    "# Optionally save\n",
    "out_path = \"../results/citations_accuracy_by_number.csv\"\n",
    "final_stats.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich previous results (`cit`) with question categories (RO + EN)\n",
    "# and build a table including: id, model, split, exact_match, citations_number,\n",
    "# incorrect_citation, question_category_id, category_name (RO), category_name_en (EN)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"cit\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `cit` from the previous step (merged citations + IR + agg).\")\n",
    "need_cols = {\"id\", \"model\", \"split\", \"exact_match\", \"citations_number\", \"incorrect_citation\"}\n",
    "missing = need_cols - set(cit.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"`cit` is missing required columns: {missing}\")\n",
    "\n",
    "# --- 1) Load split_1_train/test with only id & question_category_id ---\n",
    "base = Path(\"../data/dataset_V3\")\n",
    "q_train = pd.read_csv(base / \"split_1_train.csv\", usecols=[\"id\", \"question_category_id\"])\n",
    "q_test  = pd.read_csv(base / \"split_1_test.csv\",  usecols=[\"id\", \"question_category_id\"])\n",
    "q_all = pd.concat([q_train, q_test], ignore_index=True).drop_duplicates()\n",
    "q_all[\"id\"] = q_all[\"id\"].astype(str)\n",
    "\n",
    "# --- 2) Load category stats and keep id mapping columns ---\n",
    "# Expecting something like a CSV in the provided path. Try a few sensible filenames.\n",
    "stats_path = Path(\"../data/dataset_V3/categories_stats.csv\")\n",
    "possible_files = [\n",
    "    stats_path,                               # if it's directly a .csv with that exact name\n",
    "    stats_path.with_suffix(\".csv\"),           # question_categories_stats.csv\n",
    "    stats_path / \"question_categories_stats.csv\",\n",
    "    stats_path / \"categories.csv\",\n",
    "]\n",
    "\n",
    "cat_df = None\n",
    "for p in possible_files:\n",
    "    if p.exists() and p.is_file():\n",
    "        cat_df = pd.read_csv(p)\n",
    "        break\n",
    "\n",
    "if cat_df is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find the categories stats CSV. \"\n",
    "        f\"Tried: {', '.join(str(p) for p in possible_files)}\"\n",
    "    )\n",
    "\n",
    "# Try to normalize expected columns\n",
    "# We need 'question_category_id' and the RO category name (assume 'category_name' or similar)\n",
    "col_map = {}\n",
    "if \"question_category_id\" not in cat_df.columns:\n",
    "    # try common alternatives\n",
    "    for c in cat_df.columns:\n",
    "        if c.lower() in {\"category_id\",\"cat_id\",\"question_category\",\"q_category_id\"}:\n",
    "            col_map[c] = \"question_category_id\"\n",
    "            break\n",
    "\n",
    "if \"category_name\" not in cat_df.columns:\n",
    "    for c in cat_df.columns:\n",
    "        if c.lower() in {\"category_name_ro\",\"category\",\"name\",\"category_ro\"}:\n",
    "            col_map[c] = \"category_name\"\n",
    "            break\n",
    "\n",
    "if col_map:\n",
    "    cat_df = cat_df.rename(columns=col_map)\n",
    "\n",
    "required_cols = {\"question_category_id\",\"category_name\"}\n",
    "missing_cols = required_cols - set(cat_df.columns)\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"Category stats is missing columns: {missing_cols}. Available: {list(cat_df.columns)}\")\n",
    "\n",
    "cat_df = cat_df[[\"question_category_id\",\"category_name\"]].drop_duplicates()\n",
    "# ensure consistent dtypes\n",
    "q_all[\"question_category_id\"] = q_all[\"question_category_id\"].astype(cat_df[\"question_category_id\"].dtype)\n",
    "\n",
    "# --- 3) Join questions with category names ---\n",
    "q_cat = q_all.merge(cat_df, on=\"question_category_id\", how=\"left\")\n",
    "\n",
    "# --- 4) English translations mapping ---\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "\n",
    "q_cat[\"category_name_en\"] = q_cat[\"category_name\"].map(translations).fillna(q_cat[\"category_name\"])\n",
    "\n",
    "# --- 5) Merge with previous results (`cit`) to add RO+EN category for each id ---\n",
    "cit_ids = cit[[\"id\",\"model\",\"split\",\"exact_match\",\"citations_number\",\"incorrect_citation\"]].copy()\n",
    "cit_ids[\"id\"] = cit_ids[\"id\"].astype(str)\n",
    "\n",
    "cit_with_cat = (\n",
    "    cit_ids.merge(q_cat[[\"id\",\"question_category_id\",\"category_name\",\"category_name_en\"]], on=\"id\", how=\"left\")\n",
    "           .sort_values([\"model\",\"split\",\"id\"])\n",
    "           .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- 6) Show the resulting table ---\n",
    "display(cit_with_cat.head(20))\n",
    "\n",
    "# Optional: save for later use\n",
    "# out_path = base / \"citations_with_categories.csv\"\n",
    "# cit_with_cat.to_csv(out_path, index=False)\n",
    "# print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84a476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f457466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the existing analysis by adding CATEGORIES on the x-axis.\n",
    "# We keep your previous logic and now join question categories (RO + EN),\n",
    "# then compute per (model, split, category, citations_number):\n",
    "#   - accuracy\n",
    "#   - % incorrect citations\n",
    "# and plot them with categories on X.\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# --- 0) Load/merge categories (RO + EN) for split_1 train/test ---\n",
    "base = Path(\"../data/dataset_V3\")\n",
    "q_train = pd.read_csv(base / \"split_1_train.csv\", usecols=[\"id\", \"question_category_id\"])\n",
    "q_test  = pd.read_csv(base / \"split_1_test.csv\",  usecols=[\"id\", \"question_category_id\"])\n",
    "q_all = pd.concat([q_train, q_test], ignore_index=True).drop_duplicates()\n",
    "q_all[\"id\"] = q_all[\"id\"].astype(str)\n",
    "\n",
    "# Try to find the categories stats file\n",
    "stats_candidates = [\n",
    "    base / \"categories_stats.csv\",\n",
    "    base / \"question_categories_stats\",  # if already .csv\n",
    "    base / \"categories.csv\",\n",
    "]\n",
    "cat_df = None\n",
    "for p in stats_candidates:\n",
    "    if p.exists() and p.is_file():\n",
    "        cat_df = pd.read_csv(p)\n",
    "        break\n",
    "if cat_df is None:\n",
    "    raise FileNotFoundError(f\"Could not find categories stats file. Tried: {', '.join(str(x) for x in stats_candidates)}\")\n",
    "\n",
    "# Normalize columns to have question_category_id, category_name\n",
    "col_map = {}\n",
    "if \"question_category_id\" not in cat_df.columns:\n",
    "    for c in cat_df.columns:\n",
    "        if c.lower() in {\"category_id\",\"cat_id\",\"question_category\",\"q_category_id\"}:\n",
    "            col_map[c] = \"question_category_id\"; break\n",
    "if \"category_name\" not in cat_df.columns:\n",
    "    for c in cat_df.columns:\n",
    "        if c.lower() in {\"category_name_ro\",\"category\",\"name\",\"category_ro\"}:\n",
    "            col_map[c] = \"category_name\"; break\n",
    "if col_map:\n",
    "    cat_df = cat_df.rename(columns=col_map)\n",
    "if not {\"question_category_id\",\"category_name\"} <= set(cat_df.columns):\n",
    "    raise KeyError(f\"Category stats missing required columns. Has: {list(cat_df.columns)}\")\n",
    "\n",
    "cat_df = cat_df[[\"question_category_id\",\"category_name\"]].drop_duplicates()\n",
    "# align dtypes\n",
    "q_all[\"question_category_id\"] = q_all[\"question_category_id\"].astype(cat_df[\"question_category_id\"].dtype)\n",
    "\n",
    "# English translations\n",
    "translations = {\n",
    "    \"Indicatoare și marcaje\": \"Signs and markings\",\n",
    "    \"Semnalele polițiștilor\": \"Police signals\",\n",
    "    \"Semnalele luminoase\": \"Traffic lights\",\n",
    "    \"Poziția în timpul mersului și semnalele conducătorilor de vehicule\": \"Position while driving and vehicle drivers' signals\",\n",
    "    \"Depășirea\": \"Overtaking\",\n",
    "    \"Viteza și distanța dintre vehicule\": \"Speed and distance between vehicles\",\n",
    "    \"Reguli referitoare la manevre\": \"Rules regarding maneuvers\",\n",
    "    \"Prioritatea de trecere\": \"Right of way\",\n",
    "    \"Trecerea la nivel cu calea ferată\": \"Level crossing (railroad crossing)\",\n",
    "    \"Oprirea, staționarea și parcarea\": \"Stopping, standing, and parking\",\n",
    "    \"Circulația pe autostrăzi\": \"Highway driving\",\n",
    "    \"Obligațiile conducătorilor de autovehicule\": \"Driver obligations\",\n",
    "    \"Sancțiuni și infracțiuni\": \"Sanctions and offenses\",\n",
    "    \"Reguli generale\": \"General rules\",\n",
    "    \"Conducerea preventivă\": \"Defensive driving\",\n",
    "    \"Măsuri de prim ajutor\": \"First aid measures\",\n",
    "    \"Conducerea ecologică\": \"Eco-driving\",\n",
    "    \"Noțiuni de mecanică\": \"Basic mechanics\",\n",
    "}\n",
    "q_cat = q_all.merge(cat_df, on=\"question_category_id\", how=\"left\")\n",
    "q_cat[\"category_name_en\"] = q_cat[\"category_name\"].map(translations).fillna(q_cat[\"category_name\"])\n",
    "\n",
    "# --- 1) Citations number ---\n",
    "cit_num = pd.read_csv(\n",
    "    \"../citations-analytics-number.csv\",\n",
    "    usecols=[\"id\",\"citations_number\"]\n",
    ")\n",
    "cit_num[\"id\"] = cit_num[\"id\"].astype(str)\n",
    "\n",
    "# --- 2) Citations list ---\n",
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "cit_list[\"id\"] = cit_list[\"id\"].astype(str)\n",
    "cit_list[\"citations\"] = cit_list[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_list = cit_list[[\"id\",\"citations\"]]\n",
    "\n",
    "# --- 3) Merge both citation files ---\n",
    "cit = cit_num.merge(cit_list, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 4) Read IR refs ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 5) Merge citations with refs ---\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 6) Merge with agg (only id, model, split, exact_match) ---\n",
    "agg_ids = agg[[\"id\",\"model\",\"split\",\"exact_match\"]].copy()\n",
    "agg_ids[\"id\"] = agg_ids[\"id\"].astype(str)\n",
    "cit = cit.merge(agg_ids, on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 7) Keep only split_1 ---\n",
    "cit = cit[cit[\"split\"].isin([\"1_train\",\"1_test\"])].copy()\n",
    "\n",
    "# --- 8) Attach categories (EN on x-axis later) ---\n",
    "cit = cit.merge(q_cat[[\"id\",\"category_name_en\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 9) Mark incorrect citations ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 10) Aggregate per (model, split, category, citations_number) ---\n",
    "stats_cat = (\n",
    "    cit.groupby([\"model\",\"split\",\"category_name_en\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x==True).sum()),\n",
    "           total=(\"exact_match\",\"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "stats_cat[\"accuracy\"] = stats_cat[\"correct\"] / stats_cat[\"total\"]\n",
    "stats_cat[\"pct_incorrect\"] = stats_cat[\"incorrect_citations\"] / stats_cat[\"total\"]\n",
    "\n",
    "# Sort categories by overall accuracy (optional)\n",
    "cat_order = (\n",
    "    stats_cat.groupby(\"category_name_en\")[\"accuracy\"]\n",
    "             .mean()\n",
    "             .sort_values(ascending=False)\n",
    "             .index.tolist()\n",
    ")\n",
    "\n",
    "# --- 11) Plot: X = categories (EN). For each split, lines by citations_number; bars = % incorrect (overall per split) ---\n",
    "# To keep the plot readable, we:\n",
    "#  - draw accuracy lines for each citations_number (legend shows \"cites=k\")\n",
    "#  - overlay semi-transparent bars for pct_incorrect aggregated across citations_number (so bars are not too many)\n",
    "agg_for_bars = (\n",
    "    cit.groupby([\"model\",\"split\",\"category_name_en\"])\n",
    "       .agg(\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum()),\n",
    "           total=(\"incorrect_citation\",\"size\")\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "agg_for_bars[\"pct_incorrect\"] = agg_for_bars[\"incorrect_citations\"] / agg_for_bars[\"total\"]\n",
    "\n",
    "models = stats_cat[\"model\"].unique()\n",
    "n_models = len(models)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(8*n_models, 6), sharey=False)\n",
    "\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, model in zip(axes, models):\n",
    "    sub = stats_cat[stats_cat[\"model\"] == model]\n",
    "    bar_sub = agg_for_bars[agg_for_bars[\"model\"] == model]\n",
    "\n",
    "    categories = cat_order if cat_order else sorted(sub[\"category_name_en\"].dropna().unique().tolist())\n",
    "    x = list(range(len(categories)))\n",
    "\n",
    "    # Bars: % incorrect per split (aggregated over citations_number)\n",
    "    splits = sorted(bar_sub[\"split\"].dropna().unique().tolist())\n",
    "    n_splits = len(splits)\n",
    "    bw = 0.35  # bar width per split\n",
    "    offsets = [-bw/2, bw/2] if n_splits == 2 else [0]\n",
    "\n",
    "    for off, split in zip(offsets, splits):\n",
    "        sbar = bar_sub[bar_sub[\"split\"] == split].set_index(\"category_name_en\")\n",
    "        vals = [sbar.loc[c, \"pct_incorrect\"] if c in sbar.index else 0 for c in categories]\n",
    "        ax.bar([xi + off for xi in x], vals, width=bw, alpha=0.25, label=f\"{split} (% incorrect)\")\n",
    "\n",
    "    # Lines: accuracy per citations_number, per split\n",
    "    # We'll plot a line per (split, citations_number)\n",
    "    for split in sub[\"split\"].unique():\n",
    "        ssplit = sub[sub[\"split\"] == split]\n",
    "        for k in sorted(ssplit[\"citations_number\"].unique()):\n",
    "            sk = ssplit[ssplit[\"citations_number\"] == k].set_index(\"category_name_en\")\n",
    "            vals = [sk.loc[c, \"accuracy\"] if c in sk.index else None for c in categories]\n",
    "            ax.plot(x, vals, marker=\"o\", label=f\"{split} | cites={k}\")\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(categories, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Accuracy (lines) / % Incorrect (bars)\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend(ncol=2, fontsize=9)\n",
    "\n",
    "plt.suptitle(\"Per-Category Performance by Citations Count\\n(Accuracy lines by citations_number; Bars = % incorrect citations by split)\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load dataset\n",
    "corpus_path = \"../data/dataset_V3/corpus.csv\"\n",
    "df = pd.read_csv(corpus_path)\n",
    "\n",
    "# Extract doc_source from id (everything before the first '-')\n",
    "df[\"doc_source\"] = df[\"id\"].str.split(\"-\", n=1).str[0]\n",
    "\n",
    "# Group by title_metadata and count\n",
    "counts = df.groupby(\"title_metadata\").size().reset_index(name=\"count\")\n",
    "\n",
    "# Sources to coalesce\n",
    "coalesce_sources = [\"RCA\", \"ITP\", \"PENAL\"]\n",
    "\n",
    "# Aggregate counts at the doc_source level for those sources\n",
    "agg_counts = (\n",
    "    df[df[\"doc_source\"].isin(coalesce_sources)]\n",
    "    .groupby(\"doc_source\")\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .rename(columns={\"doc_source\": \"title_metadata\"})\n",
    ")\n",
    "\n",
    "# Remove rows that came from those sources\n",
    "counts = counts[~counts[\"title_metadata\"].isin(df.loc[df[\"doc_source\"].isin(coalesce_sources), \"title_metadata\"])]\n",
    "\n",
    "# Add aggregated rows back\n",
    "counts = pd.concat([counts, agg_counts], ignore_index=True)\n",
    "\n",
    "# Sort by count descending\n",
    "counts = counts.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) One-to-one translations for your exact titles ---\n",
    "translations_ro_en = {\n",
    "    \"Reguli de circulație |  Reguli pentru circulația vehiculelor\": \"Traffic... | Rules for vehicle traffic\",\n",
    "    \"RCA\": \"Compulsory auto liability insurance (RCA)\",\n",
    "    \"Răspunderea contravențională\": \"Contraventional liability\",\n",
    "    \"Permisul de conducere\": \"Driving licence\",\n",
    "    \"Semnalizarea rutieră |  Semnalele luminoase\": \"Road... | Traffic lights\",\n",
    "    \"ITP\": \"Periodic Technical Inspection (ITP)\",\n",
    "    \"Sancțiuni contravenționale și măsuri tehnico-administrative |  Sancțiuni contravenționale complementare\": \"Contravention... | Complementary contravention sanctions\",\n",
    "    \"Sancțiuni contravenționale și măsuri tehnico-administrative |  Măsuri tehnico-administrative\": \"Contravention... | Technical-administrative measures\",\n",
    "    \"Reguli de circulație |  Reguli pentru alți participanți la trafic\": \"Traffic... | Rules for other road users\",\n",
    "    \"Sancțiuni contravenționale și măsuri tehnico-administrative |  Constatarea contravențiilor și aplicarea sancțiunilor contravenționale\": \"Contravention... | Finding contraventions & applying sanctions\",\n",
    "    \"Infracțiuni și pedepse\": \"Criminal offenses and penalties\",\n",
    "    \"Vehiculele |  Înmatricularea, înregistrarea și radierea vehiculelor\": \"Vehicles | Registration, recording, and deregistration\",\n",
    "    \"Vehiculele |  Starea tehnică a vehiculelor și controlul acesteia\": \"Vehicles | Technical condition & inspection\",\n",
    "    \"Semnalizarea rutieră |  Marcajele\": \"Road... | Road markings\",\n",
    "    \"Conducătorii de vehicule |  Permisul de conducere\": \"Drivers | Driving licence\",\n",
    "    \"Atribuții ale unor ministere și ale altor autorități ale administrației publice\": \"Powers of ministries and other public authorities\",\n",
    "    \"PENAL\": \"Criminal law (PENAL)\",\n",
    "    \"Reguli de circulație |  Obligații în caz de accident\": \"Traffic... | Obligations in case of accident\",\n",
    "    \"Dispoziții finale\": \"Final provisions\",\n",
    "    \"Vehiculele |  Înmatricularea și înregistrarea vehiculelor\": \"Vehicles | Registration and recording\",\n",
    "    \"Semnalizarea rutieră |  Semnalizarea trecerilor la nivel cu calea ferată\": \"Road... | Railway level crossing signage\",\n",
    "    \"Semnalizarea rutieră\": \"Road signage\",\n",
    "    \"Circulația autovehiculelor cu mase și/sau gabarite depășite ori care transporta mărfuri sau produse periculoase\": \"Vehicles with exceeded mass/size or transporting dangerous goods\",\n",
    "    \"Reguli de circulație |  Utilizarea părții carosabile\": \"Traffic... | Use of the carriageway\",\n",
    "    \"Reguli de circulație |  Obligațiile participanților la trafic\": \"Traffic... | Obligations of road users\",\n",
    "    \"Semnalizarea rutieră |  Semnalele utilizate de conducătorii autovehiculelor cu regim de circulație prioritară și obligațiile celorlalți participanți la trafic\": \"Road... | Signals of priority vehicles & obligations of other road users\",\n",
    "    \"Vehiculele |  Condițiile privind circulația vehiculelor și controlul acestora\": \"Vehicles | Conditions for circulation & control\",\n",
    "    \"Semnalizarea rutieră |  Indicatoarele\": \"Road... | Road signs\",\n",
    "    \"Semnalizarea rutieră |  Semnalizarea limitelor laterale ale platformei drumului și a lucrărilor\": \"Road... | Marking road platform edges & roadworks\",\n",
    "    \"Cai de atac împotriva procesului-verbal de constatare a contravenției\": \"Appeals against the contravention report\",\n",
    "    \"Reguli de circulație |  Reguli generale\": \"Traffic... | General rules\",\n",
    "    \"Semnalizarea rutieră |  Semnalele polițiștilor și ale altor persoane care dirijează circulația\": \"Road... | Signals of police officers & other traffic controllers\",\n",
    "    \"Conducătorii de vehicule |  Dispoziții generale\": \"Drivers | General provisions\",\n",
    "    \"Reguli de circulație |  Circulația pe autostrăzi\": \"Traffic... | Motorway driving\",\n",
    "    \"Sancțiuni contravenționale și măsuri tehnico-administrative |  Restituirea permisului de conducere și reducerea perioadei de suspendare a exercitării dreptului de a conduce\": \"Contravention... | Licence return & reduction of suspension\",\n",
    "    \"Vehiculele |  Obligațiile proprietarilor sau deținătorilor de vehicule\": \"Vehicles | Obligations of owners/holders\",\n",
    "    \"Reguli de circulație |  Circulația autovehiculelor în traficul internațional\": \"Traffic... | International road traffic\",\n",
    "}\n",
    "\n",
    "# --- 2) Add English labels (fall back to RO if anything is missing) ---\n",
    "counts = counts.copy()\n",
    "counts[\"title_en\"] = counts[\"title_metadata\"].map(translations_ro_en).fillna(counts[\"title_metadata\"])\n",
    "\n",
    "# Optional: warn if anything didn’t map (should be zero with the dict above)\n",
    "unmapped = sorted(set(counts.loc[counts[\"title_en\"] == counts[\"title_metadata\"], \"title_metadata\"]))\n",
    "if unmapped:\n",
    "    print(\"⚠️ Unmapped titles (using RO as-is):\")\n",
    "    for t in unmapped:\n",
    "        print(\" -\", t)\n",
    "\n",
    "# --- 3) Sort for readability (ascending so smallest at top) ---\n",
    "plot_df = counts.sort_values(\"count\", ascending=True)\n",
    "\n",
    "# --- 4) Styling + plot (mirrors your example) ---\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 14,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10), constrained_layout=True)\n",
    "\n",
    "bars = ax.barh(\n",
    "    plot_df[\"title_en\"],\n",
    "    plot_df[\"count\"],\n",
    "    color=\"#3572b0\"\n",
    ")\n",
    "\n",
    "# Annotate bars with counts\n",
    "for bar, value in zip(bars, plot_df[\"count\"]):\n",
    "    ax.text(\n",
    "        value + max(plot_df[\"count\"])*0.01,  # small offset to the right\n",
    "        bar.get_y() + bar.get_height()/2,\n",
    "        str(int(value)),\n",
    "        va=\"center\", ha=\"left\", fontsize=13\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Number of Documents\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_title(\"Distribution of Documents per Source\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.xaxis.grid(True, linestyle=\"--\", linewidth=0.7, alpha=0.6)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "# Save + show\n",
    "output_path = \"../plots/new_plots/distribution_documents_per_source.pdf\"\n",
    "plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved plot to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
