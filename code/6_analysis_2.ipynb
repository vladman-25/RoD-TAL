{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a3f6fcd",
   "metadata": {},
   "source": [
    "# captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8587ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def count_reasoning(x):\n",
    "    return x.count(\"[REASONING]\")\n",
    "\n",
    "captions = pd.read_csv(\"../code/caption_analysis.csv\")[[\"id\", \"wrong_caption\",\"legal_interference\",\"wrong_legal\"]]\n",
    "\n",
    "df1 = pd.concat([\n",
    "    pd.read_csv(\"../results/vqa/vqa_strat_7_split_3.csv\"), \n",
    "    pd.read_csv(\"../results/vqa/vqa_strat_7_split_4.csv\")], ignore_index=True)[['id', \"exact_match\", \"output_prompt\"]]\n",
    "df1[\"output_prompt\"] = df1[\"output_prompt\"].apply(count_reasoning)\n",
    "df1['type'] = 'caption_only'\n",
    "df1 = pd.merge(df1, captions, on=[\"id\"], how=\"inner\")\n",
    "\n",
    "df2 = pd.concat([\n",
    "    pd.read_csv(\"../results/vqa/vqa_strat_8_split_3.csv\"), \n",
    "    pd.read_csv(\"../results/vqa/vqa_strat_8_split_4.csv\")], ignore_index=True)[['id', \"exact_match\", \"output_prompt\"]]\n",
    "df2['type'] = 'image_only'\n",
    "df2[\"output_prompt\"] = df2[\"output_prompt\"].apply(count_reasoning)\n",
    "df2 = pd.merge(df2, captions, on=[\"id\"], how=\"inner\")\n",
    "\n",
    "df3 = pd.concat([\n",
    "    pd.read_csv(\"../results/vqa/vqa_strat_9_split_3.csv\"), \n",
    "    pd.read_csv(\"../results/vqa/vqa_strat_9_split_4.csv\")], ignore_index=True)[['id', \"exact_match\", \"output_prompt\"]]\n",
    "df3['type'] = 'image_caption'\n",
    "df3[\"output_prompt\"] = df3[\"output_prompt\"].apply(count_reasoning)\n",
    "df3 = pd.merge(df3, captions, on=[\"id\"], how=\"inner\")\n",
    "\n",
    "merged_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "print(merged_df)\n",
    "# columns: id, exact_match, type, wrong_Caption, legal_interference, wrong_legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (\n",
    "    merged_df\n",
    "    .groupby([\"wrong_caption\", \"legal_interference\", \"wrong_legal\", \"type\"])\n",
    "    .agg(\n",
    "        count=(\"id\", \"size\"),\n",
    "        exact_match_rate=(\"exact_match\", lambda x: x.mean()),  # True counts as 1, False as 0\n",
    "        reasoning_count=(\"output_prompt\", lambda x: x.mean())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79465b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "path = \"../code/caption_analysis.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Group by the three flags and count\n",
    "counts = df.groupby([\"wrong_caption\", \"legal_interference\", \"wrong_legal\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768e0fe",
   "metadata": {},
   "source": [
    "# judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc411c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_merge_df():\n",
    "    splits = [\"1_train\", \"1_test\", \"2\"]\n",
    "\n",
    "    specs = [\n",
    "        # (model_display_name, base_path_with_{split})\n",
    "        (\"o4-mini (best reasoning)\", \"../results/qa/qa_strat_6_split_{}.csv\"),\n",
    "        (\"Mistral (worst)\",     \"../results/qa_vllm/qa_strat_4_split_{}_vllm.csv\"),\n",
    "        (\"Gemma (best open)\",         \"../results/qa_vllm2/qa_strat_4_split_{}_vllm.csv\"),\n",
    "    ]\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for model_name, base in specs:\n",
    "        for split in splits:\n",
    "            base_path = base.format(split)\n",
    "            df = pd.read_csv(base_path)\n",
    "\n",
    "            sub = df[[\"id\", \"output_prompt\", \"exact_match\", \"input_prompt\"]].copy()\n",
    "            sub.columns = [\"id\", \"output_prompt\", \"exact_match\", \"input_prompt\"]\n",
    "            sub.insert(0, \"model\", model_name)\n",
    "            sub.insert(1, \"split\", split)\n",
    "\n",
    "            frames.append(sub)\n",
    "\n",
    "    agg = pd.concat(frames, ignore_index=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ce20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def load_inputs():\n",
    "    df1 = load_merge_df()\n",
    "    qa_train = pd.read_csv(\"../data/dataset_V3/split_1_train.csv\", index_col=False)\n",
    "    qa_test  = pd.read_csv(\"../data/dataset_V3/split_1_test.csv\", index_col=False)\n",
    "    qa_test2  = pd.read_csv(\"../data/dataset_V3/split_2.csv\", index_col=False)\n",
    "    qa_refs = pd.concat([qa_train, qa_test, qa_test2], ignore_index=True)\n",
    "    qa_refs = qa_refs[[\"id\", \"question\", \"answers\" ,\"explanation\"]]\n",
    "    merged_df = pd.merge(df1, qa_refs, on=[\"id\"], how=\"inner\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c54ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "reasoning = {\n",
    "    \"effort\": \"low\",  # 'low', 'medium', or 'high'\n",
    "    \"summary\": \"detailed\",  # 'detailed', 'auto', or None\n",
    "}\n",
    "\n",
    "llm_o4 = ChatOpenAI(model_name=\"o4-mini\", api_key=\"\", reasoning=reasoning, output_version=\"responses/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "llm_judge_prompt = \"\"\"\n",
    "Vei primi:\n",
    "- O întrebare grilă (răspuns multiplu) din testele auto în limba română.\n",
    "- Documentele legale folosite de o persoană pentru a răspunde.\n",
    "- Explicația și răspunsul final oferit de acea persoană.\n",
    "- Explicația oficială și răspunsul corect.\n",
    "\n",
    "Scopul tău este să evaluezi următoarele criterii (True/False):\n",
    "1. Există pretenții greșite sau parafrazări incorecte ale legii.\n",
    "2. Sunt introduse excepții care nu există în lege.\n",
    "3. Legea este aplicată unei categorii greșite de participanți la trafic.\n",
    "4. Valorile numerice (distante, viteze, procente etc.) sunt interpretate greșit.\n",
    "5. Este citat un articol sau paragraf care nu apare identic în referințe.\n",
    "6. Explicația se contrazice în interiorul ei.\n",
    "7. Se face o generalizare incorectă dintr-o evidență parțială.\n",
    "8. Daca deși răspunsul final este corect, este posibil să fi ajuns acolo printr-o greșeală în raționament.\n",
    "\n",
    "Instrucțiuni:\n",
    "- Pentru fiecare criteriu, oferă o scurtă explicație (1–2 fraze).\n",
    "- Marchează explicit <True> sau <False>.\n",
    "- Răspunde STRICT în format XML, fără text suplimentar.\n",
    "\n",
    "Iată detaliile de analizat:\n",
    "\n",
    "<Intrebare>\n",
    "{question}\n",
    "</Intrebare>\n",
    "\n",
    "<VarianteRaspuns>\n",
    "{options_refs}\n",
    "</VarianteRaspuns>\n",
    "\n",
    "<VarianteCorecte>\n",
    "{options_correct}\n",
    "</VarianteCorecte>\n",
    "\n",
    "<DocumenteReferinta>\n",
    "{documents_ref}\n",
    "</DocumenteReferinta>\n",
    "\n",
    "<RaspunsOferit>\n",
    "{answer}\n",
    "</RaspunsOferit>\n",
    "\n",
    "<RaspunsCorect>\n",
    "{is_correct}\n",
    "</RaspunsCorect>\n",
    "\n",
    "<ExplicatieOficiala>\n",
    "{answer_ref}\n",
    "</ExplicatieOficiala>\n",
    "\n",
    "Returnează rezultatul în structura:\n",
    "\n",
    "<verdict>\n",
    "  <criteriu id=\"1\">\n",
    "    <explicatie>...</explicatie>\n",
    "    <valoare>True/False</valoare>\n",
    "  </criteriu>\n",
    "  <criteriu id=\"2\">\n",
    "    <explicatie>...</explicatie>\n",
    "    <valoare>True/False</valoare>\n",
    "  </criteriu>\n",
    "  ...\n",
    "  <criteriu id=\"8\">\n",
    "    <explicatie>...</explicatie>\n",
    "    <valoare>True/False</valoare>\n",
    "  </criteriu>\n",
    "</verdict>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MAX_CONCURRENT_REQUESTS = 15\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "async def judge(item):\n",
    "    # print(item)\n",
    "    answers = \" | \".join([entry['answer_text'] \n",
    "                                    for entry in ast.literal_eval(item['answers'])])\n",
    "    correct_answers = [entry['answer_text'][0] for entry in ast.literal_eval(item['answers']) if entry['is_correct']]\n",
    "\n",
    "    documents = item[\"input_prompt\"]\n",
    "    documents = documents.split(\"Aceastea sunt legile relevante, dar nu neaparat toate sunt relevante:\")[1]\n",
    "    documents = documents.split(\"===================\")[0]\n",
    "    input_prompt = llm_judge_prompt.format(\n",
    "        question=item[\"question\"],\n",
    "        options_refs=answers,\n",
    "        options_correct=correct_answers,\n",
    "        documents_ref=documents,\n",
    "        answer=item[\"output_prompt\"],\n",
    "        is_correct=\"Raspunsul final este corect\" if item[\"exact_match\"] else \"Raspunsul final este incorect\",\n",
    "        answer_ref=item[\"explanation\"]\n",
    "    )\n",
    "    print(input_prompt)\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": input_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    async with semaphore:\n",
    "        res = await llm_o4.ainvoke([message])\n",
    "        res = res.content\n",
    "    \n",
    "    reasoning = [x['text'] for x in res[0]['summary']]\n",
    "    output = res[1]['text']\n",
    "    print(output_full)\n",
    "    output_full = \"\\n\".join([f\"[REASONING]{x}\" for x in reasoning]) + f\"\\n[OUTPUT]{output}\"\n",
    "\n",
    "    return input_prompt, output_full, output\n",
    "\n",
    "async def mass_qa_runner(data: pd.DataFrame):\n",
    "    tasks = []\n",
    "    for real_idx, (idx, item) in tqdm(enumerate(data.iterrows())):\n",
    "        # Create tasks for parallel processing\n",
    "        tasks.append(judge(item))\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    final_input_prompt = []\n",
    "    final_output_prompt = []\n",
    "    final_result = []\n",
    "\n",
    "    for result, (idx, item) in zip(results, data.iterrows()):\n",
    "        input_prompt, output_full, output = result\n",
    "        final_input_prompt.append(input_prompt)\n",
    "        final_output_prompt.append(output_full)\n",
    "        final_result.append(output)\n",
    "        print(output)\n",
    "\n",
    "    data['judge_in'] = final_input_prompt\n",
    "    data['judge_out_full'] = final_output_prompt\n",
    "    data['judge_out'] = final_result\n",
    "\n",
    "    return data\n",
    "\n",
    "merged_df = load_inputs()\n",
    "result_df = await mass_qa_runner(merged_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52610f",
   "metadata": {},
   "source": [
    "#### next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ef514",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\"o4-mini (best reasoning)\", \"../results/qa/qa_strat_6_split_{}\"),\n",
    "    (\"Mistral (worst)\",     \"../results/qa_vllm/qa_strat_4_split_{}_vllm\"),\n",
    "    (\"Gemma (best open)\",         \"../results/qa_vllm2/qa_strat_4_split_{}_vllm\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f03e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-cell script to read the 9 result DataFrames, normalize, and concatenate\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# --- Config (edit here if paths change) ---\n",
    "splits = [\"1_train\", \"1_test\", \"2\"]\n",
    "\n",
    "specs = [\n",
    "    # (model_display_name, base_path_with_{split})\n",
    "    (\"o4-mini (best reasoning)\", \"../results/qa/qa_strat_6_split_{}\"),\n",
    "    (\"Mistral (worst)\",     \"../results/qa_vllm/qa_strat_4_split_{}_vllm\"),\n",
    "    (\"Gemma (best open)\",         \"../results/qa_vllm2/qa_strat_4_split_{}_vllm\"),\n",
    "]\n",
    "\n",
    "# --- Helpers ---\n",
    "def _guess_file(path_str: str) -> Path:\n",
    "    \"\"\"\n",
    "    Given a base path that may be a file without extension or a directory,\n",
    "    try to resolve to a concrete file we can read.\n",
    "    Tries common extensions and common filenames inside directories.\n",
    "    \"\"\"\n",
    "    p = Path(path_str)\n",
    "    # If path already points to a file, use it\n",
    "    if p.is_file():\n",
    "        return p\n",
    "\n",
    "    # Try with common single-file extensions\n",
    "    for ext in (\".parquet\", \".csv\", \".jsonl\", \".json\"):\n",
    "        q = Path(path_str + ext)\n",
    "        if q.exists() and q.is_file():\n",
    "            return q\n",
    "\n",
    "    # If it's a directory, try common filenames; else try first matching file by extension\n",
    "    if p.exists() and p.is_dir():\n",
    "        preferred_names = [\n",
    "            \"predictions.parquet\", \"results.parquet\", \"df.parquet\", \"data.parquet\",\n",
    "            \"predictions.csv\", \"results.csv\", \"df.csv\", \"data.csv\",\n",
    "            \"predictions.jsonl\", \"results.jsonl\", \"df.jsonl\", \"data.jsonl\",\n",
    "            \"predictions.json\", \"results.json\", \"df.json\", \"data.json\",\n",
    "        ]\n",
    "        for name in preferred_names:\n",
    "            q = p / name\n",
    "            if q.exists() and q.is_file():\n",
    "                return q\n",
    "        # Fallback: first file by extension priority\n",
    "        for ext in (\"*.parquet\", \"*.csv\", \"*.jsonl\", \"*.json\"):\n",
    "            matches = sorted(p.glob(ext))\n",
    "            if matches:\n",
    "                return matches[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not resolve a file for base path: {path_str}\")\n",
    "\n",
    "def _read_df(resolved: Path) -> pd.DataFrame:\n",
    "    if resolved.suffix == \".parquet\":\n",
    "        return pd.read_parquet(resolved)\n",
    "    if resolved.suffix == \".csv\":\n",
    "        return pd.read_csv(resolved)\n",
    "    if resolved.suffix in (\".jsonl\",):\n",
    "        return pd.read_json(resolved, lines=True)\n",
    "    if resolved.suffix in (\".json\",):\n",
    "        # try records; if not, fallback to auto-infer\n",
    "        try:\n",
    "            with open(resolved, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                return pd.DataFrame(data)\n",
    "            # try nested under \"data\" or \"results\"\n",
    "            for key in (\"data\", \"results\"):\n",
    "                if key in data and isinstance(data[key], list):\n",
    "                    return pd.DataFrame(data[key])\n",
    "            # last resort\n",
    "            return pd.json_normalize(data)\n",
    "        except Exception:\n",
    "            return pd.read_json(resolved)\n",
    "    raise ValueError(f\"Unsupported file type: {resolved}\")\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates) -> str:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # try case-insensitive\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in lower:\n",
    "            return lower[c.lower()]\n",
    "    raise KeyError(f\"None of the candidate columns found: {candidates}\\nAvailable: {list(df.columns)}\")\n",
    "\n",
    "# Candidate column names to normalize\n",
    "ID_CANDS          = [\"id\", \"example_id\", \"sample_id\", \"question_id\", \"qid\", \"row_id\"]\n",
    "OUTPUT_PROMPT_CANDS = [\"output_prompt\", \"final_prompt\", \"prompt_out\", \"model_output\", \"output\", \"response\", \"answer\"]\n",
    "EXACT_MATCH_CANDS = [\"exact_match\", \"em\", \"is_exact_match\", \"match_exact\", \"exact\"]\n",
    "\n",
    "# --- Load, normalize, concat ---\n",
    "frames = []\n",
    "provenance = []  # keep track for debugging\n",
    "\n",
    "for model_name, base in specs:\n",
    "    for split in splits:\n",
    "        base_path = base.format(split)\n",
    "        resolved = _guess_file(base_path)\n",
    "        df = _read_df(resolved)\n",
    "\n",
    "        id_col     = _pick_col(df, ID_CANDS)\n",
    "        out_col    = _pick_col(df, OUTPUT_PROMPT_CANDS)\n",
    "        exact_col  = _pick_col(df, EXACT_MATCH_CANDS)\n",
    "\n",
    "        sub = df[[id_col, out_col, exact_col]].copy()\n",
    "        sub.columns = [\"id\", \"output_prompt\", \"exact_match\"]\n",
    "        sub.insert(0, \"model\", model_name)\n",
    "        sub.insert(1, \"split\", split)\n",
    "\n",
    "        frames.append(sub)\n",
    "        provenance.append({\"model\": model_name, \"split\": split, \"path\": str(resolved), \"rows\": len(sub)})\n",
    "\n",
    "# Final aggregated DataFrame\n",
    "agg = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Ensure exact_match is boolean if possible\n",
    "if agg[\"exact_match\"].dtype == object:\n",
    "    # try to coerce strings like \"true\"/\"false\"/\"1\"/\"0\"\n",
    "    agg[\"exact_match\"] = agg[\"exact_match\"].map(\n",
    "        lambda x: True if str(x).strip().lower() in {\"1\",\"true\",\"t\",\"yes\",\"y\"} \n",
    "        else False if str(x).strip().lower() in {\"0\",\"false\",\"f\",\"no\",\"n\"} \n",
    "        else x\n",
    "    )\n",
    "\n",
    "# Reorder columns exactly as requested\n",
    "agg = agg[[\"model\", \"split\", \"id\", \"output_prompt\", \"exact_match\"]]\n",
    "\n",
    "# Display a brief summary and show the first few rows for sanity check (comment out if unwanted)\n",
    "print(\"Loaded pieces:\")\n",
    "for p in provenance:\n",
    "    print(f\"- {p['model']} | {p['split']} | {p['rows']} rows | {p['path']}\")\n",
    "print(\"\\nAggregated shape:\", agg.shape)\n",
    "display(agg.head(10))\n",
    "\n",
    "# If you want to save the result, uncomment:\n",
    "# agg.to_parquet(\"./results/qa_aggregated.parquet\", index=False)\n",
    "agg.to_csv(\"../citations-analytics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-cell: add `citations_number` by counting specified regex occurrences in `output_prompt` (case-insensitive)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- Preconditions ---\n",
    "if \"agg\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `agg` in memory from the previous step.\")\n",
    "if \"output_prompt\" not in agg.columns:\n",
    "    raise KeyError(\"`agg` must have an `output_prompt` column.\")\n",
    "\n",
    "# Make sure output_prompt is string\n",
    "texts = agg[\"output_prompt\"].fillna(\"\").astype(str)\n",
    "\n",
    "# --- Define the patterns exactly as requested (case-insensitive) ---\n",
    "# For the {number} placeholder we use (\\d+). Hyphens and spaces are matched literally as provided.\n",
    "patterns = [\n",
    "    r\"\\bRegulament-(\\d+)\\b\",\n",
    "    r\"\\bReg-(\\d+)\\b\",\n",
    "    r\"\\bReg\\.\\s*(\\d+)\\b\",\n",
    "    r\"\\breg\\.-\\s*(\\d+)\\b\",        # covers \"reg.-{number}\"\n",
    "    r\"\\bRegulament\\s+(\\d+)\\b\",\n",
    "    r\"\\breg\\.\\s*(\\d+)\\b\",\n",
    "    r\"\\bRegulamentul-(\\d+)\\b\",\n",
    "    r\"\\bRegulamentul\\s+(\\d+)\\b\",\n",
    "\n",
    "    r\"\\boug-(\\d+)\\b\",\n",
    "    r\"\\boug\\s+(\\d+)\\b\",\n",
    "    r\"ordonanței de urgen\",       # literal substring, as provided\n",
    "\n",
    "    r\"\\bITP-(\\d+)\\b\",\n",
    "\n",
    "    r\"\\bRCA-(\\d+)\\b\",\n",
    "\n",
    "    r\"\\bPENAL-(\\d+)\\b\",\n",
    "\n",
    "    r\"\\bCodul\\s+penal\\b\",\n",
    "    r\"\\bCod\\s+penal\\b\",\n",
    "]\n",
    "\n",
    "compiled = [re.compile(p, flags=re.IGNORECASE | re.UNICODE) for p in patterns]\n",
    "\n",
    "def count_all_patterns(text: str) -> int:\n",
    "    total = 0\n",
    "    for rx in compiled:\n",
    "        # Use finditer to count non-overlapping matches of each pattern\n",
    "        total += sum(1 for _ in rx.finditer(text))\n",
    "    return total\n",
    "\n",
    "agg[\"citations_number\"] = texts.apply(count_all_patterns)\n",
    "\n",
    "# Optional: quick peek\n",
    "print(\"Added `citations_number`. Example rows:\")\n",
    "display(agg[[\"model\", \"split\", \"id\", \"output_prompt\", \"citations_number\"]].head(10))\n",
    "agg.to_csv(\"../citations-analytics-number.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# --- 1. Citations number ---\n",
    "cit_num = pd.read_csv(\n",
    "    \"../citations-analytics-number.csv\",\n",
    "    usecols=[\"id\",\"citations_number\"]\n",
    ")\n",
    "cit_num[\"id\"] = cit_num[\"id\"].astype(str)\n",
    "\n",
    "# --- 2. Citations list ---\n",
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "cit_list[\"id\"] = cit_list[\"id\"].astype(str)\n",
    "cit_list[\"citations\"] = cit_list[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_list = cit_list[[\"id\",\"citations\"]]\n",
    "\n",
    "# --- 3. Merge both citation files ---\n",
    "cit = cit_num.merge(cit_list, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 4. Read IR refs ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "# ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 5. Merge citations with refs ---\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 6. Merge with agg (only id, model, split, exact_match) ---\n",
    "agg_ids = agg[[\"id\",\"model\",\"split\",\"exact_match\"]].copy()\n",
    "agg_ids[\"id\"] = agg_ids[\"id\"].astype(str)\n",
    "cit = cit.merge(agg_ids, on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 7. Keep only split_1 ---\n",
    "cit = cit[cit[\"split\"].isin([\"1_test\"])].copy()\n",
    "\n",
    "cit = cit[cit[\"model\"] == \"o4-mini (best reasoning)\"]\n",
    "\n",
    "# --- 8. Mark incorrect citations ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 9. Aggregate report ---\n",
    "stats = (\n",
    "    cit.groupby([\"model\",\"split\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x==True).sum()),\n",
    "           total=(\"exact_match\",\"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "\n",
    "# --- Save / Display table ---\n",
    "stats_sorted = stats.sort_values([\"model\",\"split\",\"citations_number\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Aggregated report per model/split/citations_number:\")\n",
    "display(stats_sorted)\n",
    "\n",
    "# Save to CSV if needed\n",
    "out_path = \"../results/citations_accuracy_report.csv\"\n",
    "stats_sorted.to_csv(out_path, index=False)\n",
    "print(f\"Saved table to {out_path}\")\n",
    "\n",
    "\n",
    "# Plot: accuracy lines + % incorrect citation bars per model, grouped by citations_number and split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Preconditions ---\n",
    "req_cols = {\"model\",\"split\",\"citations_number\",\"accuracy\",\"pct_incorrect\"}\n",
    "if \"stats\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `stats` from the previous step.\")\n",
    "missing = req_cols - set(stats.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"`stats` is missing columns: {missing}\")\n",
    "\n",
    "models = stats[\"model\"].unique()\n",
    "n_models = len(models)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 5), sharey=False)\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, model in zip(axes, models):\n",
    "    sub = stats[stats[\"model\"] == model].copy()\n",
    "    # consistent x order\n",
    "    x_vals = sorted(sub[\"citations_number\"].unique())\n",
    "    x = np.array(x_vals, dtype=float)\n",
    "\n",
    "    # Draw % incorrect bars per split, offset left/right to avoid overlap\n",
    "    splits = list(sub[\"split\"].unique())\n",
    "    n_splits = len(splits)\n",
    "    bw = 0.35 / max(n_splits, 1)  # bar width per split\n",
    "    offsets = np.linspace(-bw*(n_splits-1)/2, bw*(n_splits-1)/2, n_splits) if n_splits > 1 else [0.0]\n",
    "\n",
    "    for off, split in zip(offsets, splits):\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_bars = [ssplit.loc[k, \"pct_incorrect\"] if k in ssplit.index else 0 for k in x_vals]\n",
    "        ax.bar(x + off, y_bars, width=bw, alpha=0.4, label=f\"{split} (% incorrect)\")\n",
    "\n",
    "    # Draw accuracy lines per split\n",
    "    for split in splits:\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_line = [ssplit.loc[k, \"accuracy\"] if k in ssplit.index else np.nan for k in x_vals]\n",
    "        ax.plot(x, y_line, marker=\"o\", label=f\"{split} (accuracy)\")\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel(\"Citations number\")\n",
    "    ax.set_ylabel(\"Accuracy (lines) / % Incorrect (bars)\")\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Accuracy vs Citations Count per Model & Split\\n(+ % Incorrect Citations)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc28888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# --- 1. Citations number ---\n",
    "cit_num = pd.read_csv(\n",
    "    \"../citations-analytics-number.csv\",\n",
    "    usecols=[\"id\",\"citations_number\"]\n",
    ")\n",
    "cit_num[\"id\"] = cit_num[\"id\"].astype(str)\n",
    "\n",
    "# --- 2. Citations list ---\n",
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "cit_list[\"id\"] = cit_list[\"id\"].astype(str)\n",
    "cit_list[\"citations\"] = cit_list[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_list = cit_list[[\"id\",\"citations\"]]\n",
    "\n",
    "# --- 3. Merge both citation files ---\n",
    "cit = cit_num.merge(cit_list, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 4. Read IR refs ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 5. Merge citations with refs ---\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 6. Merge with agg (only id, model, split, exact_match) ---\n",
    "agg_ids = agg[[\"id\",\"model\",\"split\",\"exact_match\"]].copy()\n",
    "agg_ids[\"id\"] = agg_ids[\"id\"].astype(str)\n",
    "cit = cit.merge(agg_ids, on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 7. Keep only split_1 ---\n",
    "cit = cit[cit[\"split\"].isin([\"1_train\"])].copy()\n",
    "\n",
    "cit = cit[cit[\"model\"] == \"o4-mini (best reasoning)\"]\n",
    "\n",
    "# --- 8. Mark incorrect citations ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 9. Aggregate report ---\n",
    "stats = (\n",
    "    cit.groupby([\"model\",\"split\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x==True).sum()),\n",
    "           total=(\"exact_match\",\"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "\n",
    "# --- Save / Display table ---\n",
    "stats_sorted = stats.sort_values([\"model\",\"split\",\"citations_number\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Aggregated report per model/split/citations_number:\")\n",
    "display(stats_sorted)\n",
    "\n",
    "# Save to CSV if needed\n",
    "out_path = \"../results/citations_accuracy_report.csv\"\n",
    "stats_sorted.to_csv(out_path, index=False)\n",
    "print(f\"Saved table to {out_path}\")\n",
    "\n",
    "\n",
    "# Plot: accuracy lines + % incorrect citation bars per model, grouped by citations_number and split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Preconditions ---\n",
    "req_cols = {\"model\",\"split\",\"citations_number\",\"accuracy\",\"pct_incorrect\"}\n",
    "if \"stats\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `stats` from the previous step.\")\n",
    "missing = req_cols - set(stats.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"`stats` is missing columns: {missing}\")\n",
    "\n",
    "models = stats[\"model\"].unique()\n",
    "n_models = len(models)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 5), sharey=False)\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, model in zip(axes, models):\n",
    "    sub = stats[stats[\"model\"] == model].copy()\n",
    "    # consistent x order\n",
    "    x_vals = sorted(sub[\"citations_number\"].unique())\n",
    "    x = np.array(x_vals, dtype=float)\n",
    "\n",
    "    # Draw % incorrect bars per split, offset left/right to avoid overlap\n",
    "    splits = list(sub[\"split\"].unique())\n",
    "    n_splits = len(splits)\n",
    "    bw = 0.35 / max(n_splits, 1)  # bar width per split\n",
    "    offsets = np.linspace(-bw*(n_splits-1)/2, bw*(n_splits-1)/2, n_splits) if n_splits > 1 else [0.0]\n",
    "\n",
    "    for off, split in zip(offsets, splits):\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_bars = [ssplit.loc[k, \"pct_incorrect\"] if k in ssplit.index else 0 for k in x_vals]\n",
    "        ax.bar(x + off, y_bars, width=bw, alpha=0.4, label=f\"{split} (% incorrect)\")\n",
    "\n",
    "    # Draw accuracy lines per split\n",
    "    for split in splits:\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_line = [ssplit.loc[k, \"accuracy\"] if k in ssplit.index else np.nan for k in x_vals]\n",
    "        ax.plot(x, y_line, marker=\"o\", label=f\"{split} (accuracy)\")\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel(\"Citations number\")\n",
    "    ax.set_ylabel(\"Accuracy (lines) / % Incorrect (bars)\")\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Accuracy vs Citations Count per Model & Split\\n(+ % Incorrect Citations)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a766c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# --- 1. Citations number ---\n",
    "cit_num = pd.read_csv(\n",
    "    \"../citations-analytics-number.csv\",\n",
    "    usecols=[\"id\",\"citations_number\"]\n",
    ")\n",
    "cit_num[\"id\"] = cit_num[\"id\"].astype(str)\n",
    "\n",
    "# --- 2. Citations list ---\n",
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "cit_list[\"id\"] = cit_list[\"id\"].astype(str)\n",
    "cit_list[\"citations\"] = cit_list[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_list = cit_list[[\"id\",\"citations\"]]\n",
    "\n",
    "# --- 3. Merge both citation files ---\n",
    "cit = cit_num.merge(cit_list, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 4. Read IR refs ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 5. Merge citations with refs ---\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 6. Merge with agg (only id, model, split, exact_match) ---\n",
    "agg_ids = agg[[\"id\",\"model\",\"split\",\"exact_match\"]].copy()\n",
    "agg_ids[\"id\"] = agg_ids[\"id\"].astype(str)\n",
    "cit = cit.merge(agg_ids, on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 7. Keep only split_1 ---\n",
    "cit = cit[cit[\"split\"].isin([\"1_test\"])].copy()\n",
    "\n",
    "cit = cit[cit[\"model\"] == \"Gemma (best open)\"]\n",
    "\n",
    "# --- 8. Mark incorrect citations ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 9. Aggregate report ---\n",
    "stats = (\n",
    "    cit.groupby([\"model\",\"split\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x==True).sum()),\n",
    "           total=(\"exact_match\",\"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "\n",
    "# --- Save / Display table ---\n",
    "stats_sorted = stats.sort_values([\"model\",\"split\",\"citations_number\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Aggregated report per model/split/citations_number:\")\n",
    "display(stats_sorted)\n",
    "\n",
    "# Save to CSV if needed\n",
    "out_path = \"../results/citations_accuracy_report.csv\"\n",
    "stats_sorted.to_csv(out_path, index=False)\n",
    "print(f\"Saved table to {out_path}\")\n",
    "\n",
    "\n",
    "# Plot: accuracy lines + % incorrect citation bars per model, grouped by citations_number and split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Preconditions ---\n",
    "req_cols = {\"model\",\"split\",\"citations_number\",\"accuracy\",\"pct_incorrect\"}\n",
    "if \"stats\" not in globals():\n",
    "    raise NameError(\"Expected a DataFrame named `stats` from the previous step.\")\n",
    "missing = req_cols - set(stats.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"`stats` is missing columns: {missing}\")\n",
    "\n",
    "models = stats[\"model\"].unique()\n",
    "n_models = len(models)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 5), sharey=False)\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, model in zip(axes, models):\n",
    "    sub = stats[stats[\"model\"] == model].copy()\n",
    "    # consistent x order\n",
    "    x_vals = sorted(sub[\"citations_number\"].unique())\n",
    "    x = np.array(x_vals, dtype=float)\n",
    "\n",
    "    # Draw % incorrect bars per split, offset left/right to avoid overlap\n",
    "    splits = list(sub[\"split\"].unique())\n",
    "    n_splits = len(splits)\n",
    "    bw = 0.35 / max(n_splits, 1)  # bar width per split\n",
    "    offsets = np.linspace(-bw*(n_splits-1)/2, bw*(n_splits-1)/2, n_splits) if n_splits > 1 else [0.0]\n",
    "\n",
    "    for off, split in zip(offsets, splits):\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_bars = [ssplit.loc[k, \"pct_incorrect\"] if k in ssplit.index else 0 for k in x_vals]\n",
    "        ax.bar(x + off, y_bars, width=bw, alpha=0.4, label=f\"{split} (% incorrect)\")\n",
    "\n",
    "    # Draw accuracy lines per split\n",
    "    for split in splits:\n",
    "        ssplit = sub[sub[\"split\"] == split].set_index(\"citations_number\")\n",
    "        y_line = [ssplit.loc[k, \"accuracy\"] if k in ssplit.index else np.nan for k in x_vals]\n",
    "        ax.plot(x, y_line, marker=\"o\", label=f\"{split} (accuracy)\")\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel(\"Citations number\")\n",
    "    ax.set_ylabel(\"Accuracy (lines) / % Incorrect (bars)\")\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Accuracy vs Citations Count per Model & Split\\n(+ % Incorrect Citations)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30fe480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# --- Select model and split early ---\n",
    "target_model = \"o4-mini (best reasoning)\"\n",
    "target_split = \"1_test\"\n",
    "\n",
    "agg_sub = agg[(agg[\"model\"] == target_model) & (agg[\"split\"] == target_split)].copy()\n",
    "agg_sub[\"id\"] = agg_sub[\"id\"].astype(str)\n",
    "\n",
    "# --- 1. Citations number ---\n",
    "cit_num = pd.read_csv(\n",
    "    \"../citations-analytics-number.csv\",\n",
    "    usecols=[\"id\",\"citations_number\"]\n",
    ")\n",
    "cit_num[\"id\"] = cit_num[\"id\"].astype(str)\n",
    "cit_num = cit_num[cit_num[\"id\"].isin(agg_sub[\"id\"])]\n",
    "\n",
    "# --- 2. Citations list ---\n",
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "cit_list[\"id\"] = cit_list[\"id\"].astype(str)\n",
    "cit_list[\"citations\"] = cit_list[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_list = cit_list[[\"id\",\"citations\"]]\n",
    "cit_list = cit_list[cit_list[\"id\"].isin(agg_sub[\"id\"])]\n",
    "\n",
    "# --- 3. Merge both citation files ---\n",
    "cit = cit_num.merge(cit_list, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 4. Read IR refs ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 5. Merge citations with refs ---\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 6. Merge with filtered agg ---\n",
    "cit = cit.merge(agg_sub[[\"id\",\"model\",\"split\",\"exact_match\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 7. Mark incorrect citations ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 8. Aggregate report ---\n",
    "stats = (\n",
    "    cit.groupby([\"model\",\"split\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x==True).sum()),\n",
    "           total=(\"exact_match\",\"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "\n",
    "print(\"Aggregated report:\")\n",
    "display(stats.sort_values([\"citations_number\"]).reset_index(drop=True))\n",
    "\n",
    "# --- 9. Plot ---\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "sub = stats\n",
    "x_vals = sorted(sub[\"citations_number\"].unique())\n",
    "x = np.array(x_vals, dtype=float)\n",
    "\n",
    "# Bars: % incorrect\n",
    "y_bars = [sub.set_index(\"citations_number\").loc[k, \"pct_incorrect\"] for k in x_vals]\n",
    "ax.bar(x - 0.15, y_bars, width=0.3, alpha=0.4, label=\"Citation hallucination rate\")\n",
    "\n",
    "# Line: accuracy\n",
    "y_line = [sub.set_index(\"citations_number\").loc[k, \"accuracy\"] for k in x_vals]\n",
    "ax.plot(x, y_line, marker=\"o\", color=\"C1\", label=\"Accuracy\")\n",
    "\n",
    "ax.set_title(f\"{target_model} | {target_split}\")\n",
    "ax.set_xlabel(\"Citations number\")\n",
    "ax.set_ylabel(\"Rate\")\n",
    "ax.set_xticks(x_vals)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"Accuracy vs Citation Hallucination Rate\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "df3 = pd.read_csv(\"../citations-analytics-list-llm.csv\", index_col=False)[[\"model\", \"split\", \"output_prompt\", \"citations_list_llm\"]]\n",
    "df3 = df3[df3[\"citations_list_llm\"].apply(ast.literal_eval).apply(len) > 0]\n",
    "\n",
    "\"\"\"\n",
    "\"OUG-X\"\n",
    "\"OUG X \"\n",
    "\"Regulament-X\"\n",
    "\"Regulament X \"\n",
    "\"Regulamentul-X\"\n",
    "\"Regulamentul X \"\n",
    "\"ITP-X\"\n",
    "\"ITP X \"\n",
    "\"RCA-X\"\n",
    "\"RCA X \"\n",
    "\"PENAL-X\"\n",
    "\"PENAL X\"\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "# number = digits with optional .segments, but no slashes\n",
    "number_part = r'\\d+(?:\\.\\d+)*'\n",
    "\n",
    "# main regex\n",
    "pattern = re.compile(\n",
    "    rf'\\b(?:OUG|Regulamentul|Regulament|ITP|RCA|PENAL)[ -]{number_part}\\b',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# True if NO valid citation pattern is found in output_prompt\n",
    "df3['wrong_cit_format'] = ~df3['output_prompt'].astype(str).str.contains(pattern, na=False)\n",
    "\n",
    "stats = (\n",
    "    df3.groupby([\"model\", \"split\"])\n",
    "       .agg(\n",
    "           total=(\"wrong_cit_format\", \"size\"),\n",
    "           wrong=(\"wrong_cit_format\", \"sum\")\n",
    "       )\n",
    "       .assign(\n",
    "           wrong_pct=lambda d: 100 * d[\"wrong\"] / d[\"total\"]\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "\n",
    "print(stats.round(2).to_latex(\n",
    "    index=False,          # don't print row indices\n",
    "    escape=False,         # allow special chars like model names with ()\n",
    "    column_format=\"l l r r r\",  # adjust alignment: l=left, r=right\n",
    "    caption=\"Wrong citation format per moel & split\",\n",
    "    label=\"tab:wrong_citation_format\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b54f0",
   "metadata": {},
   "source": [
    "### hallucination/acc rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load both CSVs\n",
    "df1 = pd.read_csv(\"../citations-analytics.csv\", index_col=False)\n",
    "df2 = pd.read_csv(\"../citations-analytics-number.csv\", index_col=False)[[\"model\", \"split\", \"id\", \"citations_number\"]]\n",
    "df3 = pd.read_csv(\"../citations-analytics-list-llm.csv\", index_col=False)[[\"model\", \"split\", \"id\", \"citations_list_llm\"]]\n",
    "\n",
    "ir_train = pd.read_csv(\"../results/ir/ir_strat_6_train.csv\", index_col=False)\n",
    "ir_test  = pd.read_csv(\"../results/ir/ir_strat_6_test.csv\", index_col=False)\n",
    "\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "ir_refs = ir_refs[[\"id\", \"retrieved_documents\"]]\n",
    "\n",
    "# Merge on model, split, id\n",
    "merged_df = pd.merge(df1, df2, on=[\"model\", \"split\", \"id\"], how=\"inner\")\n",
    "merged_df = pd.merge(merged_df, df3, on=[\"model\", \"split\", \"id\"], how=\"inner\")\n",
    "merged_df = pd.merge(merged_df, ir_refs, on=[\"id\"], how=\"inner\")\n",
    "\n",
    "\n",
    "def has_incorrect(citations, refs):\n",
    "    refs = ast.literal_eval(refs)\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "def deduplicate(citations):\n",
    "    return list(set(ast.literal_eval(citations)))\n",
    "\n",
    "def transform_len(citations):\n",
    "    return len(citations)\n",
    "\n",
    "merged_df['citations_list_llm'] = merged_df.apply(\n",
    "    lambda row: deduplicate(row['citations_list_llm']), axis=1\n",
    ")\n",
    "\n",
    "merged_df[\"incorrect_citation\"] = merged_df.apply(\n",
    "    lambda row: has_incorrect(row[\"citations_list_llm\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "merged_df[\"citations_number\"] = merged_df.apply(\n",
    "    lambda row: transform_len(row[\"citations_list_llm\"]), axis=1\n",
    ")\n",
    "\n",
    "# Show head\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580be51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Aggregate report from merged_df ---\n",
    "# Requires: merged_df has columns: model, split, citations_number, incorrect_citation, exact_match\n",
    "needed = {\"model\",\"split\",\"citations_number\",\"incorrect_citation\",\"exact_match\"}\n",
    "missing = needed - set(merged_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"'merged_df' is missing required columns: {missing}\")\n",
    "\n",
    "stats = (\n",
    "    merged_df\n",
    "    .groupby([\"model\", \"split\", \"citations_number\"], as_index=False)\n",
    "    .agg(\n",
    "        correct=(\"exact_match\", lambda x: (x == True).sum()),\n",
    "        total=(\"exact_match\", \"size\"),\n",
    "        incorrect_citations=(\"incorrect_citation\", lambda x: (x == True).sum()),\n",
    "    )\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "stats = stats[stats[\"total\"] > 2]\n",
    "\n",
    "print(stats[[\"model\",\"split\",\"citations_number\", \"total\", \"accuracy\", \"pct_incorrect\"]][(stats[\"model\"] == \"Gemma (best open)\") & (stats[\"split\"] == \"1_test\")].to_markdown())\n",
    "print(stats[[\"model\",\"split\",\"citations_number\", \"total\",\"accuracy\", \"pct_incorrect\"]][(stats[\"model\"] == \"Mistral (worst)\") & (stats[\"split\"] == \"1_test\")].to_markdown())\n",
    "print(stats[[\"model\",\"split\",\"citations_number\", \"total\", \"accuracy\",\"pct_incorrect\"]][(stats[\"model\"] == \"o4-mini (best reasoning)\") & (stats[\"split\"] == \"1_test\")].to_markdown())\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Aggregated report:\")\n",
    "# print(stats)\n",
    "# display(stats.sort_values([\"model\",\"split\",\"citations_number\"]).reset_index(drop=True))\n",
    "\n",
    "# --- 2) Plot per (model, split): bars = pct_incorrect, line = accuracy ---\n",
    "def plot_model_split(sub_stats, target_model, target_split):\n",
    "    sub = sub_stats[(sub_stats[\"model\"] == target_model) & (sub_stats[\"split\"] == target_split)]\n",
    "    if sub.empty:\n",
    "        return\n",
    "\n",
    "    x_vals = sorted(sub[\"citations_number\"].unique())\n",
    "    x = np.array(x_vals, dtype=float)\n",
    "    sub_idx = sub.set_index(\"citations_number\")\n",
    "\n",
    "    # Bars: % incorrect\n",
    "    y_bars = [float(sub_idx.loc[k, \"pct_incorrect\"]) for k in x_vals]\n",
    "\n",
    "    # Line: accuracy\n",
    "    y_line = [float(sub_idx.loc[k, \"accuracy\"]) for k in x_vals]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    ax.bar(x - 0.15, y_bars, width=0.3, alpha=0.4, label=\"Citation hallucination rate\")\n",
    "    ax.plot(x, y_line, marker=\"o\", label=\"Accuracy\")\n",
    "\n",
    "    ax.set_title(f\"{target_model} | {target_split}\")\n",
    "    ax.set_xlabel(\"Citations number\")\n",
    "    ax.set_ylabel(\"Rate\")\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.suptitle(\"Accuracy vs Citation Hallucination Rate\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Render plots for all (model, split) combos present\n",
    "for m in sorted(stats[\"model\"].unique()):\n",
    "    for s in sorted(stats[\"split\"].unique()):\n",
    "        plot_model_split(stats, m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Aggregate report from merged_df ---\n",
    "needed = {\"model\", \"split\", \"citations_number\", \"incorrect_citation\", \"exact_match\"}\n",
    "missing = needed - set(merged_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"'merged_df' is missing required columns: {missing}\")\n",
    "\n",
    "stats = (\n",
    "    merged_df\n",
    "    .groupby([\"model\", \"split\", \"citations_number\"], as_index=False)\n",
    "    .agg(\n",
    "        correct=(\"exact_match\", lambda x: (x == True).sum()),\n",
    "        total=(\"exact_match\", \"size\"),\n",
    "        incorrect_citations=(\"incorrect_citation\", lambda x: (x == True).sum()),\n",
    "    )\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "stats = stats[stats[\"total\"] > 2]\n",
    "\n",
    "# --- 2) Combined plot ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# consistent color per MODEL (not per split)\n",
    "models = stats[\"model\"].unique().tolist()\n",
    "color_map = {m: c for m, c in zip(models, plt.cm.tab10.colors)}\n",
    "\n",
    "# style maps for splits\n",
    "bar_hatch_map = {\"1_train\": None, \"1_test\": \"//\"}\n",
    "line_style_map = {\"1_train\": \"-\", \"1_test\": \"--\"}\n",
    "\n",
    "groups = list(stats.groupby([\"model\", \"split\"]))\n",
    "n_groups = len(groups)\n",
    "width = 0.12  # bar width / horizontal offset step\n",
    "\n",
    "for i, ((model, split), sub) in enumerate(groups):\n",
    "    sub = sub.sort_values(\"citations_number\")\n",
    "    x_vals = sub[\"citations_number\"].to_numpy(dtype=float)\n",
    "    y_halluc = sub[\"pct_incorrect\"].to_numpy(dtype=float)\n",
    "    y_acc = sub[\"accuracy\"].to_numpy(dtype=float)\n",
    "\n",
    "    # center the grouped bars around each x by offsetting per (model, split)\n",
    "    offset = (i - (n_groups - 1) / 2) * width\n",
    "\n",
    "    # Bars: hallucination rate\n",
    "    ax.bar(\n",
    "        x_vals + offset,\n",
    "        y_halluc,\n",
    "        width=width,\n",
    "        alpha=0.45,\n",
    "        color=color_map[model],\n",
    "        hatch=bar_hatch_map.get(split, None),\n",
    "        label=f\"{model} | {split} (Halluc.)\"\n",
    "    )\n",
    "\n",
    "    # Line: accuracy\n",
    "    ax.plot(\n",
    "        x_vals,\n",
    "        y_acc,\n",
    "        marker=\"o\",\n",
    "        linewidth=1.8,\n",
    "        color=color_map[model],\n",
    "        linestyle=line_style_map.get(split, \"-\"),\n",
    "        label=f\"{model} | {split} (Acc.)\"\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Accuracy vs Citation Hallucination Rate (all models/splits)\")\n",
    "ax.set_xlabel(\"Citations number\")\n",
    "ax.set_ylabel(\"Rate\")\n",
    "ax.set_xticks(sorted(stats[\"citations_number\"].unique()))\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/new_plots/acc_vs_hallucination.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Helpers ---\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def read_ir(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=[\"id\",\"retrieved_documents\"])\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"retrieved_documents\"] = df[\"retrieved_documents\"].apply(_parse_list)\n",
    "    return df\n",
    "\n",
    "# --- Select model and split early ---\n",
    "target_model = \"Gemma (best open)\"\n",
    "target_split = \"1_test\"\n",
    "\n",
    "agg_sub = agg[(agg[\"model\"] == target_model) & (agg[\"split\"] == target_split)].copy()\n",
    "agg_sub[\"id\"] = agg_sub[\"id\"].astype(str)\n",
    "\n",
    "# --- 1. Citations number ---\n",
    "cit_num = pd.read_csv(\n",
    "    \"../citations-analytics-number.csv\",\n",
    "    usecols=[\"id\",\"citations_number\"]\n",
    ")\n",
    "cit_num[\"id\"] = cit_num[\"id\"].astype(str)\n",
    "cit_num = cit_num[cit_num[\"id\"].isin(agg_sub[\"id\"])]\n",
    "\n",
    "# --- 2. Citations list ---\n",
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "cit_list[\"id\"] = cit_list[\"id\"].astype(str)\n",
    "cit_list[\"citations\"] = cit_list[\"citations_list_llm\"].apply(_parse_list)\n",
    "cit_list = cit_list[[\"id\",\"citations\"]]\n",
    "cit_list = cit_list[cit_list[\"id\"].isin(agg_sub[\"id\"])]\n",
    "\n",
    "# --- 3. Merge both citation files ---\n",
    "cit = cit_num.merge(cit_list, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 4. Read IR refs ---\n",
    "ir_train = read_ir(\"../results/ir/ir_strat_6_train.csv\")\n",
    "ir_test  = read_ir(\"../results/ir/ir_strat_6_test.csv\")\n",
    "ir_refs = pd.concat([ir_train, ir_test], ignore_index=True)\n",
    "\n",
    "# --- 5. Merge citations with refs ---\n",
    "cit = cit.merge(ir_refs, on=\"id\", how=\"inner\")\n",
    "\n",
    "# --- 6. Merge with filtered agg ---\n",
    "cit = cit.merge(agg_sub[[\"id\",\"model\",\"split\",\"exact_match\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "# --- 7. Mark incorrect citations ---\n",
    "def has_incorrect(citations, refs):\n",
    "    if not citations:\n",
    "        return False\n",
    "    ref_set = set(refs)\n",
    "    return any(c not in ref_set for c in citations)\n",
    "\n",
    "cit[\"incorrect_citation\"] = cit.apply(\n",
    "    lambda row: has_incorrect(row[\"citations\"], row[\"retrieved_documents\"]), axis=1\n",
    ")\n",
    "\n",
    "# --- 8. Aggregate report ---\n",
    "stats = (\n",
    "    cit.groupby([\"model\",\"split\",\"citations_number\"])\n",
    "       .agg(\n",
    "           correct=(\"exact_match\", lambda x: (x==True).sum()),\n",
    "           total=(\"exact_match\",\"size\"),\n",
    "           incorrect_citations=(\"incorrect_citation\", lambda x: (x==True).sum())\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"]\n",
    "stats[\"pct_incorrect\"] = stats[\"incorrect_citations\"] / stats[\"total\"]\n",
    "\n",
    "print(\"Aggregated report:\")\n",
    "display(stats.sort_values([\"citations_number\"]).reset_index(drop=True))\n",
    "\n",
    "# --- 9. Plot ---\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "sub = stats\n",
    "x_vals = sorted(sub[\"citations_number\"].unique())\n",
    "x = np.array(x_vals, dtype=float)\n",
    "\n",
    "# Bars: % incorrect\n",
    "y_bars = [sub.set_index(\"citations_number\").loc[k, \"pct_incorrect\"] for k in x_vals]\n",
    "ax.bar(x - 0.15, y_bars, width=0.3, alpha=0.4, label=\"Citation hallucination rate\")\n",
    "\n",
    "# Line: accuracy\n",
    "y_line = [sub.set_index(\"citations_number\").loc[k, \"accuracy\"] for k in x_vals]\n",
    "ax.plot(x, y_line, marker=\"o\", color=\"C1\", label=\"Accuracy\")\n",
    "\n",
    "ax.set_title(f\"{target_model} | {target_split}\")\n",
    "ax.set_xlabel(\"Citations number\")\n",
    "ax.set_ylabel(\"Rate\")\n",
    "ax.set_xticks(x_vals)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"Accuracy vs Citation Hallucination Rate\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce448060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read \"../citations-analytics.csv\", index=False, show head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cit_list = pd.read_csv(\n",
    "    \"../citations-analytics-list-llm.csv\",\n",
    "    usecols=[\"id\",\"citations_list_llm\"]\n",
    ")\n",
    "\n",
    "splits = [\"1_train\", \"1_test\", \"2\"]\n",
    "\n",
    "specs = [\n",
    "    # (model_display_name, base_path_with_{split})\n",
    "    (\"o4-mini (best reasoning)\", \"../results/qa/qa_strat_6_split_{}.csv\"),\n",
    "    (\"Mistral (worst)\",          \"../results/qa_vllm/qa_strat_4_split_{}_vllm.csv\"),\n",
    "    (\"Gemma (best open)\",        \"../results/qa_vllm2/qa_strat_4_split_{}_vllm.csv\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9dc9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-cell script to read the 9 result DataFrames, normalize, and concatenate\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Config (edit here if paths change) ---\n",
    "splits = [\"1_train\", \"1_test\", \"2\"]\n",
    "\n",
    "specs = [\n",
    "    # (model_display_name, base_path_with_{split})\n",
    "    (\"o4-mini (best reasoning)\", \"../results/qa/qa_strat_6_split_{}.csv\"),\n",
    "    (\"Mistral (worst)\",          \"../results/qa_vllm/qa_strat_4_split_{}_vllm.csv\"),\n",
    "    (\"Gemma (best open)\",        \"../results/qa_vllm2/qa_strat_4_split_{}_vllm.csv\"),\n",
    "]\n",
    "\n",
    "# --- Helpers ---\n",
    "VALID_CHOICES = {\"A\", \"B\", \"C\"}\n",
    "\n",
    "def parse_qa_result(cell):\n",
    "    \"\"\"\n",
    "    Parse the qa_result cell using ast.literal_eval.\n",
    "    Returns (parsed_list_or_None, wrong_format_bool).\n",
    "    wrong_format is True if parsing fails, the parsed value is not a list,\n",
    "    or any element is not exactly one of 'A', 'B', 'C'.\n",
    "    \"\"\"\n",
    "    # Missing -> wrong\n",
    "    if pd.isna(cell):\n",
    "        return None, True\n",
    "    try:\n",
    "        parsed = ast.literal_eval(cell) if isinstance(cell, str) else cell\n",
    "    except Exception:\n",
    "        return None, True\n",
    "\n",
    "    # Must be a list of strings strictly in {'A','B','C'}\n",
    "    if not isinstance(parsed, (list, tuple)):\n",
    "        return None, True\n",
    "\n",
    "    # Normalize to list[str]\n",
    "    wrong = False\n",
    "    out = []\n",
    "    for x in parsed:\n",
    "        # Cast only if it's a simple string-like; otherwise mark wrong.\n",
    "        if isinstance(x, str):\n",
    "            choice = x.strip()\n",
    "        else:\n",
    "            # allow single-letter bytes etc.\n",
    "            try:\n",
    "                choice = str(x).strip()\n",
    "            except Exception:\n",
    "                wrong = True\n",
    "                continue\n",
    "\n",
    "        out.append(choice)\n",
    "        if choice not in VALID_CHOICES:\n",
    "            wrong = True\n",
    "\n",
    "    return out, wrong\n",
    "\n",
    "# --- Load, normalize, concat ---\n",
    "frames = []\n",
    "missing_files = []\n",
    "\n",
    "for model_name, tmpl in specs:\n",
    "    for split in splits:\n",
    "        path = Path(tmpl.format(split))\n",
    "        if not path.exists():\n",
    "            missing_files.append(str(path))\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        # Ensure qa_result column exists\n",
    "        if \"qa_result\" not in df.columns:\n",
    "            # Create placeholder to keep schema consistent, mark all wrong\n",
    "            df[\"qa_result\"] = None\n",
    "\n",
    "        # Parse/validate qa_result\n",
    "        parsed, wrong = zip(*[parse_qa_result(v) for v in df[\"qa_result\"]])\n",
    "        df[\"qa_result_parsed\"] = list(parsed)\n",
    "        df[\"wrong_format\"] = list(wrong)\n",
    "\n",
    "        # Normalize essential metadata columns\n",
    "        df[\"model\"] = model_name\n",
    "        df[\"split\"] = split\n",
    "        df[\"source_path\"] = str(path)\n",
    "\n",
    "        # Optional: move common columns to front if present\n",
    "        front_cols = [c for c in [\"model\", \"split\", \"source_path\", \"qa_result\", \"qa_result_parsed\", \"wrong_format\"] if c in df.columns]\n",
    "        other_cols = [c for c in df.columns if c not in front_cols]\n",
    "        df = df[front_cols + other_cols]\n",
    "\n",
    "        frames.append(df)\n",
    "\n",
    "# Concatenate all available frames\n",
    "if frames:\n",
    "    all_df = pd.concat(frames, ignore_index=True)\n",
    "else:\n",
    "    all_df = pd.DataFrame(columns=[\"model\", \"split\", \"source_path\", \"qa_result\", \"qa_result_parsed\", \"wrong_format\"])\n",
    "\n",
    "# --- Stats: incorrectness rate per model & split ---\n",
    "if not all_df.empty:\n",
    "    grp = (\n",
    "        all_df.groupby([\"model\", \"split\"], dropna=False)[\"wrong_format\"]\n",
    "        .agg(total=\"size\", wrong_count=\"sum\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    grp[\"incorrectness_rate\"] = grp[\"wrong_count\"] / grp[\"total\"] * 100\n",
    "    grp = grp.sort_values([\"model\", \"split\"]).reset_index(drop=True)\n",
    "\n",
    "    # Display results\n",
    "    pd.set_option(\"display.max_rows\", 200)\n",
    "    print(\"=== Incorrectness rate per model & split ===\")\n",
    "    print(grp.to_string(index=False))\n",
    "else:\n",
    "    print(\"No data loaded. Check that the expected CSV files exist.\")\n",
    "\n",
    "# --- Optional: also print any missing files to help debugging ---\n",
    "if missing_files:\n",
    "    print(\"\\nThe following expected files were not found:\")\n",
    "    for p in missing_files:\n",
    "        print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUG x/y,\n",
    "Regulamentul x/y\n",
    "Regulamentul 147/2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd04504",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_str = grp.round(2).to_latex(\n",
    "    index=False,          # don't print row indices\n",
    "    escape=False,         # allow special chars like model names with ()\n",
    "    column_format=\"l l r r r\",  # adjust alignment: l=left, r=right\n",
    "    caption=\"Picked vs Reference distribution per model & split\",\n",
    "    label=\"tab:picked_ref_distribution\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== LaTeX table ===\\n\")\n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef819d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bias analysis: distribution of A/B/C selections (robust) ---\n",
    "if not all_df.empty:\n",
    "    parsed_only = all_df.loc[~all_df[\"wrong_format\"]].copy()\n",
    "    exploded = parsed_only.explode(\"qa_result_parsed\")\n",
    "    exploded = exploded[exploded[\"qa_result_parsed\"].isin(VALID_CHOICES)]\n",
    "\n",
    "    if exploded.empty:\n",
    "        print(\"\\nNo valid A/B/C answers to summarize.\")\n",
    "    else:\n",
    "        # Overall distribution across all models & splits\n",
    "        overall_counts = exploded[\"qa_result_parsed\"].value_counts().reindex([\"A\",\"B\",\"C\"], fill_value=0)\n",
    "        overall_pct = overall_counts / overall_counts.sum() * 100\n",
    "        print(\"\\n=== Overall distribution of answers (all models/splits) ===\")\n",
    "        print(overall_pct.round(2).to_string())\n",
    "\n",
    "        # Per model × split distribution\n",
    "        counts = (\n",
    "            exploded.groupby([\"model\", \"split\", \"qa_result_parsed\"])\n",
    "            .size()\n",
    "            .rename(\"n\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        # totals per (model, split)\n",
    "        counts[\"total\"] = counts.groupby([\"model\", \"split\"])[\"n\"].transform(\"sum\")\n",
    "        counts[\"percent\"] = counts[\"n\"] / counts[\"total\"] * 100\n",
    "\n",
    "        # Pivot to wide format, ensure A/B/C columns present\n",
    "        grp_pivot = counts.pivot_table(\n",
    "            index=[\"model\", \"split\"],\n",
    "            columns=\"qa_result_parsed\",\n",
    "            values=\"percent\",\n",
    "            fill_value=0,\n",
    "            aggfunc=\"sum\",\n",
    "        ).reset_index()\n",
    "\n",
    "        # Reorder A/B/C columns\n",
    "        for col in [\"A\", \"B\", \"C\"]:\n",
    "            if col not in grp_pivot.columns:\n",
    "                grp_pivot[col] = 0.0\n",
    "        grp_pivot = grp_pivot[[\"model\", \"split\", \"A\", \"B\", \"C\"]]\n",
    "\n",
    "        print(\"\\n=== Distribution of answers per model & split (percent) ===\")\n",
    "        print(grp_pivot.round(2).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo data loaded to analyze.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ada8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution analysis for correct_answers ---\n",
    "if not all_df.empty and \"correct_answers\" in all_df.columns:\n",
    "    parsed_correct = all_df.copy()\n",
    "\n",
    "    # Try to parse correct_answers if they look like lists in string form\n",
    "    def parse_correct(cell):\n",
    "        if pd.isna(cell):\n",
    "            return None\n",
    "        try:\n",
    "            parsed = ast.literal_eval(cell) if isinstance(cell, str) else cell\n",
    "        except Exception:\n",
    "            return None\n",
    "        if isinstance(parsed, (list, tuple)):\n",
    "            return [str(x).strip() for x in parsed if str(x).strip() in VALID_CHOICES]\n",
    "        if isinstance(parsed, str):\n",
    "            return [parsed.strip()] if parsed.strip() in VALID_CHOICES else None\n",
    "        return None\n",
    "\n",
    "    parsed_correct[\"correct_answers_parsed\"] = parsed_correct[\"correct_answers\"].map(parse_correct)\n",
    "\n",
    "    # Explode\n",
    "    exploded_ca = parsed_correct.explode(\"correct_answers_parsed\")\n",
    "    exploded_ca = exploded_ca[exploded_ca[\"correct_answers_parsed\"].isin(VALID_CHOICES)]\n",
    "\n",
    "    if exploded_ca.empty:\n",
    "        print(\"\\nNo valid A/B/C values in correct_answers to summarize.\")\n",
    "    else:\n",
    "        # Overall distribution\n",
    "        overall_counts_ca = exploded_ca[\"correct_answers_parsed\"].value_counts().reindex([\"A\",\"B\",\"C\"], fill_value=0)\n",
    "        overall_pct_ca = overall_counts_ca / overall_counts_ca.sum() * 100\n",
    "        print(\"\\n=== Overall distribution of correct_answers (all models/splits) ===\")\n",
    "        print(overall_pct_ca.round(2).to_string())\n",
    "\n",
    "        # Per model × split distribution\n",
    "        counts_ca = (\n",
    "            exploded_ca.groupby([\"model\", \"split\", \"correct_answers_parsed\"])\n",
    "            .size()\n",
    "            .rename(\"n\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        counts_ca[\"total\"] = counts_ca.groupby([\"model\", \"split\"])[\"n\"].transform(\"sum\")\n",
    "        counts_ca[\"percent\"] = counts_ca[\"n\"] / counts_ca[\"total\"] * 100\n",
    "\n",
    "        grp_pivot_ca = counts_ca.pivot_table(\n",
    "            index=[\"model\", \"split\"],\n",
    "            columns=\"correct_answers_parsed\",\n",
    "            values=\"percent\",\n",
    "            fill_value=0,\n",
    "            aggfunc=\"sum\",\n",
    "        ).reset_index()\n",
    "\n",
    "        # Reorder A/B/C columns\n",
    "        for col in [\"A\", \"B\", \"C\"]:\n",
    "            if col not in grp_pivot_ca.columns:\n",
    "                grp_pivot_ca[col] = 0.0\n",
    "        grp_pivot_ca = grp_pivot_ca[[\"model\", \"split\", \"A\", \"B\", \"C\"]]\n",
    "\n",
    "        print(\"\\n=== Distribution of correct_answers per model & split (percent) ===\")\n",
    "        print(grp_pivot_ca.round(2).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo correct_answers column found in data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combined table: picked (qa_result) vs reference (correct_answers) ---\n",
    "if all_df.empty:\n",
    "    print(\"\\nNo data loaded.\")\n",
    "else:\n",
    "    if \"correct_answers\" not in all_df.columns:\n",
    "        print(\"\\nNo correct_answers column found in data.\")\n",
    "    else:\n",
    "        # Parse correct_answers into a list of A/B/C (non-A/B/C are dropped)\n",
    "        def parse_correct(cell):\n",
    "            if pd.isna(cell):\n",
    "                return None\n",
    "            try:\n",
    "                parsed = ast.literal_eval(cell) if isinstance(cell, str) else cell\n",
    "            except Exception:\n",
    "                return None\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(x).strip() for x in parsed if str(x).strip() in VALID_CHOICES]\n",
    "            if isinstance(parsed, str):\n",
    "                s = parsed.strip()\n",
    "                return [s] if s in VALID_CHOICES else None\n",
    "            return None\n",
    "\n",
    "        df = all_df.copy()\n",
    "        df[\"correct_answers_parsed\"] = df[\"correct_answers\"].map(parse_correct)\n",
    "\n",
    "        # Helper to compute % table for a given column (already list-like)\n",
    "        def percent_table(df_in, colname):\n",
    "            exploded = df_in.explode(colname)\n",
    "            exploded = exploded[exploded[colname].isin(VALID_CHOICES)]\n",
    "            if exploded.empty:\n",
    "                # Return empty with required structure so merges work\n",
    "                out = pd.DataFrame(columns=[\"model\", \"split\", \"A\", \"B\", \"C\"])\n",
    "                return out\n",
    "            counts = (\n",
    "                exploded.groupby([\"model\", \"split\", colname])\n",
    "                .size()\n",
    "                .rename(\"n\")\n",
    "                .reset_index()\n",
    "            )\n",
    "            counts[\"total\"] = counts.groupby([\"model\", \"split\"])[\"n\"].transform(\"sum\")\n",
    "            counts[\"percent\"] = counts[\"n\"] / counts[\"total\"] * 100\n",
    "            pivot = counts.pivot_table(\n",
    "                index=[\"model\", \"split\"],\n",
    "                columns=colname,\n",
    "                values=\"percent\",\n",
    "                fill_value=0,\n",
    "                aggfunc=\"sum\",\n",
    "            ).reset_index()\n",
    "            # Ensure A/B/C columns present and ordered\n",
    "            for c in [\"A\", \"B\", \"C\"]:\n",
    "                if c not in pivot.columns:\n",
    "                    pivot[c] = 0.0\n",
    "            return pivot[[\"model\", \"split\", \"A\", \"B\", \"C\"]]\n",
    "\n",
    "        # Use only correctly parsed qa_result rows for \"picked\"\n",
    "        picked_base = all_df.loc[~all_df[\"wrong_format\"]].copy()\n",
    "        picked_pct = percent_table(picked_base, \"qa_result_parsed\")\n",
    "        ref_pct = percent_table(df, \"correct_answers_parsed\")\n",
    "\n",
    "        # Merge and rename columns\n",
    "        combined = picked_pct.merge(ref_pct, on=[\"model\", \"split\"], how=\"outer\", suffixes=(\"_picked\", \"_ref\"))\n",
    "        # Order columns: model, split, A_picked, A_ref, B_picked, B_ref, C_picked, C_ref\n",
    "        combined = combined[[\n",
    "            \"model\", \"split\",\n",
    "            \"A_picked\", \"A_ref\",\n",
    "            \"B_picked\", \"B_ref\",\n",
    "            \"C_picked\", \"C_ref\",\n",
    "        ]].fillna(0.0)\n",
    "\n",
    "        print(\"\\n=== Picked vs Reference distribution per model & split (percent) ===\")\n",
    "        print(combined.round(2).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f157ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_str = combined.round(2).to_latex(\n",
    "    index=False,          # don't print row indices\n",
    "    escape=False,         # allow special chars like model names with ()\n",
    "    column_format=\"l l r r r r r r\",  # adjust alignment: l=left, r=right\n",
    "    caption=\"Picked vs Reference distribution per model & split\",\n",
    "    label=\"tab:picked_ref_distribution\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== LaTeX table ===\\n\")\n",
    "print(latex_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
